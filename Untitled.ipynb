{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "#from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "#from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS\n",
    "#from sklearn.decomposition import NMF\n",
    "import math\n",
    "from collections import Counter\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/sara/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spacy\n",
    "import nltk\n",
    "import nltk.tokenize as tk\n",
    "import spacy\n",
    "import string\n",
    "import en_core_web_sm\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "nlp = en_core_web_sm.load()\n",
    "\n",
    "stopwords = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = np.array(list(pickle.load(open('./data/yay_moji.pkl','rb'))))\n",
    "mojis = pd.read_pickle('./data/df_emoji.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = pd.DataFrame(tweets, columns=['tweet'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean text before feeding it to spaCy\n",
    "# to words\n",
    "punctuations = string.punctuation\n",
    "\n",
    "# Define function to cleanup text by removing personal pronouns, stopwords, and puncuation\n",
    "def cleanup_text(docs, logging=False):\n",
    "    texts = []\n",
    "    counter = 1\n",
    "    for doc in docs:\n",
    "        if counter % 1000 == 0 and logging:\n",
    "            print(\"Processed %d out of %d documents.\" % (counter, len(docs)))\n",
    "        counter += 1\n",
    "        doc = nlp(doc, disable=['parser', 'ner'])\n",
    "        tokens = [tok.lemma_.lower().strip() for tok in doc if tok.lemma_ != '-PRON-']\n",
    "        tokens = [tok for tok in tokens if tok not in stopwords and tok not in punctuations]\n",
    "        tokens = ' '.join(tokens)\n",
    "        texts.append(tokens)\n",
    "    return pd.Series(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "tw = [word for word in tweets['tweet']]\n",
    "\n",
    "# Clean up all text\n",
    "tw_clean = cleanup_text(tw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       john daly actually ride cart major ğŸ˜‚ man live ...\n",
       "1       rt @fancy2nancy3 ğŸš¨ attn patriots ğŸš¨ please retw...\n",
       "2       actual baffle people find fully acceptable mak...\n",
       "3       rt @talasayued123 beinsport âœŒ ï¸ âœŒ ï¸ Ø§Ù„Ù†ÙŒØµØ±_Ø§Ù„Ø¨...\n",
       "4       show church wedding dress ğŸ˜ ğŸ¥° https://t.co/hzo...\n",
       "5              oh shit go ğŸ˜‚ ğŸ˜‚ ğŸ˜‚ ğŸ˜‚ https://t.co/rxbvwbqo2v\n",
       "6                     @magicalsunset love madeleine fan ğŸ˜\n",
       "7       rt @souadalshammary clip many video spread wat...\n",
       "8       rt @iceicebabysa ğŸ¤¤ ğŸ”¥ chamoy lover ğŸ‰ ğŸ“ ğŸ ğŸ¥­ ğŸ’ ğŸ¥’ ...\n",
       "9       @pascosheriff hello k9 strong busy productive ...\n",
       "10      rt @cyikemen_en route preview trailer mozart c...\n",
       "11      rt @newhopeclub ğŸ‡º ğŸ‡¸ usa tickets tomorrow ğŸ‡º ğŸ‡¸ h...\n",
       "12      rt @averagemarktv absolutely unreal two people...\n",
       "13      rt @jl_kdiamond long day draining thankful wor...\n",
       "14      @vvvalker @nyachamberrs @emma_pelland damn set...\n",
       "15      rt @dave451972 @senschumer @senatemajldr repro...\n",
       "16      rt @nicol_transport busy week @volvotrucksuk a...\n",
       "17      best player premier league hazard go ğŸ¤¯ ğŸ’¥ https...\n",
       "18      rt @viduch2361 @maga2arights good morning mari...\n",
       "19      rt @cbcmurdoch join wish incredible @yannick_b...\n",
       "20               @lawrence_selema well brag different ğŸ˜‚ ğŸ˜‚\n",
       "21                                   know hard keep g ğŸ‘ ğŸ½\n",
       "22                 @martingeddes guy go dc july 4th ğŸ† ğŸ‡º ğŸ‡¸\n",
       "23      rt @djkt229 damn wish could go back play shit ...\n",
       "24      want go jet boat today available trip lewiston...\n",
       "25      rt @jadaayvette ryan destiny amp justine skye ...\n",
       "26      rt @footbaii_hq ashley young ğŸ˜‚ ğŸ˜‚ ğŸ˜‚ https://t.c...\n",
       "27      ya'll check blog https://t.co/5aasn0bf8n welco...\n",
       "28      rt @giftsinternatio longweekend launch may giv...\n",
       "29      @specialkbrook @kidgalahad90 fave uk boxer kel...\n",
       "                              ...                        \n",
       "3068    first 10 follow like retweet get 1500 usa uk c...\n",
       "3069    rt @ladydonli say addict cash 3 time front mir...\n",
       "3070    oh myyyyy ğŸ˜­ ğŸ˜­ ğŸ˜­ go cryyyyyy hanbin blonde hair...\n",
       "3071    rt @thmarchuk â­ bestfriendsday â­ celebrate giv...\n",
       "3072    finally get chance watch entire series mary ja...\n",
       "3073    @_robinhunt surprise harpy something far bad ğŸ˜¨...\n",
       "3074    play wembley arena 4 month ago today ğŸ¤¯ gratefu...\n",
       "3075    @colourpopco omg would legit die obsess guy ne...\n",
       "3076                           @lelesunnysideup u cutie ğŸ¤§\n",
       "3077    rt @nctsmtown_127 ğŸƒ ğŸŒ¾ nct 127 ã€– superhuman ã€— m...\n",
       "3078    @jamesenglish0 well play matter come anything ...\n",
       "3079    icymi illinois pass important bill reproductiv...\n",
       "3080    believe pick fabian delph kid crowd n bang mid...\n",
       "3081    rt @falanamusic backstage throwback last show ...\n",
       "3082    rt @usynt ğŸ”Š â€œ know goal go come keep play game...\n",
       "3083                  heyheyhey ğŸ˜‚ https://t.co/om3jnsa1vk\n",
       "3084    pass day love grow two queen ğŸ‘¸ ğŸ» ğŸ‘¸ ğŸ’• ğŸ‡§ ğŸ‡· ğŸ‡º ğŸ‡¸ â¤...\n",
       "3085    rt @theworkperk chance win amazing scotch Â® ta...\n",
       "3086    ğŸ”¥ 3rd charlottetown ave piece cardboard report...\n",
       "3087    finally get mf olive garden tomorrow w/ sister...\n",
       "3088      need somebody world full sin understand fully â™¾\n",
       "3089    rt @ggukgcfilm tu vecina wonder quite latelyyy...\n",
       "3090    @meganbramham oh gun basic american right ğŸ’ª ab...\n",
       "3091          í£ like whole concept ğŸ¤¯ mindblowe shit right\n",
       "3092    rt @justintraver3 get behind satan ğŸ˜ˆ @johnbren...\n",
       "3093    @jenx78019735 yes thank true hard remember som...\n",
       "3094    chicago ready party riptide tour @beatkitchenb...\n",
       "3095    rt @urstrulymahesh unbelievable energy oval .....\n",
       "3096    rt @abdulraufkhamis heart take picture see eid...\n",
       "3097    rt @amandajbrittany ronan ronan good chop fire...\n",
       "Length: 3098, dtype: object"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tw_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "tw = [word for word in tweets['tweet']]\n",
    "\n",
    "# Clean up all text\n",
    "tw_clean = cleanup_text(tw)\n",
    "tw_clean = ' '.join(tw_clean).split()\n",
    "# 's appears a lot in the text, so we get rid of it since it's not a word\n",
    "tw_clean = [word for word in tw_clean if word not in ['\\'s','â€™s','rt','â€¦','ï¸','...']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'John Daly is actually riding a cart at a major ğŸ˜‚ this man is living is best life'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# original\n",
    "tweets['tweet'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['john',\n",
       " 'daly',\n",
       " 'actually',\n",
       " 'ride',\n",
       " 'cart',\n",
       " 'major',\n",
       " 'ğŸ˜‚',\n",
       " 'man',\n",
       " 'live',\n",
       " 'good',\n",
       " 'life']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tokenized\n",
    "tw_clean[0:11]\n",
    "# noticing best became good "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 1500x1200 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "# Plot top 25 most frequently occuring words \n",
    "tw_counts = Counter(tw_clean)\n",
    "tw_common_words = [word[0] for word in tw_counts.most_common(25)]\n",
    "tw_common_counts = [word[1] for word in tw_counts.most_common(25)]\n",
    "\n",
    "plt.figure(figsize=(15, 12))\n",
    "\n",
    "sns.barplot(x=tw_common_words, y=tw_common_counts)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ğŸ‡¸',\n",
       " 'ğŸ‡º',\n",
       " 'ğŸ˜‚',\n",
       " 'ğŸ˜',\n",
       " '\\U0001f92f',\n",
       " 'ğŸ‡·',\n",
       " 'ğŸ‡ª',\n",
       " 'ğŸ˜­',\n",
       " 'ğŸ”¥',\n",
       " 'â¤',\n",
       " 'get',\n",
       " 'like',\n",
       " 'go',\n",
       " 'ğŸ™Š',\n",
       " 'ğŸ‡¦',\n",
       " 'ğŸ‡°',\n",
       " 'fire',\n",
       " 'ğŸ‡³',\n",
       " 'ğŸ‡©',\n",
       " 'love',\n",
       " 'good',\n",
       " 'ğŸ‡¬',\n",
       " 'ğŸ†',\n",
       " 'ğŸ‘',\n",
       " 'ğŸ‡¹']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tw_common_words\n",
    "# \\U0001f92f is the ğŸ¤¯ (exploding head emoji)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    ğŸ‡º\n",
       "1    ğŸ‡¸\n",
       "dtype: object"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Hmmm flags are turning into two emojis...\n",
    "cleanup_text('ğŸ‡ºğŸ‡¸')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gensim.models.Word2Vec(sentences, min_count=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
