{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "#from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "#from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS\n",
    "#from sklearn.decomposition import NMF\n",
    "import math\n",
    "from collections import Counter\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import nltk\n",
    "import nltk.tokenize as tk\n",
    "import spacy\n",
    "import string\n",
    "import en_core_web_sm\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "nlp = en_core_web_sm.load()\n",
    "\n",
    "stopwords = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = np.array(list(pickle.load(open('./data/yay_moji.pkl','rb'))))\n",
    "mojis = pd.read_pickle('./data/df_emojis.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = pd.DataFrame(tweets, columns=['tweet'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean text before feeding it to spaCy\n",
    "punctuations = string.punctuation\n",
    "\n",
    "# Define function to cleanup text by removing personal pronouns, stopwords, and puncuation\n",
    "def cleanup_text(docs, logging=False):\n",
    "    texts = []\n",
    "    counter = 1\n",
    "    for doc in docs:\n",
    "        if counter % 1000 == 0 and logging:\n",
    "            print(\"Processed %d out of %d documents.\" % (counter, len(docs)))\n",
    "        counter += 1\n",
    "        doc = nlp(doc, disable=['parser', 'ner'])\n",
    "        tokens = [tok.lemma_.lower().strip() for tok in doc if tok.lemma_ != '-PRON-']\n",
    "        tokens = [tok for tok in tokens if tok not in stopwords and tok not in punctuations]\n",
    "        tokens = ' '.join(tokens)\n",
    "        texts.append(tokens)\n",
    "    return pd.Series(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "tw = [word for word in tweets['tweet']]\n",
    "\n",
    "# Clean up all text\n",
    "tw_clean = cleanup_text(tw)\n",
    "tw_clean = ' '.join(tw_clean).split()\n",
    "# 's appears a lot in the text, so we get rid of it since it's not a word\n",
    "tw_clean = [word for word in tw_clean if word != '\\'s']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['john',\n",
       " 'daly',\n",
       " 'actually',\n",
       " 'ride',\n",
       " 'cart',\n",
       " 'major',\n",
       " '😂',\n",
       " 'man',\n",
       " 'live',\n",
       " 'good']"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tw_clean[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'John Daly is actually riding a cart at a major 😂 this man is living is best life'"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------\n",
      "Group: 0\n",
      "54 tweets\n",
      "--> btsongma\n",
      "--> gma\n",
      "--> bts_twt\n",
      "--> army\n",
      "--> ready\n",
      "--> bts\n",
      "--> bts army\n",
      "--> gma ready\n",
      "--> ready btsongma\n",
      "--> btsongma bts\n",
      "💜\n",
      "💜\n",
      "💜\n",
      "💜\n",
      "💜\n",
      "💜\n",
      "💜\n",
      "💜\n",
      "🌋\n",
      "🔥\n",
      "😂\n",
      "\n",
      "\n",
      "----------\n",
      "Group: 1\n",
      "2 tweets\n",
      "--> prettymai_0105 carl05290\n",
      "--> diosarmendoza\n",
      "--> chie_chie26\n",
      "--> axle1809 chie_chie26\n",
      "--> axle1809\n",
      "--> darwaine88 prettymai_0105\n",
      "--> darwaine88\n",
      "--> lynieg88\n",
      "--> prettymai_0105\n",
      "--> carl05290\n",
      "😘\n",
      "😘\n",
      "🐭\n",
      "😂\n",
      "😂\n",
      "😂\n",
      "😂\n",
      "😭\n",
      "💩\n",
      "😰\n",
      "😂\n",
      "🌓\n",
      "🍌\n",
      "\n",
      "\n",
      "----------\n",
      "Group: 2\n",
      "45 tweets\n",
      "--> derby\n",
      "--> leeds\n",
      "--> lampard\n",
      "--> derby players\n",
      "--> players\n",
      "--> gesture\n",
      "--> sportbible\n",
      "--> spying gesture\n",
      "--> spying\n",
      "--> stop\n",
      "👀\n",
      "😂\n",
      "😍\n",
      "😂\n",
      "🐑\n",
      "😍\n",
      "💪\n",
      "😂\n",
      "🙌\n",
      "😂\n",
      "🚨\n",
      "\n",
      "\n",
      "----------\n",
      "Group: 3\n",
      "40 tweets\n",
      "--> love\n",
      "--> yes love\n",
      "--> breakingdawnon405 love\n",
      "--> breakingdawnon405\n",
      "--> yes\n",
      "--> ok\n",
      "--> screenshotted\n",
      "--> thank love\n",
      "--> try\n",
      "--> try thank\n",
      "👌\n",
      "🍓\n",
      "💜\n",
      "😭\n",
      "😍\n",
      "😍\n",
      "👇\n",
      "❤\n",
      "✨\n",
      "❤\n",
      "❤\n",
      "😍\n",
      "😘\n",
      "🎼\n",
      "\n",
      "\n",
      "----------\n",
      "Group: 4\n",
      "55 tweets\n",
      "--> say\n",
      "--> just say\n",
      "--> shit\n",
      "--> child\n",
      "--> shit bjznweh05k\n",
      "--> say shit\n",
      "--> jzizzzle_ just\n",
      "--> bjznweh05k\n",
      "--> jzizzzle_\n",
      "--> bad\n",
      "😊\n",
      "😭\n",
      "😂\n",
      "😂\n",
      "🍆\n",
      "🍑\n",
      "💦\n",
      "😂\n",
      "🎥\n",
      "💥\n",
      "🙏\n",
      "🚨\n",
      "👨\n",
      "👩\n",
      "😂\n",
      "\n",
      "\n",
      "----------\n",
      "Group: 5\n",
      "92 tweets\n",
      "--> got\n",
      "--> going\n",
      "--> girl\n",
      "--> just going\n",
      "--> good\n",
      "--> meme\n",
      "--> 08q1zvex5r\n",
      "--> stitches girl\n",
      "--> stitches\n",
      "--> girl 08q1zvex5r\n",
      "😂\n",
      "🔥\n",
      "💎\n",
      "😂\n",
      "🚮\n",
      "😭\n",
      "😂\n",
      "😂\n",
      "🇺🇸\n",
      "😳\n",
      "🌚\n",
      "💀\n",
      "\n",
      "\n",
      "----------\n",
      "Group: 6\n",
      "66 tweets\n",
      "--> hope\n",
      "--> today\n",
      "--> best\n",
      "--> day\n",
      "--> thank\n",
      "--> tomorrow\n",
      "--> gift\n",
      "--> best dkzjoewm4m\n",
      "--> today tomorrow\n",
      "--> dkzjoewm4m\n",
      "♥\n",
      "❤\n",
      "💞\n",
      "😔\n",
      "😍\n",
      "❤\n",
      "😊\n",
      "🙌\n",
      "💚\n",
      "💙\n",
      "😭\n",
      "♥\n",
      "😍\n",
      "\n",
      "\n",
      "----------\n",
      "Group: 7\n",
      "60 tweets\n",
      "--> new\n",
      "--> buy\n",
      "--> free\n",
      "--> wait\n",
      "--> wait buy\n",
      "--> new lingerie\n",
      "--> lingerie\n",
      "--> buy new\n",
      "--> buy free\n",
      "--> gift\n",
      "😍\n",
      "🔥\n",
      "😅\n",
      "😘\n",
      "💕\n",
      "😂\n",
      "😂\n",
      "💜\n",
      "😒\n",
      "😊\n",
      "😎\n",
      "\n",
      "\n",
      "----------\n",
      "Group: 8\n",
      "52 tweets\n",
      "--> think\n",
      "--> funny\n",
      "--> worst\n",
      "--> need\n",
      "--> bitches\n",
      "--> think half\n",
      "--> daintymongeau funny\n",
      "--> daintymongeau\n",
      "--> tanamongeau daintymongeau\n",
      "--> tanamongeau\n",
      "👍\n",
      "😂\n",
      "😂\n",
      "🇺🇸\n",
      "😂\n",
      "😨\n",
      "😭\n",
      "💛\n",
      "😭\n",
      "💀\n",
      "\n",
      "\n",
      "----------\n",
      "Group: 9\n",
      "74 tweets\n",
      "--> time\n",
      "--> know\n",
      "--> win\n",
      "--> giveaway\n",
      "--> qfjdwzajqs\n",
      "--> nct127oncorden qfjdwzajqs\n",
      "--> nct127oncorden\n",
      "--> latelateshow\n",
      "--> time nct127oncorden\n",
      "--> latelateshow time\n",
      "🍑\n",
      "💞\n",
      "😍\n",
      "✌\n",
      "😂\n",
      "💔\n",
      "😍\n",
      "😭\n",
      "😂\n",
      "🌍\n",
      "🐦\n",
      "🎉\n",
      "😭\n",
      "😬\n",
      "\n",
      "\n",
      "😂 65\n",
      "😍 25\n",
      "😭 24\n",
      "💜 11\n",
      "❤ 11\n",
      "🔥 10\n",
      "😘 8\n",
      "👀 6\n",
      "🚨 6\n",
      "🙌 5\n",
      "♥ 5\n",
      "🙏 4\n",
      "😔 4\n",
      "👏 4\n",
      "💛 4\n",
      "😊 4\n",
      "😅 4\n",
      "👅 3\n",
      "😫 3\n",
      "😤 3\n",
      "💔 3\n",
      "✨ 3\n",
      "💪 3\n",
      "💰 3\n",
      "‼ 3\n",
      "🇺🇸 3\n",
      "😈 2\n",
      "☝ 2\n",
      "✊ 2\n",
      "😌 2\n",
      "💯 2\n",
      "🌙 2\n",
      "🌟 2\n",
      "✌ 2\n",
      "🐰 2\n",
      "💚 2\n",
      "😳 2\n",
      "➡ 2\n",
      "🔔 2\n",
      "👍 2\n",
      "✅ 2\n",
      "💙 2\n",
      "💥 2\n",
      "💦 2\n",
      "💀 2\n",
      "☺ 2\n",
      "🎥 2\n",
      "😩 2\n",
      "💞 2\n",
      "👇 2\n",
      "😈\n",
      "\n",
      "👨\n",
      "👩\n",
      "\n",
      "💜\n",
      "\n",
      "🙌\n",
      "\n",
      "👅\n",
      "\n",
      "😂\n",
      "\n",
      "⚽\n",
      "\n",
      "🐦\n",
      "\n",
      "❤\n",
      "\n",
      "😂\n",
      "\n",
      "™\n",
      "\n",
      "☝\n",
      "✊\n",
      "😂\n",
      "😌\n",
      "😛\n",
      "😫\n",
      "🙈\n",
      "\n",
      "😤\n",
      "\n",
      "😱\n",
      "\n",
      "🙏\n",
      "\n",
      "😤\n",
      "\n",
      "😍\n",
      "😫\n",
      "😭\n",
      "\n",
      "😂\n",
      "\n",
      "💯\n",
      "📡\n",
      "😃\n",
      "😍\n",
      "😘\n",
      "\n",
      "😔\n",
      "\n",
      "💔\n",
      "😂\n",
      "\n",
      "🌙\n",
      "🌟\n",
      "\n",
      "😂\n",
      "\n",
      "👀\n",
      "\n",
      "🌍\n",
      "\n",
      "😭\n",
      "\n",
      "✌\n",
      "🐰\n",
      "👶\n",
      "\n",
      "😔\n",
      "\n",
      "😍\n",
      "\n",
      "😂\n",
      "\n",
      "😂\n",
      "\n",
      "💚\n",
      "\n",
      "👀\n",
      "🔎\n",
      "\n",
      "💔\n",
      "\n",
      "👊\n",
      "\n",
      "😭\n",
      "\n",
      "😂\n",
      "😭\n",
      "\n",
      "❤\n",
      "\n",
      "😂\n",
      "\n",
      "😂\n",
      "\n",
      "😂\n",
      "\n",
      "👏\n",
      "\n",
      "😭\n",
      "\n",
      "✨\n",
      "\n",
      "😂\n",
      "\n",
      "😂\n",
      "\n",
      "😂\n",
      "\n",
      "😳\n",
      "\n",
      "😍\n",
      "\n",
      "💛\n",
      "\n",
      "💪\n",
      "\n",
      "😍\n",
      "\n",
      "♥\n",
      "🐰\n",
      "💜\n",
      "\n",
      "♥\n",
      "🙏\n",
      "\n",
      "😊\n",
      "\n",
      "😂\n",
      "\n",
      "😍\n",
      "\n",
      "🐑\n",
      "\n",
      "➡\n",
      "\n",
      "📋\n",
      "\n",
      "🐲\n",
      "🔔\n",
      "🔥\n",
      "\n",
      "💪\n",
      "\n",
      "👍\n",
      "\n",
      "😅\n",
      "\n",
      "⬇\n",
      "💻\n",
      "📱\n",
      "\n",
      "😂\n",
      "\n",
      "😂\n",
      "\n",
      "❌\n",
      "💰\n",
      "\n",
      "⏳\n",
      "✅\n",
      "👉\n",
      "💛\n",
      "💰\n",
      "\n",
      "💜\n",
      "\n",
      "😅\n",
      "\n",
      "💙\n",
      "😭\n",
      "\n",
      "👀\n",
      "\n",
      "👌\n",
      "\n",
      "😘\n",
      "\n",
      "💙\n",
      "😋\n",
      "\n",
      "😍\n",
      "\n",
      "😍\n",
      "\n",
      "💜\n",
      "\n",
      "😓\n",
      "\n",
      "‼\n",
      "💥\n",
      "\n",
      "📖\n",
      "\n",
      "✔\n",
      "👀\n",
      "👅\n",
      "\n",
      "😅\n",
      "\n",
      "🔥\n",
      "\n",
      "😘\n",
      "\n",
      "❤\n",
      "😊\n",
      "\n",
      "💦\n",
      "😍\n",
      "\n",
      "😭\n",
      "\n",
      "😭\n",
      "\n",
      "💜\n",
      "\n",
      "🎼\n",
      "\n",
      "😂\n",
      "\n",
      "😂\n",
      "😭\n",
      "\n",
      "🔥\n",
      "\n",
      "🍯\n",
      "🐻\n",
      "\n",
      "😂\n",
      "\n",
      "😂\n",
      "😈\n",
      "\n",
      "💁\n",
      "\n",
      "📄\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def find_emoji(word,df_emoji):\n",
    "    options = []\n",
    "    for i, name in enumerate(df_emoji['short_name']):\n",
    "        if word in name:\n",
    "            options.append((name,df_emoji['unichar'][i]))\n",
    "            print(name,df_emoji['unichar'][i])\n",
    "    return options\n",
    "\n",
    "def get_emojis(tweet_lst,df_emoji):\n",
    "    emoji_idx = []\n",
    "    emoji_char =[]\n",
    "    for tweet in tweet_lst:\n",
    "        for i,uni in enumerate(df_emoji['unichar']):\n",
    "            if uni in tweet:\n",
    "                 emoji_idx.append(i)\n",
    "                 emoji_char.append(uni)\n",
    "    return emoji_idx, emoji_char\n",
    "\n",
    "def get_emojis_by_tweet(tweet_lst,df_emoji):\n",
    "    by_tweet = []\n",
    "\n",
    "    for tweet in tweet_lst:\n",
    "        emoji_char =[]\n",
    "        for i,uni in enumerate(df_emoji['unichar']):\n",
    "            if uni in tweet:\n",
    "                 emoji_char.append(uni)\n",
    "        by_tweet.append(emoji_char)\n",
    "    return by_tweet\n",
    "\n",
    "#this seems to get some emojis that i dont but also missed some that i do. It also get duplicates per tweet that i dont\n",
    "# def get_emojis_2(tweet_lst):\n",
    "#     emojis = []\n",
    "#     for tweet in tweet_lst:\n",
    "#         emoji = re.compile(u'[\\uD800-\\uDBFF][\\uDC00-\\uDFFF]')\n",
    "#         emojis.append(emoji.findall(tweet))\n",
    "#     return emojis\n",
    "\n",
    "def print_emoji(tweet,emoji_char):\n",
    "    for uni in emoji_char:\n",
    "        if uni in tweet:\n",
    "            print(uni),\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    tweets = np.array(list(pickle.load(open('./data/yay_moji.pkl','rb'))))\n",
    "    # type is list\n",
    "\n",
    "    emojis = pd.read_pickle('./data/df_emojis.pkl')\n",
    "    # type is DataFrame\n",
    "\n",
    "\n",
    "\n",
    "    # ------------- tfidf\n",
    "    stopwords = set(list(ENGLISH_STOP_WORDS) + ['rt', 'follow', 'dm', 'https', 'ur', 'll' ,'amp', 'subscribe', 'don', 've', 'retweet', 'im', 'http'])\n",
    "\n",
    "    tfidf = TfidfVectorizer(max_features=10000, max_df=0.05, min_df=0.001, stop_words = stopwords, ngram_range = (1,2))\n",
    "\n",
    "    #lemmetizing need to consider cleaning the tweets myself\n",
    "\n",
    "    tfidf_tweets = tfidf.fit_transform(tweets)\n",
    "    bag = np.array(tfidf.get_feature_names())\n",
    "\n",
    "    # -------------- NMF\n",
    "    k = 10\n",
    "     #number of groups\n",
    "    nmf2 = NMF(n_components = k)\n",
    "    nmf2.fit(tfidf_tweets)\n",
    "    W = nmf2.transform(tfidf_tweets) #len(yay_moji,k)\n",
    "    H = nmf2.components_ #k,len(yay_moji)\n",
    "\n",
    "\n",
    "    # --------------- Printing Top 10\n",
    "    tweet_lst = []\n",
    "    top = 10\n",
    "    tweet_in_group_thresh = .001 #score thresh if we consider that tweet as part of that group\n",
    "    for group in range(k):\n",
    "        #idx of the top ten words for each group\n",
    "        i_words = np.argsort(H[group])[::-1][:top]\n",
    "        words = bag[i_words]\n",
    "\n",
    "        # idx of the top ten tweets for each group\n",
    "        i_emojis = np.argsort(W[:,group])[::-1][:top]\n",
    "        # most common 10 emojis for each group\n",
    "\n",
    "        print('-'*10)\n",
    "        print('Group:',group)\n",
    "        counted_tweets = np.argwhere(W[:,group] > tweet_in_group_thresh)\n",
    "        print(counted_tweets.shape[0], 'tweets')\n",
    "        for word in words:\n",
    "            print('-->',word)\n",
    "        for i_tweet in i_emojis:\n",
    "            print_emoji(tweets[i_tweet], emojis['unichar'])\n",
    "            tweet_lst.append(tweets[i_tweet])\n",
    "        ind, emo_lst = get_emojis(tweets[i_emojis],emojis)\n",
    "        # find percentage of emoji per group\n",
    "        most_emoji, how_many = Counter(emo_lst).most_common(1)[0]\n",
    "        score = float(how_many)/top\n",
    "        # print score #score is not perfect - similar emojis and repeat in the same tweet\n",
    "        print('\\n')\n",
    "\n",
    "    # --------------- printing most common emojis\n",
    "    most_common = 50\n",
    "    b,all_emojis = get_emojis(tweets,emojis)\n",
    "    count = Counter(all_emojis).most_common(most_common)\n",
    "    unicode_top = []\n",
    "    for emo, i in count:\n",
    "        print(emo,i)\n",
    "#        for j, char in enumerate(emojis['unichar']):\n",
    "#            if char == emo:\n",
    "#                unicode_top.append(emojis['unified'][j])\n",
    "\n",
    "\n",
    "# test stuff\n",
    "    jan = get_emojis_by_tweet(tweets[0:100],emojis)\n",
    "    for tweet in jan:\n",
    "        for emo in tweet:\n",
    "            print(emo),\n",
    "        print('')\n",
    "    # name_of = find_emoji('heart',emojis)\n",
    "\n",
    "\n",
    "    '''\n",
    "    to do's\n",
    "    --> how big are the groups? do a most common\n",
    "    --> get a better score system\n",
    "    --> allow for tweets with multiple emojis\n",
    "    --> sub set for tweets with a specific emoji\n",
    "    --> commonly combined emojis\n",
    "    --> naive bayes\n",
    "        prediction accuracy between emojis for how similar they are\n",
    "    whats the purpose:\n",
    "    --> to help use emojis as labels for tweets\n",
    "    '''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
