{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "#from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "#from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS\n",
    "#from sklearn.decomposition import NMF\n",
    "import math\n",
    "from collections import Counter\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import nltk\n",
    "import nltk.tokenize as tk\n",
    "import spacy\n",
    "import string\n",
    "import en_core_web_sm\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "nlp = en_core_web_sm.load()\n",
    "\n",
    "stopwords = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = np.array(list(pickle.load(open('./data/yay_moji.pkl','rb'))))\n",
    "mojis = pd.read_pickle('./data/df_emojis.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = pd.DataFrame(tweets, columns=['tweet'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean text before feeding it to spaCy\n",
    "punctuations = string.punctuation\n",
    "\n",
    "# Define function to cleanup text by removing personal pronouns, stopwords, and puncuation\n",
    "def cleanup_text(docs, logging=False):\n",
    "    texts = []\n",
    "    counter = 1\n",
    "    for doc in docs:\n",
    "        if counter % 1000 == 0 and logging:\n",
    "            print(\"Processed %d out of %d documents.\" % (counter, len(docs)))\n",
    "        counter += 1\n",
    "        doc = nlp(doc, disable=['parser', 'ner'])\n",
    "        tokens = [tok.lemma_.lower().strip() for tok in doc if tok.lemma_ != '-PRON-']\n",
    "        tokens = [tok for tok in tokens if tok not in stopwords and tok not in punctuations]\n",
    "        tokens = ' '.join(tokens)\n",
    "        texts.append(tokens)\n",
    "    return pd.Series(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "tw = [word for word in tweets['tweet']]\n",
    "\n",
    "# Clean up all text\n",
    "tw_clean = cleanup_text(tw)\n",
    "tw_clean = ' '.join(tw_clean).split()\n",
    "# 's appears a lot in the text, so we get rid of it since it's not a word\n",
    "tw_clean = [word for word in tw_clean if word != '\\'s']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['john',\n",
       " 'daly',\n",
       " 'actually',\n",
       " 'ride',\n",
       " 'cart',\n",
       " 'major',\n",
       " 'ðŸ˜‚',\n",
       " 'man',\n",
       " 'live',\n",
       " 'good']"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tw_clean[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'John Daly is actually riding a cart at a major ðŸ˜‚ this man is living is best life'"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------\n",
      "Group: 0\n",
      "54 tweets\n",
      "--> btsongma\n",
      "--> gma\n",
      "--> bts_twt\n",
      "--> army\n",
      "--> ready\n",
      "--> bts\n",
      "--> bts army\n",
      "--> gma ready\n",
      "--> ready btsongma\n",
      "--> btsongma bts\n",
      "ðŸ’œ\n",
      "ðŸ’œ\n",
      "ðŸ’œ\n",
      "ðŸ’œ\n",
      "ðŸ’œ\n",
      "ðŸ’œ\n",
      "ðŸ’œ\n",
      "ðŸ’œ\n",
      "ðŸŒ‹\n",
      "ðŸ”¥\n",
      "ðŸ˜‚\n",
      "\n",
      "\n",
      "----------\n",
      "Group: 1\n",
      "2 tweets\n",
      "--> prettymai_0105 carl05290\n",
      "--> diosarmendoza\n",
      "--> chie_chie26\n",
      "--> axle1809 chie_chie26\n",
      "--> axle1809\n",
      "--> darwaine88 prettymai_0105\n",
      "--> darwaine88\n",
      "--> lynieg88\n",
      "--> prettymai_0105\n",
      "--> carl05290\n",
      "ðŸ˜˜\n",
      "ðŸ˜˜\n",
      "ðŸ­\n",
      "ðŸ˜‚\n",
      "ðŸ˜‚\n",
      "ðŸ˜‚\n",
      "ðŸ˜‚\n",
      "ðŸ˜­\n",
      "ðŸ’©\n",
      "ðŸ˜°\n",
      "ðŸ˜‚\n",
      "ðŸŒ“\n",
      "ðŸŒ\n",
      "\n",
      "\n",
      "----------\n",
      "Group: 2\n",
      "45 tweets\n",
      "--> derby\n",
      "--> leeds\n",
      "--> lampard\n",
      "--> derby players\n",
      "--> players\n",
      "--> gesture\n",
      "--> sportbible\n",
      "--> spying gesture\n",
      "--> spying\n",
      "--> stop\n",
      "ðŸ‘€\n",
      "ðŸ˜‚\n",
      "ðŸ˜\n",
      "ðŸ˜‚\n",
      "ðŸ‘\n",
      "ðŸ˜\n",
      "ðŸ’ª\n",
      "ðŸ˜‚\n",
      "ðŸ™Œ\n",
      "ðŸ˜‚\n",
      "ðŸš¨\n",
      "\n",
      "\n",
      "----------\n",
      "Group: 3\n",
      "40 tweets\n",
      "--> love\n",
      "--> yes love\n",
      "--> breakingdawnon405 love\n",
      "--> breakingdawnon405\n",
      "--> yes\n",
      "--> ok\n",
      "--> screenshotted\n",
      "--> thank love\n",
      "--> try\n",
      "--> try thank\n",
      "ðŸ‘Œ\n",
      "ðŸ“\n",
      "ðŸ’œ\n",
      "ðŸ˜­\n",
      "ðŸ˜\n",
      "ðŸ˜\n",
      "ðŸ‘‡\n",
      "â¤\n",
      "âœ¨\n",
      "â¤\n",
      "â¤\n",
      "ðŸ˜\n",
      "ðŸ˜˜\n",
      "ðŸŽ¼\n",
      "\n",
      "\n",
      "----------\n",
      "Group: 4\n",
      "55 tweets\n",
      "--> say\n",
      "--> just say\n",
      "--> shit\n",
      "--> child\n",
      "--> shit bjznweh05k\n",
      "--> say shit\n",
      "--> jzizzzle_ just\n",
      "--> bjznweh05k\n",
      "--> jzizzzle_\n",
      "--> bad\n",
      "ðŸ˜Š\n",
      "ðŸ˜­\n",
      "ðŸ˜‚\n",
      "ðŸ˜‚\n",
      "ðŸ†\n",
      "ðŸ‘\n",
      "ðŸ’¦\n",
      "ðŸ˜‚\n",
      "ðŸŽ¥\n",
      "ðŸ’¥\n",
      "ðŸ™\n",
      "ðŸš¨\n",
      "ðŸ‘¨\n",
      "ðŸ‘©\n",
      "ðŸ˜‚\n",
      "\n",
      "\n",
      "----------\n",
      "Group: 5\n",
      "92 tweets\n",
      "--> got\n",
      "--> going\n",
      "--> girl\n",
      "--> just going\n",
      "--> good\n",
      "--> meme\n",
      "--> 08q1zvex5r\n",
      "--> stitches girl\n",
      "--> stitches\n",
      "--> girl 08q1zvex5r\n",
      "ðŸ˜‚\n",
      "ðŸ”¥\n",
      "ðŸ’Ž\n",
      "ðŸ˜‚\n",
      "ðŸš®\n",
      "ðŸ˜­\n",
      "ðŸ˜‚\n",
      "ðŸ˜‚\n",
      "ðŸ‡ºðŸ‡¸\n",
      "ðŸ˜³\n",
      "ðŸŒš\n",
      "ðŸ’€\n",
      "\n",
      "\n",
      "----------\n",
      "Group: 6\n",
      "66 tweets\n",
      "--> hope\n",
      "--> today\n",
      "--> best\n",
      "--> day\n",
      "--> thank\n",
      "--> tomorrow\n",
      "--> gift\n",
      "--> best dkzjoewm4m\n",
      "--> today tomorrow\n",
      "--> dkzjoewm4m\n",
      "â™¥\n",
      "â¤\n",
      "ðŸ’ž\n",
      "ðŸ˜”\n",
      "ðŸ˜\n",
      "â¤\n",
      "ðŸ˜Š\n",
      "ðŸ™Œ\n",
      "ðŸ’š\n",
      "ðŸ’™\n",
      "ðŸ˜­\n",
      "â™¥\n",
      "ðŸ˜\n",
      "\n",
      "\n",
      "----------\n",
      "Group: 7\n",
      "60 tweets\n",
      "--> new\n",
      "--> buy\n",
      "--> free\n",
      "--> wait\n",
      "--> wait buy\n",
      "--> new lingerie\n",
      "--> lingerie\n",
      "--> buy new\n",
      "--> buy free\n",
      "--> gift\n",
      "ðŸ˜\n",
      "ðŸ”¥\n",
      "ðŸ˜…\n",
      "ðŸ˜˜\n",
      "ðŸ’•\n",
      "ðŸ˜‚\n",
      "ðŸ˜‚\n",
      "ðŸ’œ\n",
      "ðŸ˜’\n",
      "ðŸ˜Š\n",
      "ðŸ˜Ž\n",
      "\n",
      "\n",
      "----------\n",
      "Group: 8\n",
      "52 tweets\n",
      "--> think\n",
      "--> funny\n",
      "--> worst\n",
      "--> need\n",
      "--> bitches\n",
      "--> think half\n",
      "--> daintymongeau funny\n",
      "--> daintymongeau\n",
      "--> tanamongeau daintymongeau\n",
      "--> tanamongeau\n",
      "ðŸ‘\n",
      "ðŸ˜‚\n",
      "ðŸ˜‚\n",
      "ðŸ‡ºðŸ‡¸\n",
      "ðŸ˜‚\n",
      "ðŸ˜¨\n",
      "ðŸ˜­\n",
      "ðŸ’›\n",
      "ðŸ˜­\n",
      "ðŸ’€\n",
      "\n",
      "\n",
      "----------\n",
      "Group: 9\n",
      "74 tweets\n",
      "--> time\n",
      "--> know\n",
      "--> win\n",
      "--> giveaway\n",
      "--> qfjdwzajqs\n",
      "--> nct127oncorden qfjdwzajqs\n",
      "--> nct127oncorden\n",
      "--> latelateshow\n",
      "--> time nct127oncorden\n",
      "--> latelateshow time\n",
      "ðŸ‘\n",
      "ðŸ’ž\n",
      "ðŸ˜\n",
      "âœŒ\n",
      "ðŸ˜‚\n",
      "ðŸ’”\n",
      "ðŸ˜\n",
      "ðŸ˜­\n",
      "ðŸ˜‚\n",
      "ðŸŒ\n",
      "ðŸ¦\n",
      "ðŸŽ‰\n",
      "ðŸ˜­\n",
      "ðŸ˜¬\n",
      "\n",
      "\n",
      "ðŸ˜‚ 65\n",
      "ðŸ˜ 25\n",
      "ðŸ˜­ 24\n",
      "ðŸ’œ 11\n",
      "â¤ 11\n",
      "ðŸ”¥ 10\n",
      "ðŸ˜˜ 8\n",
      "ðŸ‘€ 6\n",
      "ðŸš¨ 6\n",
      "ðŸ™Œ 5\n",
      "â™¥ 5\n",
      "ðŸ™ 4\n",
      "ðŸ˜” 4\n",
      "ðŸ‘ 4\n",
      "ðŸ’› 4\n",
      "ðŸ˜Š 4\n",
      "ðŸ˜… 4\n",
      "ðŸ‘… 3\n",
      "ðŸ˜« 3\n",
      "ðŸ˜¤ 3\n",
      "ðŸ’” 3\n",
      "âœ¨ 3\n",
      "ðŸ’ª 3\n",
      "ðŸ’° 3\n",
      "â€¼ 3\n",
      "ðŸ‡ºðŸ‡¸ 3\n",
      "ðŸ˜ˆ 2\n",
      "â˜ 2\n",
      "âœŠ 2\n",
      "ðŸ˜Œ 2\n",
      "ðŸ’¯ 2\n",
      "ðŸŒ™ 2\n",
      "ðŸŒŸ 2\n",
      "âœŒ 2\n",
      "ðŸ° 2\n",
      "ðŸ’š 2\n",
      "ðŸ˜³ 2\n",
      "âž¡ 2\n",
      "ðŸ”” 2\n",
      "ðŸ‘ 2\n",
      "âœ… 2\n",
      "ðŸ’™ 2\n",
      "ðŸ’¥ 2\n",
      "ðŸ’¦ 2\n",
      "ðŸ’€ 2\n",
      "â˜º 2\n",
      "ðŸŽ¥ 2\n",
      "ðŸ˜© 2\n",
      "ðŸ’ž 2\n",
      "ðŸ‘‡ 2\n",
      "ðŸ˜ˆ\n",
      "\n",
      "ðŸ‘¨\n",
      "ðŸ‘©\n",
      "\n",
      "ðŸ’œ\n",
      "\n",
      "ðŸ™Œ\n",
      "\n",
      "ðŸ‘…\n",
      "\n",
      "ðŸ˜‚\n",
      "\n",
      "âš½\n",
      "\n",
      "ðŸ¦\n",
      "\n",
      "â¤\n",
      "\n",
      "ðŸ˜‚\n",
      "\n",
      "â„¢\n",
      "\n",
      "â˜\n",
      "âœŠ\n",
      "ðŸ˜‚\n",
      "ðŸ˜Œ\n",
      "ðŸ˜›\n",
      "ðŸ˜«\n",
      "ðŸ™ˆ\n",
      "\n",
      "ðŸ˜¤\n",
      "\n",
      "ðŸ˜±\n",
      "\n",
      "ðŸ™\n",
      "\n",
      "ðŸ˜¤\n",
      "\n",
      "ðŸ˜\n",
      "ðŸ˜«\n",
      "ðŸ˜­\n",
      "\n",
      "ðŸ˜‚\n",
      "\n",
      "ðŸ’¯\n",
      "ðŸ“¡\n",
      "ðŸ˜ƒ\n",
      "ðŸ˜\n",
      "ðŸ˜˜\n",
      "\n",
      "ðŸ˜”\n",
      "\n",
      "ðŸ’”\n",
      "ðŸ˜‚\n",
      "\n",
      "ðŸŒ™\n",
      "ðŸŒŸ\n",
      "\n",
      "ðŸ˜‚\n",
      "\n",
      "ðŸ‘€\n",
      "\n",
      "ðŸŒ\n",
      "\n",
      "ðŸ˜­\n",
      "\n",
      "âœŒ\n",
      "ðŸ°\n",
      "ðŸ‘¶\n",
      "\n",
      "ðŸ˜”\n",
      "\n",
      "ðŸ˜\n",
      "\n",
      "ðŸ˜‚\n",
      "\n",
      "ðŸ˜‚\n",
      "\n",
      "ðŸ’š\n",
      "\n",
      "ðŸ‘€\n",
      "ðŸ”Ž\n",
      "\n",
      "ðŸ’”\n",
      "\n",
      "ðŸ‘Š\n",
      "\n",
      "ðŸ˜­\n",
      "\n",
      "ðŸ˜‚\n",
      "ðŸ˜­\n",
      "\n",
      "â¤\n",
      "\n",
      "ðŸ˜‚\n",
      "\n",
      "ðŸ˜‚\n",
      "\n",
      "ðŸ˜‚\n",
      "\n",
      "ðŸ‘\n",
      "\n",
      "ðŸ˜­\n",
      "\n",
      "âœ¨\n",
      "\n",
      "ðŸ˜‚\n",
      "\n",
      "ðŸ˜‚\n",
      "\n",
      "ðŸ˜‚\n",
      "\n",
      "ðŸ˜³\n",
      "\n",
      "ðŸ˜\n",
      "\n",
      "ðŸ’›\n",
      "\n",
      "ðŸ’ª\n",
      "\n",
      "ðŸ˜\n",
      "\n",
      "â™¥\n",
      "ðŸ°\n",
      "ðŸ’œ\n",
      "\n",
      "â™¥\n",
      "ðŸ™\n",
      "\n",
      "ðŸ˜Š\n",
      "\n",
      "ðŸ˜‚\n",
      "\n",
      "ðŸ˜\n",
      "\n",
      "ðŸ‘\n",
      "\n",
      "âž¡\n",
      "\n",
      "ðŸ“‹\n",
      "\n",
      "ðŸ²\n",
      "ðŸ””\n",
      "ðŸ”¥\n",
      "\n",
      "ðŸ’ª\n",
      "\n",
      "ðŸ‘\n",
      "\n",
      "ðŸ˜…\n",
      "\n",
      "â¬‡\n",
      "ðŸ’»\n",
      "ðŸ“±\n",
      "\n",
      "ðŸ˜‚\n",
      "\n",
      "ðŸ˜‚\n",
      "\n",
      "âŒ\n",
      "ðŸ’°\n",
      "\n",
      "â³\n",
      "âœ…\n",
      "ðŸ‘‰\n",
      "ðŸ’›\n",
      "ðŸ’°\n",
      "\n",
      "ðŸ’œ\n",
      "\n",
      "ðŸ˜…\n",
      "\n",
      "ðŸ’™\n",
      "ðŸ˜­\n",
      "\n",
      "ðŸ‘€\n",
      "\n",
      "ðŸ‘Œ\n",
      "\n",
      "ðŸ˜˜\n",
      "\n",
      "ðŸ’™\n",
      "ðŸ˜‹\n",
      "\n",
      "ðŸ˜\n",
      "\n",
      "ðŸ˜\n",
      "\n",
      "ðŸ’œ\n",
      "\n",
      "ðŸ˜“\n",
      "\n",
      "â€¼\n",
      "ðŸ’¥\n",
      "\n",
      "ðŸ“–\n",
      "\n",
      "âœ”\n",
      "ðŸ‘€\n",
      "ðŸ‘…\n",
      "\n",
      "ðŸ˜…\n",
      "\n",
      "ðŸ”¥\n",
      "\n",
      "ðŸ˜˜\n",
      "\n",
      "â¤\n",
      "ðŸ˜Š\n",
      "\n",
      "ðŸ’¦\n",
      "ðŸ˜\n",
      "\n",
      "ðŸ˜­\n",
      "\n",
      "ðŸ˜­\n",
      "\n",
      "ðŸ’œ\n",
      "\n",
      "ðŸŽ¼\n",
      "\n",
      "ðŸ˜‚\n",
      "\n",
      "ðŸ˜‚\n",
      "ðŸ˜­\n",
      "\n",
      "ðŸ”¥\n",
      "\n",
      "ðŸ¯\n",
      "ðŸ»\n",
      "\n",
      "ðŸ˜‚\n",
      "\n",
      "ðŸ˜‚\n",
      "ðŸ˜ˆ\n",
      "\n",
      "ðŸ’\n",
      "\n",
      "ðŸ“„\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def find_emoji(word,df_emoji):\n",
    "    options = []\n",
    "    for i, name in enumerate(df_emoji['short_name']):\n",
    "        if word in name:\n",
    "            options.append((name,df_emoji['unichar'][i]))\n",
    "            print(name,df_emoji['unichar'][i])\n",
    "    return options\n",
    "\n",
    "def get_emojis(tweet_lst,df_emoji):\n",
    "    emoji_idx = []\n",
    "    emoji_char =[]\n",
    "    for tweet in tweet_lst:\n",
    "        for i,uni in enumerate(df_emoji['unichar']):\n",
    "            if uni in tweet:\n",
    "                 emoji_idx.append(i)\n",
    "                 emoji_char.append(uni)\n",
    "    return emoji_idx, emoji_char\n",
    "\n",
    "def get_emojis_by_tweet(tweet_lst,df_emoji):\n",
    "    by_tweet = []\n",
    "\n",
    "    for tweet in tweet_lst:\n",
    "        emoji_char =[]\n",
    "        for i,uni in enumerate(df_emoji['unichar']):\n",
    "            if uni in tweet:\n",
    "                 emoji_char.append(uni)\n",
    "        by_tweet.append(emoji_char)\n",
    "    return by_tweet\n",
    "\n",
    "#this seems to get some emojis that i dont but also missed some that i do. It also get duplicates per tweet that i dont\n",
    "# def get_emojis_2(tweet_lst):\n",
    "#     emojis = []\n",
    "#     for tweet in tweet_lst:\n",
    "#         emoji = re.compile(u'[\\uD800-\\uDBFF][\\uDC00-\\uDFFF]')\n",
    "#         emojis.append(emoji.findall(tweet))\n",
    "#     return emojis\n",
    "\n",
    "def print_emoji(tweet,emoji_char):\n",
    "    for uni in emoji_char:\n",
    "        if uni in tweet:\n",
    "            print(uni),\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    tweets = np.array(list(pickle.load(open('./data/yay_moji.pkl','rb'))))\n",
    "    # type is list\n",
    "\n",
    "    emojis = pd.read_pickle('./data/df_emojis.pkl')\n",
    "    # type is DataFrame\n",
    "\n",
    "\n",
    "\n",
    "    # ------------- tfidf\n",
    "    stopwords = set(list(ENGLISH_STOP_WORDS) + ['rt', 'follow', 'dm', 'https', 'ur', 'll' ,'amp', 'subscribe', 'don', 've', 'retweet', 'im', 'http'])\n",
    "\n",
    "    tfidf = TfidfVectorizer(max_features=10000, max_df=0.05, min_df=0.001, stop_words = stopwords, ngram_range = (1,2))\n",
    "\n",
    "    #lemmetizing need to consider cleaning the tweets myself\n",
    "\n",
    "    tfidf_tweets = tfidf.fit_transform(tweets)\n",
    "    bag = np.array(tfidf.get_feature_names())\n",
    "\n",
    "    # -------------- NMF\n",
    "    k = 10\n",
    "     #number of groups\n",
    "    nmf2 = NMF(n_components = k)\n",
    "    nmf2.fit(tfidf_tweets)\n",
    "    W = nmf2.transform(tfidf_tweets) #len(yay_moji,k)\n",
    "    H = nmf2.components_ #k,len(yay_moji)\n",
    "\n",
    "\n",
    "    # --------------- Printing Top 10\n",
    "    tweet_lst = []\n",
    "    top = 10\n",
    "    tweet_in_group_thresh = .001 #score thresh if we consider that tweet as part of that group\n",
    "    for group in range(k):\n",
    "        #idx of the top ten words for each group\n",
    "        i_words = np.argsort(H[group])[::-1][:top]\n",
    "        words = bag[i_words]\n",
    "\n",
    "        # idx of the top ten tweets for each group\n",
    "        i_emojis = np.argsort(W[:,group])[::-1][:top]\n",
    "        # most common 10 emojis for each group\n",
    "\n",
    "        print('-'*10)\n",
    "        print('Group:',group)\n",
    "        counted_tweets = np.argwhere(W[:,group] > tweet_in_group_thresh)\n",
    "        print(counted_tweets.shape[0], 'tweets')\n",
    "        for word in words:\n",
    "            print('-->',word)\n",
    "        for i_tweet in i_emojis:\n",
    "            print_emoji(tweets[i_tweet], emojis['unichar'])\n",
    "            tweet_lst.append(tweets[i_tweet])\n",
    "        ind, emo_lst = get_emojis(tweets[i_emojis],emojis)\n",
    "        # find percentage of emoji per group\n",
    "        most_emoji, how_many = Counter(emo_lst).most_common(1)[0]\n",
    "        score = float(how_many)/top\n",
    "        # print score #score is not perfect - similar emojis and repeat in the same tweet\n",
    "        print('\\n')\n",
    "\n",
    "    # --------------- printing most common emojis\n",
    "    most_common = 50\n",
    "    b,all_emojis = get_emojis(tweets,emojis)\n",
    "    count = Counter(all_emojis).most_common(most_common)\n",
    "    unicode_top = []\n",
    "    for emo, i in count:\n",
    "        print(emo,i)\n",
    "#        for j, char in enumerate(emojis['unichar']):\n",
    "#            if char == emo:\n",
    "#                unicode_top.append(emojis['unified'][j])\n",
    "\n",
    "\n",
    "# test stuff\n",
    "    jan = get_emojis_by_tweet(tweets[0:100],emojis)\n",
    "    for tweet in jan:\n",
    "        for emo in tweet:\n",
    "            print(emo),\n",
    "        print('')\n",
    "    # name_of = find_emoji('heart',emojis)\n",
    "\n",
    "\n",
    "    '''\n",
    "    to do's\n",
    "    --> how big are the groups? do a most common\n",
    "    --> get a better score system\n",
    "    --> allow for tweets with multiple emojis\n",
    "    --> sub set for tweets with a specific emoji\n",
    "    --> commonly combined emojis\n",
    "    --> naive bayes\n",
    "        prediction accuracy between emojis for how similar they are\n",
    "    whats the purpose:\n",
    "    --> to help use emojis as labels for tweets\n",
    "    '''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
