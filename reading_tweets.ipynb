{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from __future__ import unicode_literals\n",
    "import twitter_credentials as cred\n",
    "import tweepy\n",
    "import pickle\n",
    "import os\n",
    "import datetime\n",
    "import time\n",
    "import re\n",
    "import boto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "altered code from https://github.com/janvanzeghbroeck/urban-emoji/blob/master/twitter_api.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading 1 of 20\n",
      "Loading 2 of 20\n",
      "Loading 3 of 20\n",
      "Loading 4 of 20\n",
      "Loading 5 of 20\n",
      "Loading 6 of 20\n",
      "Loading 7 of 20\n",
      "Loading 8 of 20\n",
      "Loading 9 of 20\n",
      "Loading 10 of 20\n",
      "Loading 11 of 20\n",
      "Loading 12 of 20\n",
      "Loading 13 of 20\n",
      "Loading 14 of 20\n",
      "Loading 15 of 20\n",
      "Loading 16 of 20\n",
      "Loading 17 of 20\n",
      "Loading 18 of 20\n",
      "Loading 19 of 20\n",
      "Loading 20 of 20\n",
      "Succesfully pickled 1688 tweets!\n",
      "Successfully saved tweets_Mon_Jul__1_17-20-32_2019_is.pkl to S3 bucket emoji-tweets\n",
      "Loading 1 of 20\n",
      "Loading 2 of 20\n",
      "Loading 3 of 20\n",
      "Loading 4 of 20\n",
      "Loading 5 of 20\n",
      "Loading 6 of 20\n",
      "Loading 7 of 20\n",
      "Loading 8 of 20\n",
      "Loading 9 of 20\n",
      "Loading 10 of 20\n",
      "Loading 11 of 20\n",
      "Loading 12 of 20\n",
      "Waiting for API to allow more calls...\n",
      "Loading 13 of 20\n",
      "Waiting for API to allow more calls...\n",
      "Loading 14 of 20\n",
      "Waiting for API to allow more calls...\n",
      "Loading 15 of 20\n",
      "Loading 16 of 20\n",
      "Loading 17 of 20\n",
      "Loading 18 of 20\n",
      "Loading 19 of 20\n",
      "Loading 20 of 20\n",
      "Succesfully pickled 1460 tweets!\n",
      "Successfully saved tweets_Mon_Jul__1_17-20-32_2019_it.pkl to S3 bucket emoji-tweets\n",
      "Loading 1 of 20\n",
      "Loading 2 of 20\n",
      "Loading 3 of 20\n",
      "Loading 4 of 20\n",
      "Loading 5 of 20\n",
      "Loading 6 of 20\n",
      "Loading 7 of 20\n",
      "Loading 8 of 20\n",
      "Loading 9 of 20\n",
      "Loading 10 of 20\n",
      "Loading 11 of 20\n",
      "Loading 12 of 20\n",
      "Loading 13 of 20\n",
      "Loading 14 of 20\n",
      "Loading 15 of 20\n",
      "Waiting for API to allow more calls...\n",
      "Loading 16 of 20\n",
      "Waiting for API to allow more calls...\n",
      "Loading 17 of 20\n",
      "Waiting for API to allow more calls...\n",
      "Loading 18 of 20\n",
      "Loading 19 of 20\n",
      "Loading 20 of 20\n",
      "Succesfully pickled 1580 tweets!\n",
      "Successfully saved tweets_Mon_Jul__1_17-20-32_2019_the.pkl to S3 bucket emoji-tweets\n",
      "Loading 1 of 20\n",
      "Loading 2 of 20\n",
      "Loading 3 of 20\n",
      "Loading 4 of 20\n",
      "Loading 5 of 20\n",
      "Loading 6 of 20\n",
      "Loading 7 of 20\n",
      "Loading 8 of 20\n",
      "Loading 9 of 20\n",
      "Loading 10 of 20\n",
      "Loading 11 of 20\n",
      "Loading 12 of 20\n",
      "Loading 13 of 20\n",
      "Loading 14 of 20\n",
      "Loading 15 of 20\n",
      "Loading 16 of 20\n",
      "Loading 17 of 20\n",
      "Loading 18 of 20\n",
      "Waiting for API to allow more calls...\n",
      "Loading 19 of 20\n",
      "Waiting for API to allow more calls...\n",
      "Loading 20 of 20\n",
      "Waiting for API to allow more calls...\n",
      "Succesfully pickled 1456 tweets!\n",
      "Successfully saved tweets_Mon_Jul__1_17-20-32_2019_are.pkl to S3 bucket emoji-tweets\n",
      "Loading 1 of 20\n",
      "Loading 2 of 20\n",
      "Loading 3 of 20\n",
      "Loading 4 of 20\n",
      "Loading 5 of 20\n",
      "Loading 6 of 20\n",
      "Loading 7 of 20\n",
      "Loading 8 of 20\n",
      "Loading 9 of 20\n",
      "Loading 10 of 20\n",
      "Loading 11 of 20\n",
      "Loading 12 of 20\n",
      "Loading 13 of 20\n",
      "Loading 14 of 20\n",
      "Loading 15 of 20\n",
      "Loading 16 of 20\n",
      "Loading 17 of 20\n",
      "Loading 18 of 20\n",
      "Loading 19 of 20\n",
      "Loading 20 of 20\n",
      "Waiting for API to allow more calls...\n",
      "Succesfully pickled 1582 tweets!\n",
      "Successfully saved tweets_Mon_Jul__1_17-20-32_2019_if.pkl to S3 bucket emoji-tweets\n",
      "Loading 1 of 20\n",
      "Waiting for API to allow more calls...\n",
      "Loading 2 of 20\n",
      "Waiting for API to allow more calls...\n",
      "Loading 3 of 20\n",
      "Waiting for API to allow more calls...\n",
      "Loading 4 of 20\n",
      "Loading 5 of 20\n",
      "Loading 6 of 20\n",
      "Loading 7 of 20\n",
      "Loading 8 of 20\n",
      "Loading 9 of 20\n",
      "Loading 10 of 20\n",
      "Loading 11 of 20\n",
      "Loading 12 of 20\n",
      "Loading 13 of 20\n",
      "Loading 14 of 20\n",
      "Loading 15 of 20\n",
      "Loading 16 of 20\n",
      "Loading 17 of 20\n",
      "Loading 18 of 20\n",
      "Loading 19 of 20\n",
      "Loading 20 of 20\n",
      "Succesfully pickled 1445 tweets!\n",
      "Successfully saved tweets_Mon_Jul__1_17-20-32_2019_to.pkl to S3 bucket emoji-tweets\n",
      "Loading 1 of 20\n",
      "Loading 2 of 20\n",
      "Loading 3 of 20\n",
      "Loading 4 of 20\n",
      "Waiting for API to allow more calls...\n",
      "Loading 5 of 20\n",
      "Waiting for API to allow more calls...\n",
      "Loading 6 of 20\n",
      "Waiting for API to allow more calls...\n",
      "Loading 7 of 20\n",
      "Loading 8 of 20\n",
      "Loading 9 of 20\n",
      "Loading 10 of 20\n",
      "Loading 11 of 20\n",
      "Loading 12 of 20\n",
      "Loading 13 of 20\n",
      "Loading 14 of 20\n",
      "Loading 15 of 20\n",
      "Loading 16 of 20\n",
      "Loading 17 of 20\n",
      "Loading 18 of 20\n",
      "Loading 19 of 20\n",
      "Loading 20 of 20\n",
      "Succesfully pickled 1562 tweets!\n",
      "Successfully saved tweets_Mon_Jul__1_17-20-32_2019_and.pkl to S3 bucket emoji-tweets\n",
      "Loading 1 of 20\n",
      "Loading 2 of 20\n",
      "Loading 3 of 20\n",
      "Loading 4 of 20\n",
      "Loading 5 of 20\n",
      "Loading 6 of 20\n",
      "Waiting for API to allow more calls...\n",
      "Loading 7 of 20\n",
      "Waiting for API to allow more calls...\n",
      "Loading 8 of 20\n",
      "Waiting for API to allow more calls...\n",
      "Loading 9 of 20\n",
      "Waiting for API to allow more calls...\n",
      "Loading 10 of 20\n",
      "Loading 11 of 20\n",
      "Loading 12 of 20\n",
      "Loading 13 of 20\n",
      "Loading 14 of 20\n",
      "Loading 15 of 20\n",
      "Loading 16 of 20\n",
      "Loading 17 of 20\n",
      "Loading 18 of 20\n",
      "Loading 19 of 20\n",
      "Loading 20 of 20\n",
      "Succesfully pickled 1429 tweets!\n",
      "Successfully saved tweets_Mon_Jul__1_17-20-32_2019_of.pkl to S3 bucket emoji-tweets\n",
      "Loading 1 of 20\n",
      "Loading 2 of 20\n",
      "Loading 3 of 20\n",
      "Loading 4 of 20\n",
      "Loading 5 of 20\n",
      "Loading 6 of 20\n",
      "Loading 7 of 20\n",
      "Loading 8 of 20\n",
      "Loading 9 of 20\n",
      "Loading 10 of 20\n",
      "Waiting for API to allow more calls...\n",
      "Loading 11 of 20\n",
      "Waiting for API to allow more calls...\n",
      "Loading 12 of 20\n",
      "Waiting for API to allow more calls...\n",
      "Loading 13 of 20\n",
      "Loading 14 of 20\n",
      "Loading 15 of 20\n",
      "Loading 16 of 20\n",
      "Loading 17 of 20\n",
      "Loading 18 of 20\n",
      "Loading 19 of 20\n"
     ]
    }
   ],
   "source": [
    "auth = tweepy.OAuthHandler(cred.consumer_key, cred.consumer_secret)\n",
    "auth.set_access_token(cred.access_token, cred.access_token_secret)\n",
    "\n",
    "api = tweepy.API(auth)\n",
    "\n",
    "\n",
    "\n",
    "def get_tweets(topic, save_file_name, num_batches=25, num_tweets = 20, to_bucket = False): # num_batches * 100 is total tweets target\n",
    "    tweets = set()\n",
    "    # public_tweets = api.home_timeline()\n",
    "    for i in range(num_batches):\n",
    "        try:\n",
    "            print('Loading', i+1, 'of', num_batches)\n",
    "            for tweet in tweepy.Cursor(api.search, q=topic).items(num_tweets): #100 batches of 20\n",
    "\n",
    "                if tweet.lang == 'en':\n",
    "                    tweets.add(tweet.text) \n",
    "\n",
    "            time.sleep(35) \n",
    "        except:\n",
    "            print('Waiting for API to allow more calls...')\n",
    "            time.sleep(60)\n",
    "            pass\n",
    "\n",
    "    # if to_bucket:\n",
    "        pass\n",
    "    else:\n",
    "        pickle.dump( tweets, open( \"{}.pkl\".format(save_file_name), \"wb\" ) )\n",
    "        print('Succesfully pickled', len(tweets), 'tweets!')\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    now = datetime.datetime.today().ctime()\n",
    "    now = re.sub(' ','_',now)\n",
    "    now = re.sub(':','-',now)\n",
    "\n",
    "\n",
    "    # use boto to connect to aws buckets\n",
    "    conn = boto.connect_s3(cred.aws_access_key, cred.aws_access_secret_key)\n",
    "\n",
    "    # what bucket?\n",
    "    bucket_name = 'emoji-tweets'\n",
    "\n",
    "    # check if bucket exists if not make it\n",
    "    if conn.lookup(bucket_name) is None:\n",
    "        b = conn.create_bucket(bucket_name)\n",
    "    else:\n",
    "        b = conn.get_bucket(bucket_name)\n",
    "\n",
    "    simple_words = ['is', 'it', 'the', 'are','if','to', 'and', 'of', 'in']\n",
    "    \n",
    "    for word in simple_words:\n",
    "        pkl_name = './tweet_data/tweets_{}_{}'.format(now,word)\n",
    "        s3_name = 'tweets_{}_{}.pkl'.format(now,word)\n",
    "        loc_name = './tweet_data/tweets_{}_{}.pkl'.format(now,word)\n",
    "        get_tweets(word, pkl_name, num_batches = 20, num_tweets = 100)\n",
    "\n",
    "        # save the pkl file\n",
    "        file_object = b.new_key(s3_name)#where to save\n",
    "        file_object.set_contents_from_filename(loc_name)\n",
    "\n",
    "        print('Successfully saved {} to S3 bucket {}'.format(s3_name,bucket_name))\n",
    "\n",
    "    \n",
    "    # to read the file\n",
    "    #fil_object.get_contents_to_file('folder/file')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir \n",
    "from os.path import isfile, join \n",
    "import pandas as pd\n",
    "mypath = './tweet_data'\n",
    "files = [f for f in listdir(mypath) if isfile(join(mypath, f))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tweets = []\n",
    "for i in files:\n",
    "    if i != '.DS_Store':\n",
    "        file = './tweet_data/'+ i\n",
    "        tweets += list(pickle.load(open(file,'rb')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "43827"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read this many tweets!\n",
    "len(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Succesfully pickled 4642 tweets and emoji data frame\n"
     ]
    }
   ],
   "source": [
    "# ----- creates the emoji df and adds the unichar\n",
    "def df_emojis():\n",
    "    # create full df_emoji\n",
    "    df = pd.read_pickle('./data/df_emoji.pkl')\n",
    "    return df\n",
    "\n",
    "# ------------ finding the tweets with emojis\n",
    "def yay_no(tweets,df_emojis):\n",
    "    no_moji = []\n",
    "    yay_moji = []\n",
    "    for tweet in tweets:\n",
    "        tweet = str(tweet) #some have type tweepy.models.Status\n",
    "        yay = False\n",
    "        for uni in df_emojis['unichar']:\n",
    "            #if emoji in str(tweet):\n",
    "            if uni in tweet:\n",
    "                yay = True\n",
    "        if yay:\n",
    "            yay_moji.append(tweet)\n",
    "        else: # else statement to create no_moji list\n",
    "            no_moji.append(tweet)\n",
    "    return yay_moji, no_moji\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    tweets = list(set(tweets))\n",
    "\n",
    "    df_emojis = df_emojis()\n",
    "\n",
    "    yay_moji, no_moji = yay_no(tweets,df_emojis)\n",
    "\n",
    "    pickle.dump( yay_moji, open( \"./data/yay_moji.pkl\", \"wb\"))\n",
    "    pickle.dump( df_emojis, open( \"./data/df_emojis.pkl\", \"wb\"))\n",
    "\n",
    "    print('Succesfully pickled {} tweets and emoji data frame'.format(len(yay_moji)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41985"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list(set(tweets)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41985"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(no_moji) + len(yay_moji)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
