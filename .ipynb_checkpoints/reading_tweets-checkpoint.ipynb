{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from __future__ import unicode_literals\n",
    "import twitter_credentials as cred\n",
    "import tweepy\n",
    "import pickle\n",
    "import os\n",
    "import datetime\n",
    "import time\n",
    "import re\n",
    "import boto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "altered code from https://github.com/janvanzeghbroeck/urban-emoji/blob/master/twitter_api.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading 1 of 20\n",
      "Loading 2 of 20\n",
      "Loading 3 of 20\n",
      "Loading 4 of 20\n",
      "Loading 5 of 20\n",
      "Loading 6 of 20\n",
      "Loading 7 of 20\n",
      "Waiting for API to allow more calls...\n",
      "Loading 8 of 20\n",
      "Waiting for API to allow more calls...\n",
      "Loading 9 of 20\n",
      "Waiting for API to allow more calls...\n",
      "Loading 10 of 20\n",
      "Loading 11 of 20\n",
      "Loading 12 of 20\n",
      "Loading 13 of 20\n",
      "Loading 14 of 20\n",
      "Loading 15 of 20\n",
      "Loading 16 of 20\n",
      "Loading 17 of 20\n",
      "Loading 18 of 20\n",
      "Loading 19 of 20\n",
      "Loading 20 of 20\n",
      "Succesfully pickled 1518 tweets!\n",
      "Successfully saved tweets_Sat_Jun_29_12-14-48_2019_is.pkl to S3 bucket emoji-tweets\n",
      "Loading 1 of 20\n",
      "Loading 2 of 20\n",
      "Loading 3 of 20\n",
      "Loading 4 of 20\n",
      "Loading 5 of 20\n",
      "Loading 6 of 20\n",
      "Loading 7 of 20\n",
      "Loading 8 of 20\n",
      "Loading 9 of 20\n",
      "Loading 10 of 20\n",
      "Loading 11 of 20\n",
      "Loading 12 of 20\n",
      "Loading 13 of 20\n",
      "Loading 14 of 20\n",
      "Loading 15 of 20\n",
      "Loading 16 of 20\n",
      "Loading 17 of 20\n",
      "Loading 18 of 20\n",
      "Loading 19 of 20\n",
      "Loading 20 of 20\n",
      "Succesfully pickled 1790 tweets!\n",
      "Successfully saved tweets_Sat_Jun_29_12-14-48_2019_it.pkl to S3 bucket emoji-tweets\n",
      "Loading 1 of 20\n",
      "Loading 2 of 20\n",
      "Loading 3 of 20\n",
      "Loading 4 of 20\n",
      "Loading 5 of 20\n",
      "Loading 6 of 20\n",
      "Loading 7 of 20\n",
      "Loading 8 of 20\n",
      "Loading 9 of 20\n",
      "Loading 10 of 20\n",
      "Waiting for API to allow more calls...\n",
      "Loading 11 of 20\n",
      "Waiting for API to allow more calls...\n",
      "Loading 12 of 20\n",
      "Waiting for API to allow more calls...\n",
      "Loading 13 of 20\n",
      "Loading 14 of 20\n",
      "Loading 15 of 20\n",
      "Loading 16 of 20\n",
      "Loading 17 of 20\n",
      "Loading 18 of 20\n",
      "Loading 19 of 20\n",
      "Loading 20 of 20\n",
      "Succesfully pickled 1589 tweets!\n",
      "Successfully saved tweets_Sat_Jun_29_12-14-48_2019_the.pkl to S3 bucket emoji-tweets\n",
      "Loading 1 of 20\n",
      "Loading 2 of 20\n",
      "Loading 3 of 20\n",
      "Loading 4 of 20\n",
      "Loading 5 of 20\n",
      "Loading 6 of 20\n",
      "Loading 7 of 20\n",
      "Loading 8 of 20\n",
      "Loading 9 of 20\n",
      "Loading 10 of 20\n",
      "Loading 11 of 20\n",
      "Loading 12 of 20\n",
      "Waiting for API to allow more calls...\n",
      "Loading 13 of 20\n",
      "Waiting for API to allow more calls...\n",
      "Loading 14 of 20\n",
      "Waiting for API to allow more calls...\n",
      "Loading 15 of 20\n",
      "Waiting for API to allow more calls...\n",
      "Loading 16 of 20\n",
      "Loading 17 of 20\n",
      "Loading 18 of 20\n",
      "Loading 19 of 20\n",
      "Loading 20 of 20\n",
      "Succesfully pickled 1441 tweets!\n",
      "Successfully saved tweets_Sat_Jun_29_12-14-48_2019_are.pkl to S3 bucket emoji-tweets\n",
      "Loading 1 of 20\n",
      "Loading 2 of 20\n",
      "Loading 3 of 20\n",
      "Loading 4 of 20\n",
      "Loading 5 of 20\n",
      "Loading 6 of 20\n",
      "Loading 7 of 20\n",
      "Loading 8 of 20\n",
      "Loading 9 of 20\n",
      "Loading 10 of 20\n",
      "Loading 11 of 20\n",
      "Loading 12 of 20\n",
      "Loading 13 of 20\n",
      "Loading 14 of 20\n",
      "Loading 15 of 20\n",
      "Waiting for API to allow more calls...\n",
      "Loading 16 of 20\n",
      "Waiting for API to allow more calls...\n",
      "Loading 17 of 20\n",
      "Waiting for API to allow more calls...\n",
      "Loading 18 of 20\n",
      "Waiting for API to allow more calls...\n",
      "Loading 19 of 20\n",
      "Loading 20 of 20\n",
      "Succesfully pickled 257 tweets!\n",
      "Successfully saved tweets_Sat_Jun_29_12-14-48_2019_vegan.pkl to S3 bucket emoji-tweets\n",
      "Loading 1 of 20\n",
      "Loading 2 of 20\n",
      "Loading 3 of 20\n",
      "Loading 4 of 20\n",
      "Loading 5 of 20\n",
      "Loading 6 of 20\n",
      "Loading 7 of 20\n",
      "Loading 8 of 20\n",
      "Loading 9 of 20\n",
      "Loading 10 of 20\n",
      "Loading 11 of 20\n",
      "Loading 12 of 20\n",
      "Loading 13 of 20\n",
      "Loading 14 of 20\n",
      "Loading 15 of 20\n",
      "Loading 16 of 20\n",
      "Loading 17 of 20\n",
      "Loading 18 of 20\n",
      "Loading 19 of 20\n",
      "Loading 20 of 20\n",
      "Succesfully pickled 994 tweets!\n",
      "Successfully saved tweets_Sat_Jun_29_12-14-48_2019_climate.pkl to S3 bucket emoji-tweets\n"
     ]
    }
   ],
   "source": [
    "auth = tweepy.OAuthHandler(cred.consumer_key, cred.consumer_secret)\n",
    "auth.set_access_token(cred.access_token, cred.access_token_secret)\n",
    "\n",
    "api = tweepy.API(auth)\n",
    "\n",
    "\n",
    "\n",
    "def get_tweets(topic, save_file_name, num_batches=25, num_tweets = 20, to_bucket = False): # num_batches * 100 is total tweets target\n",
    "    tweets = set()\n",
    "    # public_tweets = api.home_timeline()\n",
    "    for i in range(num_batches):\n",
    "        try:\n",
    "            print('Loading', i+1, 'of', num_batches)\n",
    "            for tweet in tweepy.Cursor(api.search, q=topic).items(num_tweets): #100 batches of 20\n",
    "\n",
    "                if tweet.lang == 'en':\n",
    "                    tweets.add(tweet.text) \n",
    "\n",
    "            time.sleep(35) \n",
    "        except:\n",
    "            print('Waiting for API to allow more calls...')\n",
    "            time.sleep(60)\n",
    "            pass\n",
    "\n",
    "    # if to_bucket:\n",
    "        pass\n",
    "    else:\n",
    "        pickle.dump( tweets, open( \"{}.pkl\".format(save_file_name), \"wb\" ) )\n",
    "        print('Succesfully pickled', len(tweets), 'tweets!')\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    now = datetime.datetime.today().ctime()\n",
    "    now = re.sub(' ','_',now)\n",
    "    now = re.sub(':','-',now)\n",
    "\n",
    "\n",
    "    # use boto to connect to aws buckets\n",
    "    conn = boto.connect_s3(cred.aws_access_key, cred.aws_access_secret_key)\n",
    "\n",
    "    # what bucket?\n",
    "    bucket_name = 'emoji-tweets'\n",
    "\n",
    "    # check if bucket exists if not make it\n",
    "    if conn.lookup(bucket_name) is None:\n",
    "        b = conn.create_bucket(bucket_name)\n",
    "    else:\n",
    "        b = conn.get_bucket(bucket_name)\n",
    "\n",
    "    simple_words = ['is', 'it', 'the', 'are','vegan','climate']\n",
    "    \n",
    "    for word in simple_words:\n",
    "        pkl_name = './tweet_data/tweets_{}_{}'.format(now,word)\n",
    "        s3_name = 'tweets_{}_{}.pkl'.format(now,word)\n",
    "        loc_name = './tweet_data/tweets_{}_{}.pkl'.format(now,word)\n",
    "        get_tweets(word, pkl_name, num_batches = 20, num_tweets = 100)\n",
    "\n",
    "        # save the pkl file\n",
    "        file_object = b.new_key(s3_name)#where to save\n",
    "        file_object.set_contents_from_filename(loc_name)\n",
    "\n",
    "        print('Successfully saved {} to S3 bucket {}'.format(s3_name,bucket_name))\n",
    "\n",
    "    \n",
    "    # to read the file\n",
    "    #fil_object.get_contents_to_file('folder/file')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir \n",
    "from os.path import isfile, join \n",
    "import pandas as pd\n",
    "mypath = './tweet_data'\n",
    "files = [f for f in listdir(mypath) if isfile(join(mypath, f))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tweets = []\n",
    "for i in files:\n",
    "    if i != '.DS_Store':\n",
    "        file = './tweet_data/'+ i\n",
    "        tweets += list(pickle.load(open(file,'rb')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45072"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read this many tweets!\n",
    "len(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Succesfully pickled 5693 tweets and emoji data frame\n"
     ]
    }
   ],
   "source": [
    "# ----- creates the emoji df and adds the unichar\n",
    "def df_emojis():\n",
    "    # create full df_emoji\n",
    "    df = pd.read_pickle('./data/df_emoji.pkl')\n",
    "    return df\n",
    "\n",
    "# ------------ finding the tweets with emojis\n",
    "def yay_no(tweets,df_emojis):\n",
    "    no_moji = []\n",
    "    yay_moji = []\n",
    "    for tweet in tweets:\n",
    "        tweet = str(tweet) #some have type tweepy.models.Status\n",
    "        yay = False\n",
    "        for uni in df_emojis['unichar']:\n",
    "            #if emoji in str(tweet):\n",
    "            if uni in tweet:\n",
    "                yay = True\n",
    "        if yay:\n",
    "            yay_moji.append(tweet)\n",
    "        else: # else statement to create no_moji list\n",
    "            no_moji.append(tweet)\n",
    "    return yay_moji, no_moji\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    tweets = list(set(tweets))\n",
    "\n",
    "    df_emojis = df_emojis()\n",
    "\n",
    "    yay_moji, no_moji = yay_no(tweets,df_emojis)\n",
    "\n",
    "    pickle.dump( yay_moji, open( \"./data/yay_moji.pkl\", \"wb\"))\n",
    "    pickle.dump( df_emojis, open( \"./data/df_emojis.pkl\", \"wb\"))\n",
    "\n",
    "    print('Succesfully pickled {} tweets and emoji data frame'.format(len(yay_moji)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "43193"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list(set(tweets)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "43193"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(no_moji) + len(yay_moji)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
