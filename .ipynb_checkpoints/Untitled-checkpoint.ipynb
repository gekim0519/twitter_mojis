{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------\n",
      "Group: 0\n",
      "56 tweets\n",
      "--> btsongma\n",
      "--> gma\n",
      "--> bts_twt\n",
      "--> army\n",
      "--> ready\n",
      "--> bts\n",
      "--> bts army\n",
      "--> gma ready\n",
      "--> ready btsongma\n",
      "--> btsongma bts\n",
      "ðŸ’œ\n",
      "ðŸ’œ\n",
      "ðŸ’œ\n",
      "ðŸ’œ\n",
      "ðŸ’œ\n",
      "ðŸ’œ\n",
      "ðŸ’œ\n",
      "ðŸ’œ\n",
      "ðŸŒ‹\n",
      "ðŸ”¥\n",
      "ðŸ˜‚\n",
      "\n",
      "\n",
      "----------\n",
      "Group: 1\n",
      "2 tweets\n",
      "--> adnloveconnection\n",
      "--> adnloveconnection lynieg88\n",
      "--> axle1809\n",
      "--> carl05290 diosarmendoza\n",
      "--> carl05290\n",
      "--> prettymai_0105\n",
      "--> diosarmendoza axle1809\n",
      "--> lynieg88 darwaine88\n",
      "--> lynieg88\n",
      "--> chie_chie26\n",
      "ðŸ˜˜\n",
      "ðŸ˜˜\n",
      "ðŸ˜\n",
      "ðŸ”¥\n",
      "ðŸ˜‚\n",
      "ðŸ˜…\n",
      "ðŸ’•\n",
      "ðŸ˜‚\n",
      "ðŸ˜’\n",
      "ðŸ˜­\n",
      "ðŸ’œ\n",
      "\n",
      "\n",
      "----------\n",
      "Group: 2\n",
      "38 tweets\n",
      "--> derby\n",
      "--> leeds\n",
      "--> lampard\n",
      "--> derby players\n",
      "--> gesture\n",
      "--> players\n",
      "--> spying gesture\n",
      "--> spying\n",
      "--> sportbible\n",
      "--> stop\n",
      "ðŸ‘€\n",
      "ðŸ˜‚\n",
      "ðŸ˜\n",
      "ðŸ˜‚\n",
      "ðŸ‘\n",
      "ðŸ˜\n",
      "ðŸ’ª\n",
      "ðŸ˜‚\n",
      "ðŸ˜‚\n",
      "ðŸ™Œ\n",
      "ðŸš¨\n",
      "\n",
      "\n",
      "----------\n",
      "Group: 3\n",
      "40 tweets\n",
      "--> love\n",
      "--> yes love\n",
      "--> breakingdawnon405\n",
      "--> breakingdawnon405 love\n",
      "--> yes\n",
      "--> ok\n",
      "--> screenshotted try\n",
      "--> try thank\n",
      "--> screenshotted\n",
      "--> ok screenshotted\n",
      "ðŸ“\n",
      "ðŸ‘Œ\n",
      "ðŸ’œ\n",
      "ðŸ˜­\n",
      "ðŸ˜\n",
      "ðŸ˜\n",
      "ðŸ‘‡\n",
      "â¤\n",
      "âœ¨\n",
      "â¤\n",
      "â¤\n",
      "ðŸ˜\n",
      "ðŸ˜˜\n",
      "ðŸŽ¼\n",
      "\n",
      "\n",
      "----------\n",
      "Group: 4\n",
      "67 tweets\n",
      "--> say\n",
      "--> just say\n",
      "--> shit\n",
      "--> child\n",
      "--> bjznweh05k\n",
      "--> jzizzzle_\n",
      "--> say shit\n",
      "--> jzizzzle_ just\n",
      "--> shit bjznweh05k\n",
      "--> bad\n",
      "ðŸ˜Š\n",
      "ðŸ˜­\n",
      "ðŸ˜‚\n",
      "ðŸ˜‚\n",
      "ðŸ†\n",
      "ðŸ‘\n",
      "ðŸ’¦\n",
      "ðŸ˜‚\n",
      "ðŸŽ¥\n",
      "ðŸ’¥\n",
      "ðŸ™\n",
      "ðŸš¨\n",
      "ðŸ‘¨\n",
      "ðŸ‘©\n",
      "ðŸ˜‚\n",
      "ðŸ˜­\n",
      "\n",
      "\n",
      "----------\n",
      "Group: 5\n",
      "73 tweets\n",
      "--> got\n",
      "--> girl\n",
      "--> meme\n",
      "--> got stitches\n",
      "--> meme got\n",
      "--> girl 08q1zvex5r\n",
      "--> stitches\n",
      "--> stitches girl\n",
      "--> 08q1zvex5r\n",
      "--> elizabetjas proudly\n",
      "ðŸ’Ž\n",
      "ðŸ˜‚\n",
      "ðŸ˜‚\n",
      "ðŸ˜­\n",
      "ðŸš®\n",
      "ðŸ˜‚\n",
      "ðŸ‡ºðŸ‡¸\n",
      "ðŸ˜³\n",
      "ðŸŒš\n",
      "ðŸ’€\n",
      "ðŸ˜‚\n",
      "ðŸ”¥\n",
      "\n",
      "\n",
      "----------\n",
      "Group: 6\n",
      "57 tweets\n",
      "--> think\n",
      "--> funny\n",
      "--> worst\n",
      "--> need\n",
      "--> bitches\n",
      "--> daintymongeau funny\n",
      "--> half\n",
      "--> funny think\n",
      "--> tanamongeau daintymongeau\n",
      "--> tanamongeau\n",
      "ðŸ‘\n",
      "ðŸ˜‚\n",
      "ðŸ˜‚\n",
      "ðŸ‡ºðŸ‡¸\n",
      "ðŸ˜­\n",
      "ðŸ˜‚\n",
      "ðŸ˜¨\n",
      "ðŸ’›\n",
      "ðŸ˜­\n",
      "ðŸ’€\n",
      "\n",
      "\n",
      "----------\n",
      "Group: 7\n",
      "72 tweets\n",
      "--> hope\n",
      "--> today\n",
      "--> gift\n",
      "--> best\n",
      "--> thank\n",
      "--> day\n",
      "--> tomorrow\n",
      "--> dkzjoewm4m\n",
      "--> best dkzjoewm4m\n",
      "--> hope day\n",
      "â™¥\n",
      "â¤\n",
      "ðŸ’ž\n",
      "ðŸ˜”\n",
      "ðŸ˜\n",
      "â¤\n",
      "ðŸ˜Š\n",
      "ðŸ™Œ\n",
      "ðŸ’š\n",
      "ðŸ’™\n",
      "ðŸ˜­\n",
      "ðŸ˜˜\n",
      "ðŸ’›\n",
      "\n",
      "\n",
      "----------\n",
      "Group: 8\n",
      "66 tweets\n",
      "--> going\n",
      "--> nice\n",
      "--> talk\n",
      "--> instead going\n",
      "--> instead\n",
      "--> 8ivdixjboi\n",
      "--> chopdaily\n",
      "--> chopdaily going\n",
      "--> going 8ivdixjboi\n",
      "--> work\n",
      "ðŸ˜¤\n",
      "ðŸ˜‚\n",
      "ðŸ”´\n",
      "â˜º\n",
      "ðŸ˜‚\n",
      "â˜\n",
      "ðŸ˜Œ\n",
      "ðŸ‡ºðŸ‡¸\n",
      "ðŸ”Š\n",
      "ðŸ˜­\n",
      "ðŸ”¥\n",
      "ðŸ˜‚\n",
      "\n",
      "\n",
      "----------\n",
      "Group: 9\n",
      "80 tweets\n",
      "--> time\n",
      "--> know\n",
      "--> win\n",
      "--> giveaway\n",
      "--> enter\n",
      "--> time nct127oncorden\n",
      "--> latelateshow time\n",
      "--> latelateshow\n",
      "--> nct127oncorden\n",
      "--> nct127oncorden qfjdwzajqs\n",
      "ðŸ‘\n",
      "ðŸ’ž\n",
      "ðŸ˜\n",
      "âœŒ\n",
      "ðŸ˜‚\n",
      "ðŸ’”\n",
      "ðŸ˜\n",
      "ðŸ˜­\n",
      "ðŸ˜Ž\n",
      "ðŸŒ\n",
      "ðŸ˜‚\n",
      "ðŸ™\n",
      "ðŸš¨\n",
      "\n",
      "\n",
      "ðŸ˜‚ 65\n",
      "ðŸ˜ 25\n",
      "ðŸ˜­ 24\n",
      "ðŸ’œ 11\n",
      "â¤ 11\n",
      "ðŸ”¥ 10\n",
      "ðŸ˜˜ 8\n",
      "ðŸ‘€ 6\n",
      "ðŸš¨ 6\n",
      "ðŸ™Œ 5\n",
      "â™¥ 5\n",
      "ðŸ™ 4\n",
      "ðŸ˜” 4\n",
      "ðŸ‘ 4\n",
      "ðŸ’› 4\n",
      "ðŸ˜Š 4\n",
      "ðŸ˜… 4\n",
      "ðŸ‘… 3\n",
      "ðŸ˜« 3\n",
      "ðŸ˜¤ 3\n",
      "ðŸ’” 3\n",
      "âœ¨ 3\n",
      "ðŸ’ª 3\n",
      "ðŸ’° 3\n",
      "â€¼ 3\n",
      "ðŸ‡ºðŸ‡¸ 3\n",
      "ðŸ˜ˆ 2\n",
      "â˜ 2\n",
      "âœŠ 2\n",
      "ðŸ˜Œ 2\n",
      "ðŸ’¯ 2\n",
      "ðŸŒ™ 2\n",
      "ðŸŒŸ 2\n",
      "âœŒ 2\n",
      "ðŸ° 2\n",
      "ðŸ’š 2\n",
      "ðŸ˜³ 2\n",
      "âž¡ 2\n",
      "ðŸ”” 2\n",
      "ðŸ‘ 2\n",
      "âœ… 2\n",
      "ðŸ’™ 2\n",
      "ðŸ’¥ 2\n",
      "ðŸ’¦ 2\n",
      "ðŸ’€ 2\n",
      "â˜º 2\n",
      "ðŸŽ¥ 2\n",
      "ðŸ˜© 2\n",
      "ðŸ’ž 2\n",
      "ðŸ‘‡ 2\n",
      "ðŸ˜ˆ\n",
      "\n",
      "ðŸ‘¨\n",
      "ðŸ‘©\n",
      "\n",
      "ðŸ’œ\n",
      "\n",
      "ðŸ™Œ\n",
      "\n",
      "ðŸ‘…\n",
      "\n",
      "ðŸ˜‚\n",
      "\n",
      "âš½\n",
      "\n",
      "ðŸ¦\n",
      "\n",
      "â¤\n",
      "\n",
      "ðŸ˜‚\n",
      "\n",
      "â„¢\n",
      "\n",
      "â˜\n",
      "âœŠ\n",
      "ðŸ˜‚\n",
      "ðŸ˜Œ\n",
      "ðŸ˜›\n",
      "ðŸ˜«\n",
      "ðŸ™ˆ\n",
      "\n",
      "ðŸ˜¤\n",
      "\n",
      "ðŸ˜±\n",
      "\n",
      "ðŸ™\n",
      "\n",
      "ðŸ˜¤\n",
      "\n",
      "ðŸ˜\n",
      "ðŸ˜«\n",
      "ðŸ˜­\n",
      "\n",
      "ðŸ˜‚\n",
      "\n",
      "ðŸ’¯\n",
      "ðŸ“¡\n",
      "ðŸ˜ƒ\n",
      "ðŸ˜\n",
      "ðŸ˜˜\n",
      "\n",
      "ðŸ˜”\n",
      "\n",
      "ðŸ’”\n",
      "ðŸ˜‚\n",
      "\n",
      "ðŸŒ™\n",
      "ðŸŒŸ\n",
      "\n",
      "ðŸ˜‚\n",
      "\n",
      "ðŸ‘€\n",
      "\n",
      "ðŸŒ\n",
      "\n",
      "ðŸ˜­\n",
      "\n",
      "âœŒ\n",
      "ðŸ°\n",
      "ðŸ‘¶\n",
      "\n",
      "ðŸ˜”\n",
      "\n",
      "ðŸ˜\n",
      "\n",
      "ðŸ˜‚\n",
      "\n",
      "ðŸ˜‚\n",
      "\n",
      "ðŸ’š\n",
      "\n",
      "ðŸ‘€\n",
      "ðŸ”Ž\n",
      "\n",
      "ðŸ’”\n",
      "\n",
      "ðŸ‘Š\n",
      "\n",
      "ðŸ˜­\n",
      "\n",
      "ðŸ˜‚\n",
      "ðŸ˜­\n",
      "\n",
      "â¤\n",
      "\n",
      "ðŸ˜‚\n",
      "\n",
      "ðŸ˜‚\n",
      "\n",
      "ðŸ˜‚\n",
      "\n",
      "ðŸ‘\n",
      "\n",
      "ðŸ˜­\n",
      "\n",
      "âœ¨\n",
      "\n",
      "ðŸ˜‚\n",
      "\n",
      "ðŸ˜‚\n",
      "\n",
      "ðŸ˜‚\n",
      "\n",
      "ðŸ˜³\n",
      "\n",
      "ðŸ˜\n",
      "\n",
      "ðŸ’›\n",
      "\n",
      "ðŸ’ª\n",
      "\n",
      "ðŸ˜\n",
      "\n",
      "â™¥\n",
      "ðŸ°\n",
      "ðŸ’œ\n",
      "\n",
      "â™¥\n",
      "ðŸ™\n",
      "\n",
      "ðŸ˜Š\n",
      "\n",
      "ðŸ˜‚\n",
      "\n",
      "ðŸ˜\n",
      "\n",
      "ðŸ‘\n",
      "\n",
      "âž¡\n",
      "\n",
      "ðŸ“‹\n",
      "\n",
      "ðŸ²\n",
      "ðŸ””\n",
      "ðŸ”¥\n",
      "\n",
      "ðŸ’ª\n",
      "\n",
      "ðŸ‘\n",
      "\n",
      "ðŸ˜…\n",
      "\n",
      "â¬‡\n",
      "ðŸ’»\n",
      "ðŸ“±\n",
      "\n",
      "ðŸ˜‚\n",
      "\n",
      "ðŸ˜‚\n",
      "\n",
      "âŒ\n",
      "ðŸ’°\n",
      "\n",
      "â³\n",
      "âœ…\n",
      "ðŸ‘‰\n",
      "ðŸ’›\n",
      "ðŸ’°\n",
      "\n",
      "ðŸ’œ\n",
      "\n",
      "ðŸ˜…\n",
      "\n",
      "ðŸ’™\n",
      "ðŸ˜­\n",
      "\n",
      "ðŸ‘€\n",
      "\n",
      "ðŸ‘Œ\n",
      "\n",
      "ðŸ˜˜\n",
      "\n",
      "ðŸ’™\n",
      "ðŸ˜‹\n",
      "\n",
      "ðŸ˜\n",
      "\n",
      "ðŸ˜\n",
      "\n",
      "ðŸ’œ\n",
      "\n",
      "ðŸ˜“\n",
      "\n",
      "â€¼\n",
      "ðŸ’¥\n",
      "\n",
      "ðŸ“–\n",
      "\n",
      "âœ”\n",
      "ðŸ‘€\n",
      "ðŸ‘…\n",
      "\n",
      "ðŸ˜…\n",
      "\n",
      "ðŸ”¥\n",
      "\n",
      "ðŸ˜˜\n",
      "\n",
      "â¤\n",
      "ðŸ˜Š\n",
      "\n",
      "ðŸ’¦\n",
      "ðŸ˜\n",
      "\n",
      "ðŸ˜­\n",
      "\n",
      "ðŸ˜­\n",
      "\n",
      "ðŸ’œ\n",
      "\n",
      "ðŸŽ¼\n",
      "\n",
      "ðŸ˜‚\n",
      "\n",
      "ðŸ˜‚\n",
      "ðŸ˜­\n",
      "\n",
      "ðŸ”¥\n",
      "\n",
      "ðŸ¯\n",
      "ðŸ»\n",
      "\n",
      "ðŸ˜‚\n",
      "\n",
      "ðŸ˜‚\n",
      "ðŸ˜ˆ\n",
      "\n",
      "ðŸ’\n",
      "\n",
      "ðŸ“„\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS\n",
    "from sklearn.decomposition import NMF\n",
    "import math\n",
    "from collections import Counter\n",
    "import re\n",
    "\n",
    "def find_emoji(word,df_emoji):\n",
    "    options = []\n",
    "    for i, name in enumerate(df_emoji['short_name']):\n",
    "        if word in name:\n",
    "            options.append((name,df_emoji['unichar'][i]))\n",
    "            print(name,df_emoji['unichar'][i])\n",
    "    return options\n",
    "\n",
    "def get_emojis(tweet_lst,df_emoji):\n",
    "    emoji_idx = []\n",
    "    emoji_char =[]\n",
    "    for tweet in tweet_lst:\n",
    "        for i,uni in enumerate(df_emoji['unichar']):\n",
    "            if uni in tweet:\n",
    "                 emoji_idx.append(i)\n",
    "                 emoji_char.append(uni)\n",
    "    return emoji_idx, emoji_char\n",
    "\n",
    "def get_emojis_by_tweet(tweet_lst,df_emoji):\n",
    "    by_tweet = []\n",
    "\n",
    "    for tweet in tweet_lst:\n",
    "        emoji_char =[]\n",
    "        for i,uni in enumerate(df_emoji['unichar']):\n",
    "            if uni in tweet:\n",
    "                 emoji_char.append(uni)\n",
    "        by_tweet.append(emoji_char)\n",
    "    return by_tweet\n",
    "\n",
    "#this seems to get some emojis that i dont but also missed some that i do. It also get duplicates per tweet that i dont\n",
    "# def get_emojis_2(tweet_lst):\n",
    "#     emojis = []\n",
    "#     for tweet in tweet_lst:\n",
    "#         emoji = re.compile(u'[\\uD800-\\uDBFF][\\uDC00-\\uDFFF]')\n",
    "#         emojis.append(emoji.findall(tweet))\n",
    "#     return emojis\n",
    "\n",
    "def print_emoji(tweet,emoji_char):\n",
    "    for uni in emoji_char:\n",
    "        if uni in tweet:\n",
    "            print(uni),\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    tweets = np.array(list(pickle.load(open('./data/yay_moji.pkl','rb'))))\n",
    "    # type is list\n",
    "\n",
    "    emojis = pd.read_pickle('./data/df_emojis.pkl')\n",
    "    # type is DataFrame\n",
    "\n",
    "\n",
    "\n",
    "    # ------------- tfidf\n",
    "    stopwords = set(list(ENGLISH_STOP_WORDS) + ['rt', 'follow', 'dm', 'https', 'ur', 'll' ,'amp', 'subscribe', 'don', 've', 'retweet', 'im', 'http'])\n",
    "\n",
    "    tfidf = TfidfVectorizer(max_features=10000, max_df=0.05, min_df=0.001, stop_words = stopwords, ngram_range = (1,2))\n",
    "\n",
    "    #lemmetizing need to consider cleaning the tweets myself\n",
    "\n",
    "    tfidf_tweets = tfidf.fit_transform(tweets)\n",
    "    bag = np.array(tfidf.get_feature_names())\n",
    "\n",
    "    # -------------- NMF\n",
    "    k = 10\n",
    "     #number of groups\n",
    "    nmf2 = NMF(n_components = k)\n",
    "    nmf2.fit(tfidf_tweets)\n",
    "    W = nmf2.transform(tfidf_tweets) #len(yay_moji,k)\n",
    "    H = nmf2.components_ #k,len(yay_moji)\n",
    "\n",
    "\n",
    "    # --------------- Printing Top 10\n",
    "    tweet_lst = []\n",
    "    top = 10\n",
    "    tweet_in_group_thresh = .001 #score thresh if we consider that tweet as part of that group\n",
    "    for group in range(k):\n",
    "        #idx of the top ten words for each group\n",
    "        i_words = np.argsort(H[group])[::-1][:top]\n",
    "        words = bag[i_words]\n",
    "\n",
    "        # idx of the top ten tweets for each group\n",
    "        i_emojis = np.argsort(W[:,group])[::-1][:top]\n",
    "        # most common 10 emojis for each group\n",
    "\n",
    "        print('-'*10)\n",
    "        print('Group:',group)\n",
    "        counted_tweets = np.argwhere(W[:,group] > tweet_in_group_thresh)\n",
    "        print(counted_tweets.shape[0], 'tweets')\n",
    "        for word in words:\n",
    "            print('-->',word)\n",
    "        for i_tweet in i_emojis:\n",
    "            print_emoji(tweets[i_tweet], emojis['unichar'])\n",
    "            tweet_lst.append(tweets[i_tweet])\n",
    "        ind, emo_lst = get_emojis(tweets[i_emojis],emojis)\n",
    "        # find percentage of emoji per group\n",
    "        most_emoji, how_many = Counter(emo_lst).most_common(1)[0]\n",
    "        score = float(how_many)/top\n",
    "        # print score #score is not perfect - similar emojis and repeat in the same tweet\n",
    "        print('\\n')\n",
    "\n",
    "    # --------------- printing most common emojis\n",
    "    most_common = 50\n",
    "    b,all_emojis = get_emojis(tweets,emojis)\n",
    "    count = Counter(all_emojis).most_common(most_common)\n",
    "    unicode_top = []\n",
    "    for emo, i in count:\n",
    "        print(emo,i)\n",
    "#        for j, char in enumerate(emojis['unichar']):\n",
    "#            if char == emo:\n",
    "#                unicode_top.append(emojis['unified'][j])\n",
    "\n",
    "\n",
    "# test stuff\n",
    "    jan = get_emojis_by_tweet(tweets[0:100],emojis)\n",
    "    for tweet in jan:\n",
    "        for emo in tweet:\n",
    "            print(emo),\n",
    "        print('')\n",
    "    # name_of = find_emoji('heart',emojis)\n",
    "\n",
    "\n",
    "    '''\n",
    "    to do's\n",
    "    --> how big are the groups? do a most common\n",
    "    --> get a better score system\n",
    "    --> allow for tweets with multiple emojis\n",
    "    --> sub set for tweets with a specific emoji\n",
    "    --> commonly combined emojis\n",
    "    --> naive bayes\n",
    "        prediction accuracy between emojis for how similar they are\n",
    "    whats the purpose:\n",
    "    --> to help use emojis as labels for tweets\n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'model_emoji'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-b00005532d74>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcollections\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCounter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mmodel_emoji\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_emojis_by_tweet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprint_emoji\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_emojis\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnaive_bayes\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mGaussianNB\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'model_emoji'"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS\n",
    "from sklearn.decomposition import NMF\n",
    "import math\n",
    "from collections import Counter\n",
    "import re\n",
    "from model_emoji import get_emojis_by_tweet, print_emoji, get_emojis\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "\n",
    "def sub_set(tweets,emojis,emo_char):\n",
    "    sub = []\n",
    "    by_tweets = get_emojis_by_tweet(tweets,emojis)\n",
    "    for tweet,emos in zip(tweets,by_tweets):\n",
    "        if emo_char in tweet:\n",
    "            sub.append((tweet,emos))\n",
    "    return sub\n",
    "\n",
    "def wordize(tweet):\n",
    "    stops = string.digits + '_@'\n",
    "    words = tweet.split()\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        yes = True\n",
    "        for char in stops:\n",
    "            if char in word:\n",
    "                yes = False\n",
    "        if yes:\n",
    "            new_words.append(word)\n",
    "\n",
    "    return ' '.join(new_words)\n",
    "\n",
    "\n",
    "def NB_acc(emo1, emo2, tweets, emoji):\n",
    "    a = sub_set(tweets,emojis, emo1)\n",
    "    b = sub_set(tweets,emojis, emo2)\n",
    "    labels = np.hstack((np.zeros(len(a)),np.ones(len(b))))\n",
    "    # me and\n",
    "\n",
    "    tweets_a, emos_a = zip(*a)\n",
    "    tweets_b, emos_b = zip(*b)\n",
    "\n",
    "    tweets = list(tweets_a) + list(tweets_b)\n",
    "\n",
    "# ------------- tfidf\n",
    "    stopwords = set(list(ENGLISH_STOP_WORDS) + ['rt', 'follow', 'dm', 'https', 'ur', 'll' ,'amp', 'subscribe', 'don', 've', 'retweet', 'im', 'http'])\n",
    "\n",
    "    tfidf = TfidfVectorizer(max_features=10000, max_df=0.3, min_df = .001, stop_words = stopwords, ngram_range = (1, 2))\n",
    "\n",
    "    #lemmetizing need to consider cleaning the tweets myself\n",
    "\n",
    "    tfidf_tweets = tfidf.fit_transform(tweets)\n",
    "    bag = np.array(tfidf.get_feature_names())\n",
    "\n",
    "\n",
    "# ------------------ predict this emoji or another\n",
    "\n",
    "    # ----- train test split\n",
    "    np.random.seed(2)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(tfidf_tweets.todense(),labels)\n",
    "\n",
    "    nb = GaussianNB()\n",
    "    mod = nb.fit(X_train,y_train)\n",
    "    y_pred = mod.predict(X_test)\n",
    "\n",
    "    acc = np.mean(y_test == y_pred)\n",
    "    return acc\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    tweets1 = np.array(list(pickle.load(open('./data/yay_moji.pkl','rb'))))\n",
    "    # type is list\n",
    "    emojis = pd.read_pickle('./data/df_emojis.pkl')\n",
    "    # type is DataFrame\n",
    "\n",
    "    #removing words with digits and ['_@']\n",
    "    tweets = [wordize(tweet) for tweet in tweets1]\n",
    "\n",
    "    sparkle = u'\\u2728'\n",
    "    laugh_cry = u'\\U0001f602'\n",
    "    heart_face = u'\\U0001f60d'\n",
    "    praise = u'\\U0001f64c'\n",
    "    earth = u'\\U0001f30d'\n",
    "\n",
    "    a,b=get_emojis(tweets, emojis)\n",
    "\n",
    "    n = 100 #must be even\n",
    "\n",
    "    # do a similarity matrix instead\n",
    "    emoji_choice = Counter(b).most_common(100)\n",
    "    rnd = np.random.randint(0,100,n)\n",
    "    half_rnd = zip(rnd[:n/2],rnd[n/2:])\n",
    "    connections = []\n",
    "    b = emoji_choice\n",
    "    # for a, b in half_rnd:\n",
    "    for b in range(10):\n",
    "        a = 0\n",
    "        emo1 = emoji_choice[a][0]\n",
    "        num1 = emoji_choice[a][1]\n",
    "        emo2 = emoji_choice[b][0]\n",
    "        num2 = emoji_choice[b][1]\n",
    "        acc = NB_acc(emo1, emo2, tweets, emojis)\n",
    "        connections.append([emo1, num1, emo2, num2, acc])\n",
    "        print(emo1, num1, emo2, num2, acc)\n",
    "    emo1, num1, emo2, num2, acc = zip(*connections)\n",
    "\n",
    "    sorted_best = np.argsort(acc)[::-1]\n",
    "    for i in sorted_best:\n",
    "        print(emo1[i],num1[i],emo2[i],num2[i],acc[i])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
