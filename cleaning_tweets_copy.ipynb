{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Emojis Speak More than Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GOAL: \n",
    "    1. give an \"issue word\" as an input (ex. ocasio, climate change) and find the most related emoji\n",
    "    to kinda grasp people's opinions\n",
    "    2. give any word or a saying and get a emoji that is most related ex. sparkle --> âœ¨"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make a columns for emojis and its corresponding tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/sara/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "import nltk.tokenize as tk\n",
    "import en_core_web_sm\n",
    "import string\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "nlp = en_core_web_sm.load()\n",
    "\n",
    "stopwords = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = list(pickle.load(open('./data/yay_moji.pkl','rb')))\n",
    "mojis = pd.read_pickle('./data/df_emoji.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'RT @Fancy2Nancy3: ðŸš¨ ATTN  PATRIOTS ðŸš¨ \\nPlease Retweet &amp; Follow        ðŸ‡ºðŸ‡¸@Commonm69164249ðŸ‡ºðŸ‡¸        \\n    ðŸŽ‰ Help Reach ðŸŽ‰\\nðŸ”¥5K FOLLOWERS ðŸ”¥ \\nðŸ‡ºðŸ‡¸ Aâ€¦'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords.extend(['\\'s','â€™s','rt','â€¦','ï¸','...','follow', 'dm', 'https', 'ur', 'll' ,'amp', 'subscribe', 'don', 've', 'retweet', 'im', 'http'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean text \n",
    "punctuations = string.punctuation\n",
    "\n",
    "# Define function to cleanup text by removing personal pronouns, stopwords, and puncuation\n",
    "def cleanup_text(docs, logging=False):\n",
    "    texts = []\n",
    "    counter = 1\n",
    "    for doc in docs:\n",
    "        if counter % 1000 == 0 and logging:\n",
    "            print(\"Processed %d out of %d documents.\" % (counter, len(docs)))\n",
    "        counter += 1\n",
    "        doc = nlp(doc, disable=['parser', 'ner'])\n",
    "        tokens = [tok.lemma_.lower().strip() for tok in doc if tok.lemma_ != '-PRON-']\n",
    "        tokens = [tok for tok in tokens if tok not in stopwords and tok not in punctuations]\n",
    "        tokens = ' '.join(tokens)\n",
    "        tokens = re.sub('@[^\\s]+','', tokens)\n",
    "        texts.append(tokens)\n",
    "    return pd.Series(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tweets = pd.DataFrame(tweets, columns=['tweet'])\n",
    "#tw = [word for word in tweets['tweet']]\n",
    "\n",
    "# Clean up all text\n",
    "tw_clean = cleanup_text(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "letters = 'ðŸ‡¦ ðŸ‡§ ðŸ‡¨ ðŸ‡© ðŸ‡ª ðŸ‡« ðŸ‡¬ ðŸ‡­ ðŸ‡® ðŸ‡¯ ðŸ‡° ðŸ‡± ðŸ‡² ðŸ‡³ ðŸ‡´ ðŸ‡µ ðŸ‡¶ ðŸ‡· ðŸ‡¸ ðŸ‡¹ ðŸ‡º ðŸ‡» ðŸ‡¼ ðŸ‡½ ðŸ‡¾ ðŸ‡¿'.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "flags = mojis['unichar'][1458:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change letters to flags\n",
    "def fix_flags(tweets):\n",
    "    fixed = []\n",
    "    for tweet in tweets:\n",
    "        for l in letters:\n",
    "            if l in tweet:\n",
    "                tweet = re.sub(l+\" \", l, tweet)\n",
    "        fixed.append(tweet)\n",
    "    return(fixed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tw_clean_flags = fix_flags(tw_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# put space after flag emojis\n",
    "def flags_space(tweets):\n",
    "    fixed = []\n",
    "    for tweet in tweets:\n",
    "        for l in flags:\n",
    "            if l in tweet:\n",
    "                tweet = re.sub(l, l+\" \", tweet)\n",
    "        fixed.append(tweet)\n",
    "    return(fixed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "tw_cleaned = flags_space(tw_clean_flags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# complete list of emojis\n",
    "from emoji import UNICODE_EMOJI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "emojis = list(UNICODE_EMOJI.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_mojis(tweets):\n",
    "    emoji = defaultdict(list)\n",
    "\n",
    "    for i, tweet in enumerate(tweets):\n",
    "        for word in tweet.split():\n",
    "            if word in emojis:\n",
    "                emoji['emoji'].append(word)\n",
    "                emoji['index'].append(i)\n",
    "    \n",
    "    # delete overlapping emojis in a tweet\n",
    "    emoji = pd.DataFrame(emoji).drop_duplicates()\n",
    "    \n",
    "    return(emoji)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted = extract_mojis(tw_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_emojis(tweets):\n",
    "    no_emojis = []\n",
    "    \n",
    "    for tweet in tweets:\n",
    "        tweet = tweet.split()\n",
    "        words = []\n",
    "        \n",
    "        for word in tweet:\n",
    "            if word not in list(extracted['emoji']):\n",
    "                words.append(word)\n",
    "        words = ' '.join(words)\n",
    "        no_emojis.append(words)\n",
    "    return(no_emojis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "tw_no_emo = remove_emojis(tw_cleaned)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are still some unremoved emojis but I will come back to that later. Maybe try tdidf..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df = pd.DataFrame(tw_no_emo, columns = ['tweets'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_merged = pd.merge(extracted, tweets_df.reset_index(), on='index', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "# emojis with at least 10 tweets\n",
    "enough_emoji = tweets_merged.groupby('emoji').count()[tweets_merged.groupby('emoji').count()['tweets']>=10]\n",
    "enough_emoji = pd.merge(enough_emoji.reset_index()[['emoji']], tweets_merged, on='emoji', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>emoji</th>\n",
       "      <th>index</th>\n",
       "      <th>tweets</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ðŸ˜‚</td>\n",
       "      <td>0</td>\n",
       "      <td>john daly actually ride cart major man live go...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ðŸš¨</td>\n",
       "      <td>1</td>\n",
       "      <td>attn patriots please retweet amp follow help r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ðŸ‡ºðŸ‡¸</td>\n",
       "      <td>1</td>\n",
       "      <td>attn patriots please retweet amp follow help r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ðŸŽ‰</td>\n",
       "      <td>1</td>\n",
       "      <td>attn patriots please retweet amp follow help r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ðŸ”¥</td>\n",
       "      <td>1</td>\n",
       "      <td>attn patriots please retweet amp follow help r...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  emoji  index                                             tweets\n",
       "0     ðŸ˜‚      0  john daly actually ride cart major man live go...\n",
       "1     ðŸš¨      1  attn patriots please retweet amp follow help r...\n",
       "2    ðŸ‡ºðŸ‡¸      1  attn patriots please retweet amp follow help r...\n",
       "3     ðŸŽ‰      1  attn patriots please retweet amp follow help r...\n",
       "4     ðŸ”¥      1  attn patriots please retweet amp follow help r..."
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_merged.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['ðŸ˜‚', 'ðŸš¨', 'ðŸ‡ºðŸ‡¸', ..., 'ðŸ', 'ðŸ˜Ž', 'â­'], dtype=object)"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_merged['emoji'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------\n",
      "Group: 0\n",
      "256 tweets\n",
      "--> love\n",
      "--> yes\n",
      "--> gonna\n",
      "--> cute\n",
      "--> life\n",
      "--> people\n",
      "--> amazing\n",
      "--> guys\n",
      "--> friends\n",
      "--> page\n",
      "ðŸ’›\n",
      "ðŸ‡ºðŸ‡¸\n",
      "ðŸ™Š\n",
      "â™¥\n",
      "ðŸ˜\n",
      "ðŸ¤¯\n",
      "â™¥\n",
      "ðŸ‡ºðŸ‡¸\n",
      "ðŸ˜\n",
      "ðŸ’¯\n",
      "ðŸ¶\n",
      "ðŸ˜‚\n",
      "ðŸ‡§ðŸ‡©\n",
      "ðŸ‡©ðŸ‡ª\n",
      "ðŸ‡¬ðŸ‡§\n",
      "ðŸ˜\n",
      "â˜º\n",
      "ðŸ¤—\n",
      "ðŸ™ˆ\n",
      "ðŸ™‰\n",
      "ðŸ™Š\n",
      "ðŸ’•\n",
      "â¤\n",
      "ðŸ’›\n",
      "ðŸ’™\n",
      "ðŸ’œ\n",
      "ðŸ’«\n",
      "ðŸ¶\n",
      "ðŸ¦„\n",
      "ðŸ»\n",
      "ðŸ£\n",
      "ðŸŒ¸\n",
      "ðŸŒº\n",
      "ðŸŒž\n",
      "â­\n",
      "ðŸŒˆ\n",
      "â˜ƒ\n",
      "âœ¨\n",
      "ðŸ˜\n",
      "ðŸ’ƒ\n",
      "ðŸ˜\n",
      "\n",
      "\n",
      "----------\n",
      "Group: 1\n",
      "749 tweets\n",
      "--> just\n",
      "--> need\n",
      "--> think\n",
      "--> man\n",
      "--> really\n",
      "--> shit\n",
      "--> woke\n",
      "--> wow\n",
      "--> just like\n",
      "--> little\n",
      "ðŸ¤¯\n",
      "ðŸ˜‚\n",
      "ðŸ‡ºðŸ‡¸\n",
      "ðŸ¤¢\n",
      "ðŸŒ±\n",
      "ðŸ™Š\n",
      "ðŸ˜‚\n",
      "â¤\n",
      "ðŸ™\n",
      "ðŸ‡ºðŸ‡¸\n",
      "ðŸ’Ž\n",
      "ðŸ‡©ðŸ‡ª\n",
      "ðŸ¤¯\n",
      "\n",
      "\n",
      "----------\n",
      "Group: 2\n",
      "902 tweets\n",
      "--> like\n",
      "--> people\n",
      "--> got\n",
      "--> just like\n",
      "--> feel\n",
      "--> hurt\n",
      "--> cute\n",
      "--> look\n",
      "--> change\n",
      "--> feel like\n",
      "ðŸ˜‚\n",
      "ðŸ˜±\n",
      "ðŸ¤¯\n",
      "ðŸ™ˆ\n",
      "ðŸ™Š\n",
      "ðŸ’™\n",
      "ðŸ™Š\n",
      "ðŸ˜‚\n",
      "â˜º\n",
      "ðŸ’œ\n",
      "ðŸ¯\n",
      "ðŸ˜…\n",
      "ðŸ˜¢\n",
      "ðŸ˜‚\n",
      "ðŸ¤£\n",
      "\n",
      "\n",
      "----------\n",
      "Group: 3\n",
      "778 tweets\n",
      "--> good\n",
      "--> morning\n",
      "--> good morning\n",
      "--> know\n",
      "--> better\n",
      "--> news\n",
      "--> luck\n",
      "--> night\n",
      "--> hope\n",
      "--> post_abortive87\n",
      "ðŸ˜©\n",
      "ðŸ‘\n",
      "ðŸ‡ºðŸ‡¸\n",
      "â­\n",
      "ðŸ¥º\n",
      "ðŸ¥º\n",
      "ðŸ™Š\n",
      "ðŸ˜‚\n",
      "ðŸ™„\n",
      "ðŸ˜Ž\n",
      "ðŸ¤£\n",
      "ðŸ˜‚\n",
      "ðŸ\n",
      "ðŸ‡ºðŸ‡¸\n",
      "ðŸŒ¹\n",
      "ðŸ‡°ðŸ‡·\n",
      "ðŸ‡¸ðŸ‡¦\n",
      "\n",
      "\n",
      "----------\n",
      "Group: 4\n",
      "1091 tweets\n",
      "--> trump\n",
      "--> realdonaldtrump\n",
      "--> president\n",
      "--> america\n",
      "--> god\n",
      "--> president trump\n",
      "--> 2020\n",
      "--> people\n",
      "--> god bless\n",
      "--> bless\n",
      "ðŸ‡ºðŸ‡¸\n",
      "ðŸ‡ºðŸ‡¸\n",
      "ðŸ‘\n",
      "ðŸ‘\n",
      "ðŸ‡ºðŸ‡¸\n",
      "ðŸ¤¦\n",
      "ðŸ¤¦â€â™€ï¸\n",
      "â™€\n",
      "ðŸ˜‚\n",
      "ðŸ‡ºðŸ‡¸\n",
      "â¤\n",
      "ðŸ‘\n",
      "ðŸ™\n",
      "ðŸ‡ºðŸ‡¸\n",
      "ðŸ‡ºðŸ‡¸\n",
      "ðŸ‡ºðŸ‡¸\n",
      "\n",
      "\n",
      "----------\n",
      "Group: 5\n",
      "495 tweets\n",
      "--> vegan\n",
      "--> food\n",
      "--> let\n",
      "--> friend\n",
      "--> years\n",
      "--> 2019\n",
      "--> lol\n",
      "--> think\n",
      "--> animals\n",
      "--> mind\n",
      "ðŸŒ±\n",
      "ðŸ˜¢\n",
      "ðŸ˜\n",
      "ðŸ‘\n",
      "ðŸ˜€\n",
      "ðŸ¤®\n",
      "Â©\n",
      "ðŸ´\n",
      "ðŸ˜\n",
      "ðŸŒ±\n",
      "ðŸ’š\n",
      "ðŸŒ±\n",
      "ðŸ˜­\n",
      "\n",
      "\n",
      "----------\n",
      "Group: 6\n",
      "1217 tweets\n",
      "--> day\n",
      "--> today\n",
      "--> great\n",
      "--> happy\n",
      "--> july\n",
      "--> 4th\n",
      "--> hope\n",
      "--> going\n",
      "--> 4th july\n",
      "--> great day\n",
      "â™¥\n",
      "ðŸ¤¯\n",
      "â¤\n",
      "ðŸŒ¨\n",
      "ðŸŒ«\n",
      "â˜”\n",
      "ðŸ˜‚\n",
      "ðŸ’¨\n",
      "ðŸ‘†\n",
      "ðŸŽ\n",
      "ðŸ\n",
      "ðŸ‡©ðŸ‡ª\n",
      "ðŸ‡®ðŸ‡¹\n",
      "ðŸ‡ºðŸ‡¸\n",
      "ðŸ’«\n",
      "ðŸ™Œ\n",
      "ðŸ’›\n",
      "â™¥\n",
      "ðŸ¤“\n",
      "ðŸ¤¯\n",
      "\n",
      "\n",
      "----------\n",
      "Group: 7\n",
      "1498 tweets\n",
      "--> time\n",
      "--> new\n",
      "--> world\n",
      "--> 10\n",
      "--> win\n",
      "--> week\n",
      "--> home\n",
      "--> did\n",
      "--> know\n",
      "--> chance\n",
      "ðŸ˜‚\n",
      "ðŸ‡ºðŸ‡¸\n",
      "ðŸ˜‚\n",
      "âœŒ\n",
      "âš¡\n",
      "â™€\n",
      "ðŸ¤¯\n",
      "ðŸ˜¤\n",
      "â¤\n",
      "ðŸ˜˜\n",
      "ðŸ¤—\n",
      "ðŸŒ¼\n",
      "â˜¹\n",
      "ðŸ˜\n",
      "ðŸ’ž\n",
      "ðŸ‘\n",
      "ðŸ¶\n",
      "ðŸ±\n",
      "\n",
      "\n",
      "----------\n",
      "Group: 8\n",
      "695 tweets\n",
      "--> thank\n",
      "--> great\n",
      "--> yes\n",
      "--> beautiful\n",
      "--> guys\n",
      "--> right\n",
      "--> hope\n",
      "--> look\n",
      "--> following\n",
      "--> mr\n",
      "ðŸ’™\n",
      "ðŸ‘\n",
      "â™¥\n",
      "ðŸ˜Š\n",
      "ðŸ’ª\n",
      "ðŸ‡ºðŸ‡¸\n",
      "ðŸ™Š\n",
      "ðŸ’“\n",
      "â¤\n",
      "ðŸ™\n",
      "ðŸ˜ˆ\n",
      "ðŸ’˜\n",
      "â£\n",
      "ðŸ™ˆ\n",
      "ðŸ˜˜\n",
      "ðŸ’š\n",
      "\n",
      "\n",
      "----------\n",
      "Group: 9\n",
      "1062 tweets\n",
      "--> best\n",
      "--> want\n",
      "--> followers\n",
      "--> followers want\n",
      "--> 1k\n",
      "--> 3k\n",
      "--> 4k\n",
      "--> 2k\n",
      "--> world\n",
      "--> going\n",
      "ðŸŽŸ\n",
      "ðŸ˜\n",
      "â¤\n",
      "ðŸ‡ºðŸ‡¸\n",
      "â˜ \n",
      "ðŸ´\n",
      "ðŸ´â€â˜ ï¸\n",
      "ðŸ‡ºðŸ‡¸\n",
      "â¤\n",
      "ðŸ‡¦ðŸ‡º\n",
      "ðŸ‡¬ðŸ‡§\n",
      "ðŸ‡®ðŸ‡³\n",
      "ðŸ‡°ðŸ‡ª\n",
      "ðŸ‡ºðŸ‡²\n",
      "ðŸ‡¦ðŸ‡º\n",
      "ðŸ‡¬ðŸ‡§\n",
      "ðŸ‡®ðŸ‡³\n",
      "ðŸ‡µðŸ‡°\n",
      "ðŸ‡§ðŸ‡·\n",
      "ðŸ‡¨ðŸ‡¦\n",
      "ðŸ‡¬ðŸ‡§\n",
      "ðŸ‡³ðŸ‡¬\n",
      "ðŸ‡ºðŸ‡²\n",
      "ðŸ‡¦ðŸ‡º\n",
      "ðŸ‡¬ðŸ‡§\n",
      "ðŸ‡®ðŸ‡³\n",
      "ðŸ‡µðŸ‡°\n",
      "\n",
      "\n",
      "ðŸ‡ºðŸ‡¸ 996\n",
      "ðŸ¤¯ 400\n",
      "ðŸ˜‚ 395\n",
      "ðŸ˜ 219\n",
      "ðŸ™Š 211\n",
      "â¤ 203\n",
      "ðŸ”¥ 141\n",
      "ðŸ˜­ 127\n",
      "ðŸ‘ 101\n",
      "ðŸ‡©ðŸ‡ª 95\n",
      "ðŸ‡°ðŸ‡· 93\n",
      "ðŸ‡¬ðŸ‡§ 86\n",
      "ðŸ™ 81\n",
      "ðŸ¤£ 61\n",
      "ðŸ‘ 58\n",
      "ðŸ‡¯ðŸ‡µ 57\n",
      "â™€ 53\n",
      "ðŸ’œ 53\n",
      "ðŸ‡¨ðŸ‡¦ 52\n",
      "ðŸ™ˆ 47\n",
      "ðŸ’™ 45\n",
      "ðŸ’š 45\n",
      "â™¥ 44\n",
      "âœ¨ 44\n",
      "ðŸ˜˜ 43\n",
      "â™‚ 43\n",
      "ðŸ’¯ 42\n",
      "ðŸ˜Ž 42\n",
      "ðŸ‡«ðŸ‡· 42\n",
      "ðŸ˜Š 42\n",
      "ðŸ¥º 41\n",
      "ðŸ’ª 40\n",
      "ðŸ’• 38\n",
      "ðŸ‡§ðŸ‡· 38\n",
      "ðŸ‡³ðŸ‡¬ 38\n",
      "ðŸ‘€ 37\n",
      "ðŸ‡¦ðŸ‡º 37\n",
      "ðŸŽ‰ 37\n",
      "ðŸ™Œ 36\n",
      "ðŸ‡µðŸ‡° 36\n",
      "ðŸš¨ 35\n",
      "ðŸ‡®ðŸ‡³ 35\n",
      "ðŸ‡ªðŸ‡¸ 35\n",
      "ðŸ¤” 33\n",
      "ðŸ˜© 32\n",
      "ðŸ’¥ 32\n",
      "ðŸŒŽ 31\n",
      "ðŸ¤¦ 30\n",
      "ðŸ¤· 29\n",
      "ðŸ˜³ 29\n",
      "ðŸ˜‚\n",
      "\n",
      "ðŸ†\n",
      "ðŸ”™\n",
      "ðŸ‡ºðŸ‡¸\n",
      "\n",
      "Â®\n",
      "\n",
      "ðŸ¥º\n",
      "\n",
      "ðŸŽ\n",
      "ðŸ»\n",
      "ðŸ‡ºðŸ‡¸\n",
      "\n",
      "ðŸ˜­\n",
      "ðŸ‘‡\n",
      "\n",
      "ðŸ¤¯\n",
      "ðŸ‘€\n",
      "\n",
      "ðŸ˜‚\n",
      "\n",
      "âš¡\n",
      "\n",
      "ðŸ™Š\n",
      "ðŸ’™\n",
      "\n",
      "ðŸ—½\n",
      "\n",
      "ðŸš¨\n",
      "\n",
      "ðŸ˜\n",
      "ðŸŒˆ\n",
      "ðŸ³\n",
      "ðŸ³ï¸â€ðŸŒˆ\n",
      "\n",
      "ðŸ‡ºðŸ‡¸\n",
      "\n",
      "ðŸ˜¬\n",
      "\n",
      "ðŸ¤§\n",
      "\n",
      "ðŸ¤¦\n",
      "â™€\n",
      "\n",
      "âŒ\n",
      "ðŸ‡¦ðŸ‡º\n",
      "ðŸ‡°ðŸ‡·\n",
      "ðŸ‡²ðŸ‡¾\n",
      "\n",
      "ðŸ‡ºðŸ‡¸\n",
      "\n",
      "ðŸ˜©\n",
      "\n",
      "ðŸ˜‚\n",
      "\n",
      "ðŸ“£\n",
      "\n",
      "ðŸ§¹\n",
      "\n",
      "â¤\n",
      "ðŸ¢\n",
      "\n",
      "ðŸ˜\n",
      "ðŸ™Š\n",
      "\n",
      "ðŸ˜\n",
      "ðŸ’œ\n",
      "ðŸ‡ºðŸ‡¸\n",
      "\n",
      "ðŸ’š\n",
      "\n",
      "ðŸ’”\n",
      "ðŸ‡ºðŸ‡¸\n",
      "\n",
      "â™¥\n",
      "\n",
      "â­\n",
      "ðŸ”¸\n",
      "\n",
      "ðŸ‘\n",
      "ðŸ‡ºðŸ‡¸\n",
      "\n",
      "ðŸ‘‡\n",
      "ðŸŒŽ\n",
      "\n",
      "ðŸ™Š\n",
      "ðŸƒ\n",
      "\n",
      "ðŸ˜˜\n",
      "\n",
      "ðŸ‡ºðŸ‡¸\n",
      "\n",
      "ðŸ¤¯\n",
      "\n",
      "ðŸ“¦\n",
      "ðŸ”¨\n",
      "ðŸ”ž\n",
      "âž¡\n",
      "â¬…\n",
      "âœ…\n",
      "ðŸ‡ºðŸ‡¸\n",
      "\n",
      "ðŸ˜“\n",
      "\n",
      "ðŸ™„\n",
      "ðŸ˜¢\n",
      "ðŸšŒ\n",
      "ðŸ‡¬ðŸ‡§\n",
      "ðŸ‡¸ðŸ‡¬\n",
      "ðŸ‡ºðŸ‡¸\n",
      "\n",
      "ðŸ˜‚\n",
      "\n",
      "ðŸ¦„\n",
      "ðŸ‡ºðŸ‡¸\n",
      "\n",
      "ðŸ˜’\n",
      "\n",
      "ðŸ¤¯\n",
      "\n",
      "ðŸ˜\n",
      "ðŸ¤¯\n",
      "ðŸ˜©\n",
      "\n",
      "ðŸ‘‡\n",
      "\n",
      "ðŸ’•\n",
      "ðŸ¤›\n",
      "ðŸ¤œ\n",
      "\n",
      "ðŸ’•\n",
      "ðŸ“½\n",
      "\n",
      "ðŸ˜¡\n",
      "\n",
      "ðŸ˜\n",
      "â¤\n",
      "ðŸ’‡\n",
      "ðŸ—£\n",
      "ðŸ“²\n",
      "â™€\n",
      "\n",
      "ðŸ˜¹\n",
      "\n",
      "ðŸ’¯\n",
      "ðŸ™\n",
      "\n",
      "ðŸš¨\n",
      "ðŸ”—\n",
      "\n",
      "ðŸ˜Ž\n",
      "ðŸ‘‹\n",
      "ðŸ‘\n",
      "âœ”\n",
      "ðŸ‡ºðŸ‡¸\n",
      "\n",
      "ðŸ˜‚\n",
      "\n",
      "â™¥\n",
      "\n",
      "ðŸŽ¾\n",
      "\n",
      "ðŸ”¥\n",
      "ðŸ’°\n",
      "1ï¸âƒ£\n",
      "2ï¸âƒ£\n",
      "\n",
      "ðŸ‘¨\n",
      "ðŸ‘©\n",
      "\n",
      "ðŸ¤¯\n",
      "\n",
      "ðŸŒŽ\n",
      "ðŸ‡ºðŸ‡¸\n",
      "\n",
      "ðŸ™Œ\n",
      "ðŸ‡ºðŸ‡¸\n",
      "\n",
      "ðŸ’œ\n",
      "\n",
      "ðŸ™Š\n",
      "\n",
      "ðŸ˜‡\n",
      "ðŸ’¯\n",
      "ðŸ‡ºðŸ‡¸\n",
      "\n",
      "ðŸ‡ºðŸ‡¸\n",
      "\n",
      "ðŸ‡¦ðŸ‡ª\n",
      "ðŸ‡¦ðŸ‡º\n",
      "ðŸ‡§ðŸ‡ª\n",
      "ðŸ‡§ðŸ‡·\n",
      "ðŸ‡©ðŸ‡ª\n",
      "ðŸ‡¬ðŸ‡§\n",
      "ðŸ‡®ðŸ‡³\n",
      "ðŸ‡°ðŸ‡·\n",
      "ðŸ‡³ðŸ‡«\n",
      "ðŸ‡³ðŸ‡¬\n",
      "ðŸ‡³ðŸ‡±\n",
      "ðŸ‡ºðŸ‡¸\n",
      "ðŸ‡¿ðŸ‡¦\n",
      "ðŸ‡¿ðŸ‡¼\n",
      "\n",
      "ðŸ˜‚\n",
      "\n",
      "ðŸ™Š\n",
      "\n",
      "ðŸ‡¸ðŸ‡ª\n",
      "\n",
      "ðŸ‘‡\n",
      "\n",
      "ðŸ˜\n",
      "ðŸ¤—\n",
      "â¤\n",
      "ðŸ¶\n",
      "ðŸ¾\n",
      "ðŸ‡¦ðŸ‡º\n",
      "ðŸ‡¨ðŸ‡¦\n",
      "ðŸ‡ºðŸ‡¸\n",
      "\n",
      "ðŸ–\n",
      "\n",
      "ðŸ˜‚\n",
      "\n",
      "ðŸ¥š\n",
      "\n",
      "ðŸš¨\n",
      "\n",
      "ðŸ˜¢\n",
      "ðŸ‡ºðŸ‡¸\n",
      "\n",
      "ðŸ˜\n",
      "\n",
      "ðŸ”¥\n",
      "\n",
      "ðŸ’œ\n",
      "ðŸ”¥\n",
      "\n",
      "â¤\n",
      "ðŸ™\n",
      "ðŸ‡ºðŸ‡¸\n",
      "\n",
      "ðŸ¥º\n",
      "ðŸ˜­\n",
      "\n",
      "ðŸ‡¦ðŸ‡²\n",
      "ðŸ‡¬ðŸ‡³\n",
      "ðŸ‡­ðŸ‡°\n",
      "ðŸ‡®ðŸ‡±\n",
      "ðŸ‡®ðŸ‡²\n",
      "ðŸ‡®ðŸ‡¹\n",
      "ðŸ‡¯ðŸ‡²\n",
      "ðŸ‡¯ðŸ‡´\n",
      "ðŸ‡¯ðŸ‡µ\n",
      "ðŸ‡°ðŸ‡ª\n",
      "ðŸ‡°ðŸ‡­\n",
      "ðŸ‡°ðŸ‡²\n",
      "ðŸ‡°ðŸ‡³\n",
      "ðŸ‡°ðŸ‡µ\n",
      "ðŸ‡°ðŸ‡·\n",
      "ðŸ‡°ðŸ‡¼\n",
      "ðŸ‡±ðŸ‡¦\n",
      "ðŸ‡±ðŸ‡®\n",
      "ðŸ‡±ðŸ‡°\n",
      "ðŸ‡±ðŸ‡·\n",
      "ðŸ‡±ðŸ‡¸\n",
      "ðŸ‡±ðŸ‡¹\n",
      "ðŸ‡²ðŸ‡±\n",
      "ðŸ‡²ðŸ‡´\n",
      "ðŸ‡²ðŸ‡·\n",
      "ðŸ‡²ðŸ‡½\n",
      "ðŸ‡²ðŸ‡¾\n",
      "ðŸ‡³ðŸ‡¦\n",
      "ðŸ‡³ðŸ‡¬\n",
      "ðŸ‡³ðŸ‡®\n",
      "ðŸ‡³ðŸ‡±\n",
      "ðŸ‡³ðŸ‡´\n",
      "ðŸ‡³ðŸ‡·\n",
      "ðŸ‡³ðŸ‡º\n",
      "ðŸ‡³ðŸ‡¿\n",
      "ðŸ‡µðŸ‡°\n",
      "ðŸ‡¸ðŸ‡±\n",
      "ðŸ‡¹ðŸ‡±\n",
      "ðŸ‡ºðŸ‡³\n",
      "\n",
      "ðŸ¤¦\n",
      "ðŸ¤¦â€â™€ï¸\n",
      "â™€\n",
      "\n",
      "ðŸ˜¢\n",
      "â¤\n",
      "âœ¨\n",
      "ðŸ‡ºðŸ‡¸\n",
      "\n",
      "ðŸ¤£\n",
      "ðŸ‘\n",
      "ðŸ‡ºðŸ‡¸\n",
      "\n",
      "ðŸ‡ºðŸ‡¸\n",
      "\n",
      "ðŸ—£\n",
      "\n",
      "ðŸ˜­\n",
      "\n",
      "ðŸ‡ºðŸ‡¸\n",
      "\n",
      "ðŸ”¥\n",
      "âœ¨\n",
      "ðŸ‡°ðŸ‡·\n",
      "\n",
      "ðŸ¤£\n",
      "ðŸ¤·\n",
      "â™‚\n",
      "\n",
      "â¤\n",
      "ðŸ‘\n",
      "ðŸ‡ºðŸ‡¸\n",
      "\n",
      "ðŸ’\n",
      "â™€\n",
      "\n",
      "ðŸ‡ºðŸ‡¸\n",
      "\n",
      "ðŸŽ\n",
      "ðŸ»\n",
      "ðŸ‡ºðŸ‡¸\n",
      "\n",
      "ðŸ˜\n",
      "ðŸ’œ\n",
      "ðŸ’¯\n",
      "\n",
      "ðŸ˜­\n",
      "\n",
      "ðŸ˜»\n",
      "ðŸ˜½\n",
      "ðŸ’¬\n",
      "ðŸ™Œ\n",
      "ðŸ“£\n",
      "ðŸŽ¼\n",
      "ðŸ“²\n",
      "ðŸ’±\n",
      "ðŸ”„\n",
      "ðŸ”‚\n",
      "ðŸ‡¦ðŸ‡º\n",
      "\n",
      "ðŸ¦–\n",
      "ðŸŒˆ\n",
      "\n",
      "ðŸ¤¯\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS\n",
    "from sklearn.decomposition import NMF\n",
    "import math\n",
    "from collections import Counter\n",
    "import re\n",
    "\n",
    "def find_emoji(word,df_emoji):\n",
    "    options = []\n",
    "    for i, name in enumerate(df_emoji['short_name']):\n",
    "        if word in name:\n",
    "            options.append((name,df_emoji['unichar'][i]))\n",
    "            print(name,df_emoji['unichar'][i])\n",
    "    return options\n",
    "\n",
    "def get_emojis(tweet_lst,df_emoji):\n",
    "    emoji_idx = []\n",
    "    emoji_char =[]\n",
    "    for tweet in tweet_lst:\n",
    "        for i,uni in enumerate(df_emoji['unichar']):\n",
    "            if uni in tweet:\n",
    "                 emoji_idx.append(i)\n",
    "                 emoji_char.append(uni)\n",
    "    return emoji_idx, emoji_char\n",
    "\n",
    "def get_emojis_by_tweet(tweet_lst,df_emoji):\n",
    "    by_tweet = []\n",
    "\n",
    "    for tweet in tweet_lst:\n",
    "        emoji_char =[]\n",
    "        for i,uni in enumerate(df_emoji['unichar']):\n",
    "            if uni in tweet:\n",
    "                 emoji_char.append(uni)\n",
    "        by_tweet.append(emoji_char)\n",
    "    return by_tweet\n",
    "\n",
    "#this seems to get some emojis that i dont but also missed some that i do. It also get duplicates per tweet that i dont\n",
    "# def get_emojis_2(tweet_lst):\n",
    "#     emojis = []\n",
    "#     for tweet in tweet_lst:\n",
    "#         emoji = re.compile(u'[\\uD800-\\uDBFF][\\uDC00-\\uDFFF]')\n",
    "#         emojis.append(emoji.findall(tweet))\n",
    "#     return emojis\n",
    "\n",
    "def print_emoji(tweet,emoji_char):\n",
    "    for uni in emoji_char:\n",
    "        if uni in tweet:\n",
    "            print(uni)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    tweets = np.array(list(pickle.load(open('./data/yay_moji.pkl','rb'))))\n",
    "    # type is list\n",
    "\n",
    "    emojis = pd.read_pickle('./data/df_emoji.pkl')\n",
    "    # type is DataFrame\n",
    "\n",
    "\n",
    "\n",
    "    # ------------- tfidf\n",
    "    stopwords = set(list(ENGLISH_STOP_WORDS) + ['rt', 'follow', 'dm', 'https', 'ur', 'll' ,'amp', 'subscribe', 'don', 've', 'retweet', 'im', 'http'])\n",
    "\n",
    "    tfidf = TfidfVectorizer(max_features=10000, max_df=0.05, min_df=0.001, stop_words = stopwords, ngram_range = (1,2))\n",
    "\n",
    "    #lemmetizing need to consider cleaning the tweets myself\n",
    "\n",
    "    tfidf_tweets = tfidf.fit_transform(tweets)\n",
    "    bag = np.array(tfidf.get_feature_names())\n",
    "\n",
    "    # -------------- NMF\n",
    "    k = 10\n",
    "     #number of groups\n",
    "    nmf2 = NMF(n_components = k)\n",
    "    nmf2.fit(tfidf_tweets)\n",
    "    W = nmf2.transform(tfidf_tweets) #len(yay_moji,k)\n",
    "    H = nmf2.components_ #k,len(yay_moji)\n",
    "\n",
    "\n",
    "    # --------------- Printing Top 10\n",
    "    tweet_lst = []\n",
    "    top = 10\n",
    "    tweet_in_group_thresh = .005 #score thresh if we consider that tweet as part of that group\n",
    "    for group in range(k):\n",
    "        #idx of the top ten words for each group\n",
    "        i_words = np.argsort(H[group])[::-1][:top]\n",
    "        words = bag[i_words]\n",
    "\n",
    "        # idx of the top ten tweets for each group\n",
    "        i_emojis = np.argsort(W[:,group])[::-1][:top]\n",
    "        # most common 10 emojis for each group\n",
    "\n",
    "        print('-'*10)\n",
    "        print('Group:',group)\n",
    "        counted_tweets = np.argwhere(W[:,group] > tweet_in_group_thresh)\n",
    "        print(counted_tweets.shape[0], 'tweets')\n",
    "        for word in words:\n",
    "            print('-->',word)\n",
    "        for i_tweet in i_emojis:\n",
    "            print_emoji(tweets[i_tweet], emojis['unichar'])\n",
    "            tweet_lst.append(tweets[i_tweet])\n",
    "        ind, emo_lst = get_emojis(tweets[i_emojis],emojis)\n",
    "        # find percentage of emoji per group\n",
    "        most_emoji, how_many = Counter(emo_lst).most_common(1)[0]\n",
    "        score = float(how_many)/top\n",
    "        # print score #score is not perfect - similar emojis and repeat in the same tweet\n",
    "        print('\\n')\n",
    "\n",
    "    # --------------- printing most common emojis\n",
    "    most_common = 50\n",
    "    b,all_emojis = get_emojis(tweets,emojis)\n",
    "    count = Counter(all_emojis).most_common(most_common)\n",
    "    unicode_top = []\n",
    "    for emo, i in count:\n",
    "        print(emo,i)\n",
    "        for j, char in enumerate(emojis['unichar']):\n",
    "            if char == emo:\n",
    "                unicode_top.append(emojis['unichar'][j])\n",
    "\n",
    "\n",
    "# test stuff\n",
    "    jan = get_emojis_by_tweet(tweets[0:100],emojis)\n",
    "    for tweet in jan:\n",
    "        for emo in tweet:\n",
    "            print(emo,)\n",
    "        print('')\n",
    "    # name_of = find_emoji('heart',emojis)\n",
    "\n",
    "\n",
    "    '''\n",
    "    to do's\n",
    "    --> how big are the groups? do a most common\n",
    "    --> get a better score system\n",
    "    --> allow for tweets with multiple emojis\n",
    "    --> sub set for tweets with a specific emoji\n",
    "    --> commonly combined emojis\n",
    "    --> naive bayes\n",
    "        prediction accuracy between emojis for how similar they are\n",
    "    whats the purpose:\n",
    "    --> to help use emojis as labels for tweets\n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--> ocasio =\n",
      "ðŸ§¹   3.2315936025548407e-40  \n",
      "ðŸ§¡   5.134160423670935e-69  \n",
      "ðŸ§   3.7119745947176606e-35  \n",
      "ðŸ¦“   0.11111088111755894  \n",
      "ðŸ¦‹   9.340148717550855e-83  \n",
      "\n",
      "--> climate change =\n",
      "ðŸ¤’   6.634989776190943e-33  \n",
      "ðŸš—   5.1888566027721555e-96  \n",
      "ðŸ—³   2.294988066281684e-52  \n",
      "ðŸ‘‡   9.257689864392038e-303  \n",
      "ðŸŽ“   2.524296308827204e-144  \n",
      "\n",
      "--> vegan =\n",
      "ðŸ¤¦   3.599614923792856e-232  \n",
      "ðŸ˜¢   4.0794779157928546e-75  \n",
      "ðŸ˜¡   1.127158024820009e-168  \n",
      "ðŸ˜”   1.0662844337858481e-198  \n",
      "ðŸ˜‹   6.122445647588763e-110  \n",
      "\n",
      "--> earth =\n",
      "ðŸ¤Ÿ   2.028946692897737e-36  \n",
      "ðŸ‹   1.0  \n",
      "ðŸŸ   1.3189489143397244e-51  \n",
      "ðŸŒ   7.057320726225616e-287  \n",
      "\n",
      "--> greta =\n",
      "ðŸ§¹   3.2315936025548407e-40  \n",
      "ðŸ§¡   5.134160423670935e-69  \n",
      "ðŸ§   3.7119745947176606e-35  \n",
      "ðŸ¦“   0.11111088111755894  \n",
      "ðŸ¦‹   9.340148717550855e-83  \n",
      "\n",
      "--> korea =\n",
      "ðŸ•º   8.480675382235633e-109  \n",
      "ðŸ’›   1.8814951513477593e-209  \n",
      "ðŸ’”   1.0805641867324546e-120  \n",
      "ðŸ‘‘   1.0  \n",
      "ðŸŽ‰   4.2086070926140675e-306  \n",
      "\n",
      "--> abortion =\n",
      "ðŸ¤¬   2.6287289042468105e-21  \n",
      "ðŸ¤   4.341096738792882e-10  \n",
      "ðŸ¤’   8.491163585538909e-29  \n",
      "ðŸ˜³   1.473566594080649e-250  \n",
      "ðŸ˜£   2.3566451314108866e-36  \n",
      "\n",
      "--> love you =\n",
      "ðŸ§¡   3.926179898523362e-37  \n",
      "ðŸ¦‹   1.2523513123842175e-77  \n",
      "ðŸ¤·   1.6984523642347511e-298  \n",
      "ðŸ¤œ   1.7606501363575637e-32  \n",
      "ðŸ¤›   1.0402848252882199e-29  \n",
      "\n",
      "--> birthday =\n",
      "ðŸ§¡   3.534752075779215e-24  \n",
      "ðŸ¤·   2.0368564797358588e-266  \n",
      "ðŸ¤¦   3.820451157903502e-286  \n",
      "ðŸ˜³   2.61600675461105e-258  \n",
      "ðŸŽ¶   3.5383772526604523e-295  \n",
      "\n",
      "--> i want a divorce =\n",
      "ðŸ¥´   2.2474385840075704e-85  \n",
      "ðŸ¤¡   1.3720875791743404e-166  \n",
      "ðŸ˜»   1.839898928686422e-57  \n",
      "ðŸ˜®   1.0432037668532157e-213  \n",
      "ðŸ˜¢   2.7692598826147156e-136  \n",
      "\n",
      "--> life =\n",
      "ðŸ¥´   9.14389091022032e-25  \n",
      "ðŸ¤ª   3.677989735300684e-103  \n",
      "ðŸ¤£   5e-324  \n",
      "ðŸ˜¡   1.7322762635846725e-149  \n",
      "ðŸ˜   2.9402719535910826e-99  \n",
      "\n",
      "--> baby =\n",
      "ðŸ¦   1.0  \n",
      "ðŸ’”   1.515849186285941e-220  \n",
      "ðŸ‘¶   3.938588461129528e-68  \n",
      "ðŸ°   3.4629578516397155e-65  \n",
      "ðŸŒ¸   7.440683998421424e-138  \n",
      "\n",
      "--> basketball =\n",
      "ðŸ§¹   3.2315936025548407e-40  \n",
      "ðŸ§¡   5.134160423670935e-69  \n",
      "ðŸ§   3.7119745947176606e-35  \n",
      "ðŸ¦“   0.11111088111755894  \n",
      "ðŸ¦‹   9.340148717550855e-83  \n",
      "\n",
      "--> i need coffee =\n",
      "ðŸ˜˜   1.0  \n",
      "\n",
      "--> la la land =\n",
      "ðŸŸ   3.0541529188756623e-32  \n",
      "ðŸŽ¶   6.469453313756039e-243  \n",
      "â¬‡   1.0  \n",
      "\n",
      "--> netflix and chill =\n",
      "ðŸ¥°   1.2126687423441382e-129  \n",
      "ðŸ˜µ   1.1887172648636834e-80  \n",
      "ðŸ“€   2.514549911531027e-34  \n",
      "ðŸ’»   4.201244206727935e-149  \n",
      "ðŸ…   1.0  \n",
      "\n",
      "--> i am thankful to be alive =\n",
      "ðŸ¥°   1.0  \n",
      "\n",
      "--> ocean =\n",
      "ðŸ¢   1.0  \n",
      "ðŸ™   1.254335799832252e-65  \n",
      "ðŸ‹   6.332816078630981e-45  \n",
      "\n",
      "--> trump =\n",
      "ðŸ¤¦   3.72109421816215e-245  \n",
      "ðŸš‚   8.1888327114749e-58  \n",
      "ðŸ˜¡   7.947575429311131e-161  \n",
      "ðŸ˜‰   1.3109909261213645e-232  \n",
      "ðŸ˜‡   2.4362578995895307e-251  \n",
      "\n",
      "--> plastic =\n",
      "ðŸ§¹   3.2315936025548407e-40  \n",
      "ðŸ§¡   5.134160423670935e-69  \n",
      "ðŸ§   3.7119745947176606e-35  \n",
      "ðŸ¦“   0.11111088111755894  \n",
      "ðŸ¦‹   9.340148717550855e-83  \n",
      "\n",
      "--> boyfriend =\n",
      "ðŸ§¹   3.2315936025548407e-40  \n",
      "ðŸ§¡   5.134160423670935e-69  \n",
      "ðŸ§   3.7119745947176606e-35  \n",
      "ðŸ¦“   0.11111088111755894  \n",
      "ðŸ¦‹   9.340148717550855e-83  \n",
      "\n",
      "\n",
      "----- Top 5 words for each Emoji in Train set\n",
      "------------------------------------------------------------\n",
      "1ï¸âƒ£  --> ['10' 'enter' 'winner' 'giveaway' '100']\n",
      "3ï¸âƒ£  --> ['year' 'year award' 'coach' 'award' 'leader']\n",
      "6ï¸âƒ£  --> ['year' 'year award' 'coach' 'award' 'leader']\n",
      "Â©  --> ['work' 'art' 'point' 'update' 'concert']\n",
      "Â®  --> ['chance win' 'post' 'chance' 'amazing' 'win']\n",
      "â„¢  --> ['gay' 'photo' 'lol' 'abortion' 'th']\n",
      "â©  --> ['currently' 'united states' 'states' 'united' 'elland road']\n",
      "âª  --> ['currently' 'united states' 'states' 'united' 'elland road']\n",
      "â°  --> ['expect' 'house' 'vote' 'time' 'change']\n",
      "â³  --> ['berlin' 'brexit' 'lose' 'climate change' 'climate']\n",
      "â–¶  --> ['perfect gift' 'order' 'gift' 'perfect' 'family member']\n",
      "â—€  --> ['sale' 'american' 'maga' 'buy' 'usa']\n",
      "â˜€  --> ['ya' 'beach' 'debut' 'summer' '19']\n",
      "â˜  --> ['nctzenselcaday' 'maybe' 'star' 'nctzenselcaday przwv8jdxg' 'fairy']\n",
      "â˜„  --> ['yea' 'baby' 'fan' 'set' 'life']\n",
      "â˜‘  --> ['gain' 'like' 'stock' 'lucky' 'twitter world']\n",
      "â˜”  --> ['rain' 'day' 'muafgoi4pq' 'day forecast' 'forecast']\n",
      "â˜•  --> ['morning' 'twitter' 'pilot' 'morning person' 'drive people']\n",
      "â˜  --> ['talk' 'champ' 'tag' 'nice' 'time']\n",
      "â˜   --> ['best' 'ðŒð€ð“ð‚ð‡ computer' 'engineer khatputli' 'elland road' 'elli']\n",
      "â˜¦  --> ['kag2020' 'washington' 'ðŒð€ð“ð‚ð‡ computer' 'energy drinks' 'energy']\n",
      "â˜®  --> ['run' 'resist fgkqdvmvfr' 'strongertogether resist' 'resist'\n",
      " 'list strongertogether']\n",
      "â˜¹  --> ['sad' 'home' 'time' 'tech' 'control']\n",
      "â˜º  --> ['yo' 'excited' 'love' 'like' 'cool']\n",
      "â™€  --> ['time' 'say' 'start' 'talk' 'trump']\n",
      "â™‚  --> ['smoke' 'ball' 'try' 'best' 'blonde']\n",
      "â™ˆ  --> ['mercury' 'ekcraiquxc' 'mar' 'venus mar' 'sun moon']\n",
      "â™‰  --> ['mercury' 'ekcraiquxc' 'mar' 'venus mar' 'sun moon']\n",
      "â™  --> ['mercury' 'ekcraiquxc' 'mar' 'venus mar' 'sun moon']\n",
      "â™Ž  --> ['mercury' 'ekcraiquxc' 'mar' 'venus mar' 'sun moon']\n",
      "â™  --> ['mercury' 'ekcraiquxc' 'mar' 'venus mar' 'sun moon']\n",
      "â™  --> ['mercury' 'ekcraiquxc' 'mar' 'venus mar' 'sun moon']\n",
      "â™‘  --> ['mercury' 'ekcraiquxc' 'mar' 'venus mar' 'sun moon']\n",
      "â™’  --> ['mercury' 'ekcraiquxc' 'mar' 'venus mar' 'sun moon']\n",
      "â™“  --> ['mercury' 'ekcraiquxc' 'mar' 'venus mar' 'sun moon']\n",
      "â™¥  --> ['love' 'thank' 'protect' 'heart' 'twice']\n",
      "â™¦  --> ['job' 'uk' 'really' 'usa' 'ðŒð€ð“ð‚ð‡ computer']\n",
      "â™»  --> ['fast reply' 'brexit' 'fast' 'climate change' 'climate']\n",
      "â™¾  --> ['somebody' 'fully' 'understand' 'need' 'world']\n",
      "âš“  --> ['add' '2nd' 'officially' 'second' 'friday']\n",
      "âš”  --> ['wwg1wga' 'thank' 'emergency' 'energy drinks' 'energy']\n",
      "âš–  --> ['inside' 'barr' 'brexit' 'year' 'climate change']\n",
      "âš   --> ['best' 'bts' 'vote' 'let' 'proceed caution']\n",
      "âš¡  --> ['acc' 'time' 'start' 'happen' 'excite']\n",
      "âšª  --> ['sky' 'relationship' 'single' 'beach' 'lose']\n",
      "âš«  --> ['85' 'ðŒð€ð“ð‚ð‡ computer' 'enjoy' 'elli' 'elli tom']\n",
      "âš½  --> ['arrive' 'stand' 'gt' 'fan' 'gt gt']\n",
      "âš¾  --> ['great' 'defeat' 'baseball' 'line' 'sure']\n",
      "â›ˆ  --> ['disaster cycl' 'hurricanes wildfire' 'extreme weather' 'extreme' 'flood']\n",
      "â›“  --> ['refuse' 'forget' 'ðŒð€ð“ð‚ð‡ computer' 'enter' 'engineer khatputli']\n",
      "â›”  --> ['join' 'north' 'stand' 'stop' 'let']\n",
      "â›°  --> ['book' 'car' 'buy' 'come' 'rbmitpin chance']\n",
      "â›³  --> ['play' 'course' 'best' 'think' 'make']\n",
      "â›·  --> ['motivate' 'dump' 'seriously' 'bear' 'exam']\n",
      "â›º  --> ['fast reply' 'fast' 'reply' 'elland road' 'elli']\n",
      "â›½  --> ['believe cow' 'medium like' 'cause climate' 'easytarget' 'people believe']\n",
      "âœ…  --> ['trump' 'suffer' 'climate change' 'climate' 'late']\n",
      "âœˆ  --> ['safe flight' 'flight' 'safe' 'send' 'home']\n",
      "âœŠ  --> ['air' 'child' 'hate' 'man' 'today']\n",
      "âœŒ  --> ['time' 'bts' 'real' 'goal' 'hoe']\n",
      "âœ  --> ['ðŒð€ð“ð‚ð‡ computer' 'elland' 'elli' 'elli tom' 'ellis']\n",
      "âœ”  --> ['president' 'favorite' 'kag2020' 'brand' 'online']\n",
      "âœ  --> ['prayer' 'great' 'amen' 'thank' 'service']\n",
      "âœ¡  --> ['kag2020' 'washington' 'ðŒð€ð“ð‚ð‡ computer' 'energy drinks' 'energy']\n",
      "âœ¨  --> ['right' 'need' 'shine' 'star' 'ready']\n",
      "â„  --> ['past' 'order' 'read' 'open' 'hi']\n",
      "âŒ  --> ['light' 'wall' 'emergency' 'border' 'fake']\n",
      "â“  --> ['reply' '50k reply' '1k 5k' 'country reply' 'reply flag']\n",
      "â”  --> ['fast reply' 'fast' 'reply' 'elland road' 'elli']\n",
      "â—  --> ['watch' 'new' 'let' 'prayer' 'beach']\n",
      "â£  --> ['want' 'tweet' 'women' 'power' 'leg']\n",
      "â¤  --> ['love' 'thank' 'happy' 'good' 'great']\n",
      "âž•  --> ['gain' 'like' 'gain 800' '800 follower' 'gain time']\n",
      "âž–  --> ['ðŒð€ð“ð‚ð‡ computer' 'elland' 'elli' 'elli tom' 'ellis']\n",
      "âž¡  --> ['leader' 'reveal' 'read' 'ya' 'sport']\n",
      "â¬…  --> ['border' '10' 'wall' 'population' 'euro']\n",
      "â¬‡  --> ['live' 'tale' 'late' 'afternoon' 'stadium']\n",
      "â¬›  --> ['create' '40' 'buy' 'today' 'day']\n",
      "â­  --> ['good' 'question' 'proud' 'era' 'rollercoaster']\n",
      "â­•  --> ['gain' 'like' 'twitter world' 'reply yes' 'hi twitter']\n",
      "ðŸ…°  --> ['era' 'single' 'season' '12' '10']\n",
      "ðŸ†š  --> ['stadium' 'el' 'su' 'park' 'america']\n",
      "ðŸ‡¦ðŸ‡¨  --> ['new york' 'york' 'toronto' 'la' 'washington']\n",
      "ðŸ‡¦ðŸ‡©  --> ['fast 100' '200 300' '500 600' '400 500' '100 200']\n",
      "ðŸ‡¦ðŸ‡ª  --> ['trade' 'sell' 'uk run' 'surplus sell' 'sell sell']\n",
      "ðŸ‡¦ðŸ‡«  --> ['100' '100 000' 'death' 'pakistan' 'age']\n",
      "ðŸ‡¦ðŸ‡±  --> ['grand final' 'eurovision grand' 'grand' 'eurovision' 'final']\n",
      "ðŸ‡¦ðŸ‡´  --> ['fast 100' '200 300' '500 600' '400 500' '100 200']\n",
      "ðŸ‡¦ðŸ‡¶  --> ['followers' 'followers followers' 'real fast' 'fast followers'\n",
      " 'followers follower']\n",
      "ðŸ‡¦ðŸ‡·  --> ['followers' 'country' 'follower' 'follower want' '50k 100k']\n",
      "ðŸ‡¦ðŸ‡¸  --> ['fast 100' '200 300' '500 600' '400 500' '100 200']\n",
      "ðŸ‡¦ðŸ‡¹  --> ['itst' 'scotland' 'face future' 'yes itst' 'future yes']\n",
      "ðŸ‡¦ðŸ‡º  --> ['followers' 'followers followers' 'follower' 'follower want'\n",
      " 'want followers']\n",
      "ðŸ‡¦ðŸ‡¼  --> ['followers' 'followers followers' 'real fast' 'fast followers'\n",
      " 'followers follower']\n",
      "ðŸ‡¦ðŸ‡¿  --> ['fast 100' '200 300' '500 600' '400 500' '100 200']\n",
      "ðŸ‡§ðŸ‡§  --> ['fast 100' '200 300' '500 600' '400 500' '100 200']\n",
      "ðŸ‡§ðŸ‡©  --> ['fast 100' '200 300' '500 600' '400 500' '100 200']\n",
      "ðŸ‡§ðŸ‡ª  --> ['attack' 'project' 'school' 'project project' 'result']\n",
      "ðŸ‡§ðŸ‡¬  --> ['su' 'grand final' 'eurovision grand' 'grand' 'young']\n",
      "ðŸ‡§ðŸ‡®  --> ['gaqhihqews' 'offer xlskobir3z' 'late bookmaker' 'bookmaker offer'\n",
      " 'bookmaker']\n",
      "ðŸ‡§ðŸ‡¯  --> ['tv episode' 'mjf' 'tonight feature' 'episode tonight' 'feature vs']\n",
      "ðŸ‡§ðŸ‡±  --> ['country' 'ðŒð€ð“ð‚ð‡ computer' 'enjoy' 'elli' 'elli tom']\n",
      "ðŸ‡§ðŸ‡·  --> ['followers' 'reply' 'country' 'want' 'like reply']\n",
      "ðŸ‡§ðŸ‡¸  --> ['fast 100' '200 300' '500 600' '400 500' '100 200']\n",
      "ðŸ‡§ðŸ‡¾  --> ['trade' 'sell' '500 600' '300 400' '400 500']\n",
      "ðŸ‡¨ðŸ‡¦  --> ['canada' 'country' 'followers' '2003' 'age']\n",
      "ðŸ‡¨ðŸ‡«  --> ['67' 'mli' '67 mli' 'sle' 'les 67']\n",
      "ðŸ‡¨ðŸ‡¬  --> ['country' 'ðŒð€ð“ð‚ð‡ computer' 'enjoy' 'elli' 'elli tom']\n",
      "ðŸ‡¨ðŸ‡­  --> ['switzerland' 'australia' 'usa' 'canada' '62']\n",
      "ðŸ‡¨ðŸ‡®  --> ['followers' 'followers followers' 'want followers' 'follower want'\n",
      " 'follower']\n",
      "ðŸ‡¨ðŸ‡±  --> ['want armymutual' 'armymutual' 'want' 'want gain' 'fast comment']\n",
      "ðŸ‡¨ðŸ‡³  --> ['china' '2019' 'june' 'germany' '2003 2007']\n",
      "ðŸ‡¨ðŸ‡´  --> ['cloud9' 'eunited' 'ghost' 'gt' 'vs']\n",
      "ðŸ‡¨ðŸ‡µ  --> ['grand final' 'eurovision grand' 'grand' 'eurovision' 'final']\n",
      "ðŸ‡¨ðŸ‡º  --> ['country' 'ðŒð€ð“ð‚ð‡ computer' 'enjoy' 'elli' 'elli tom']\n",
      "ðŸ‡¨ðŸ‡¾  --> ['eurovision' 'grand final' 'eurovision grand' 'grand' 'final']\n",
      "ðŸ‡¨ðŸ‡¿  --> ['korean' 'air' 'eurovision grand' 'grand final' 'grand']\n",
      "ðŸ‡©ðŸ‡ª  --> ['germany' 'klose' 'german' 'berlin' 'country']\n",
      "ðŸ‡©ðŸ‡°  --> ['country' 'reply' 'like reply' 'like' 'connect']\n",
      "ðŸ‡©ðŸ‡¿  --> ['country' 'handsome njkgdf6qiz' 'handsome' 'njkgdf6qiz' 'like reply']\n",
      "ðŸ‡ªðŸ‡¦  --> ['grand final' 'eurovision grand' 'grand' 'eurovision' 'final']\n",
      "ðŸ‡ªðŸ‡¨  --> ['age' 'forget' 'little' 'emotional' 'engineer khatputli']\n",
      "ðŸ‡ªðŸ‡ª  --> ['estonia' 'summer' 'council' 'happy' 'million']\n",
      "ðŸ‡ªðŸ‡¬  --> ['eurovision grand' 'grand final' 'grand' 'eurovision' 'final']\n",
      "ðŸ‡ªðŸ‡·  --> ['reply' 'world reply' 'country like' 'flag country' 'friend world']\n",
      "ðŸ‡ªðŸ‡¸  --> ['country' 'connect' 'comment' 'fast' '26']\n",
      "ðŸ‡ªðŸ‡º  --> ['brexit' 'climate change' 'climate' 'change' 'country']\n",
      "ðŸ‡«ðŸ‡®  --> ['navy' 'ship' 'stand' 'group' 'exercise']\n",
      "ðŸ‡«ðŸ‡¯  --> ['followers' 'followers followers' 'real fast' 'fast followers'\n",
      " 'followers follower']\n",
      "ðŸ‡«ðŸ‡·  --> ['instagram' 'safe flight' 'flight' 'want' '2019']\n",
      "ðŸ‡¬ðŸ‡§  --> ['followers' 'followers followers' 'follower' 'follower want'\n",
      " 'want followers']\n",
      "ðŸ‡¬ðŸ‡­  --> ['follower' 'let' 'follower want' 'let connect' 'connect']\n",
      "ðŸ‡¬ðŸ‡³  --> ['follower want' 'follower' 'want' '500k reply' '50k 100k']\n",
      "ðŸ‡¬ðŸ‡·  --> ['map' 'battle' 'early' 'continue' 'july']\n",
      "ðŸ‡¬ðŸ‡¸  --> ['travels 0r47ay58e5' '0r47ay58e5' 'let far' 'far tweet' 'tweet travels']\n",
      "ðŸ‡¬ðŸ‡¹  --> ['offer xlskobir3z' 'xlskobir3z gaqhihqews' 'bookmaker offer' 'bookmaker'\n",
      " 'late bookmaker']\n",
      "ðŸ‡¬ðŸ‡¾  --> ['country' 'ðŒð€ð“ð‚ð‡ computer' 'enjoy' 'elli' 'elli tom']\n",
      "ðŸ‡­ðŸ‡°  --> ['country' 'trade' 'sell' 'trade surplus' 'uk run']\n",
      "ðŸ‡­ðŸ‡²  --> ['53' '62' 'norway' 'gdp' 'singapore']\n",
      "ðŸ‡­ðŸ‡³  --> ['country' 'ðŒð€ð“ð‚ð‡ computer' 'enjoy' 'elli' 'elli tom']\n",
      "ðŸ‡­ðŸ‡·  --> ['country' 'face future' 'itst' 'yes itst' 'future yes']\n",
      "ðŸ‡­ðŸ‡¹  --> ['country' 'reply' 'like reply' 'connect' 'comment']\n",
      "ðŸ‡­ðŸ‡º  --> ['country' '15 different' 'member 15' 'service member' 'build inte']\n",
      "ðŸ‡®ðŸ‡©  --> ['00pm' 'bear' '50' '1995' 'armymutual']\n",
      "ðŸ‡®ðŸ‡ª  --> ['country' 'armymutual' 'want armymutual' 'want' '1kfollower 2kfollower']\n",
      "ðŸ‡®ðŸ‡±  --> ['country' 'fly' 'second' 'thank' 'travels country']\n",
      "ðŸ‡®ðŸ‡²  --> ['country' 'ðŒð€ð“ð‚ð‡ computer' 'enjoy' 'elli' 'elli tom']\n",
      "ðŸ‡®ðŸ‡³  --> ['followers' 'followers followers' 'want followers' 'india' 'follower']\n",
      "ðŸ‡®ðŸ‡´  --> ['country' 'ðŒð€ð“ð‚ð‡ computer' 'enjoy' 'elli' 'elli tom']\n",
      "ðŸ‡®ðŸ‡¶  --> ['country' 'ðŒð€ð“ð‚ð‡ computer' 'enjoy' 'elli' 'elli tom']\n",
      "ðŸ‡®ðŸ‡·  --> ['country' 'ðŒð€ð“ð‚ð‡ computer' 'enjoy' 'elli' 'elli tom']\n",
      "ðŸ‡®ðŸ‡¸  --> ['eurovision grand' 'grand final' 'grand' '2010 2012' '2008 2009']\n",
      "ðŸ‡®ðŸ‡¹  --> ['instagram' 'project' 'eurovision' 'tom' 'project project']\n",
      "ðŸ‡¯ðŸ‡²  --> ['followers' 'follower' 'jamaica' 'let' 'follower want']\n",
      "ðŸ‡¯ðŸ‡´  --> ['art sunset' 'perfect goodnight' 'sunset' 'goodnight art'\n",
      " 'travels country']\n",
      "ðŸ‡¯ðŸ‡µ  --> ['country' 'japan' 'want' 'let' 'reply']\n",
      "ðŸ‡°ðŸ‡ª  --> ['country' 'follower' 'let' 'follower want' 'want']\n",
      "ðŸ‡°ðŸ‡¬  --> ['country' 'ðŒð€ð“ð‚ð‡ computer' 'enjoy' 'elli' 'elli tom']\n",
      "ðŸ‡°ðŸ‡­  --> ['country' 'travels country' 'tweet travels' 'travels' 'let far']\n",
      "ðŸ‡°ðŸ‡³  --> ['country' 'perfect goodnight' 'sunset' 'goodnight art' 'art sunset']\n",
      "ðŸ‡°ðŸ‡µ  --> ['country' 'country reply' 'reply flag' 'flag' 'reply']\n",
      "ðŸ‡°ðŸ‡·  --> ['korea' 'south korea' 'south' 'country' 'seoul']\n",
      "ðŸ‡°ðŸ‡¼  --> ['country' 'ðŒð€ð“ð‚ð‡ computer' 'enjoy' 'elli' 'elli tom']\n",
      "ðŸ‡°ðŸ‡¾  --> ['country' 'ðŒð€ð“ð‚ð‡ computer' 'enjoy' 'elli' 'elli tom']\n",
      "ðŸ‡±ðŸ‡¦  --> ['country reply' 'reply flag' 'country' 'flag' 'reply']\n",
      "ðŸ‡±ðŸ‡®  --> ['perfect goodnight' 'sunset' 'goodnight art' 'art sunset'\n",
      " 'travels country']\n",
      "ðŸ‡±ðŸ‡°  --> ['perfect goodnight' 'sunset' 'goodnight art' 'art sunset'\n",
      " 'travels country']\n",
      "ðŸ‡±ðŸ‡·  --> ['country' '80' 'country reply' 'reply flag' 'flag']\n",
      "ðŸ‡±ðŸ‡¸  --> ['country reply' 'reply flag' 'country' 'flag' 'reply']\n",
      "ðŸ‡±ðŸ‡¹  --> ['country' 'perfect goodnight' 'sunset' 'art sunset' 'goodnight art']\n",
      "ðŸ‡±ðŸ‡º  --> ['country' 'country reply' 'reply flag' 'flag' 'reply']\n",
      "ðŸ‡±ðŸ‡»  --> ['country reply' 'reply flag' 'flag' 'reply' 'country']\n",
      "ðŸ‡²ðŸ‡¦  --> ['country' 'reply' 'connect' 'comment' 'like reply']\n",
      "ðŸ‡²ðŸ‡°  --> ['grand final' 'eurovision grand' 'grand' 'eurovision' 'final']\n",
      "ðŸ‡²ðŸ‡²  --> ['country reply' 'reply flag' 'flag' 'reply' 'country']\n",
      "ðŸ‡²ðŸ‡·  --> ['travels 0r47ay58e5' '0r47ay58e5' 'let far' 'far tweet' 'tweet travels']\n",
      "ðŸ‡²ðŸ‡¹  --> ['0r47ay58e5' 'travels 0r47ay58e5' 'yes itst' 'face future' 'scotland']\n",
      "ðŸ‡²ðŸ‡º  --> ['travels 0r47ay58e5' '0r47ay58e5' 'let far' 'far tweet' 'tweet travels']\n",
      "ðŸ‡²ðŸ‡»  --> ['travels 0r47ay58e5' '0r47ay58e5' 'let far' 'far tweet' 'tweet travels']\n",
      "ðŸ‡²ðŸ‡¼  --> ['travels 0r47ay58e5' '0r47ay58e5' 'let far' 'far tweet' 'tweet travels']\n",
      "ðŸ‡²ðŸ‡½  --> ['pm' 'author' 'yay' 'realize' 'music']\n",
      "ðŸ‡²ðŸ‡¾  --> ['country' 'country reply' 'reply flag' 'flag' 'reply']\n",
      "ðŸ‡²ðŸ‡¿  --> ['travels 0r47ay58e5' '0r47ay58e5' 'let far' 'far tweet' 'tweet travels']\n",
      "ðŸ‡³ðŸ‡¦  --> ['travels country' 'tweet travels' 'travels' 'let far' 'far tweet']\n",
      "ðŸ‡³ðŸ‡ª  --> ['reply' 'let' 'connect comment' 'let connect' 'connect']\n",
      "ðŸ‡³ðŸ‡¬  --> ['let' 'followers' 'nigeria' 'let connect' 'reply']\n",
      "ðŸ‡³ðŸ‡®  --> ['travels country' 'tweet travels' 'travels' 'let far' 'far tweet']\n",
      "ðŸ‡³ðŸ‡±  --> ['eurovision' '2010' '2009 2010' '2003 2005' '2005']\n",
      "ðŸ‡³ðŸ‡´  --> ['country reply' 'reply flag' 'country' 'flag' 'reply']\n",
      "ðŸ‡³ðŸ‡·  --> ['goodnight art' 'art sunset' 'sunset' 'perfect goodnight' 'goodnight']\n",
      "ðŸ‡³ðŸ‡¿  --> ['armymutual' 'want armymutual' '2000 2003' '2006 2008' '2008 2009']\n",
      "ðŸ‡µðŸ‡ª  --> ['country' 'ðŒð€ð“ð‚ð‡ computer' 'enjoy' 'elli' 'elli tom']\n",
      "ðŸ‡µðŸ‡­  --> ['philippines' 'thailand' 'thailand philippines' 'philippines united'\n",
      " 'countries']\n",
      "ðŸ‡µðŸ‡°  --> ['country' 'reply' 'like reply' 'followers' 'like']\n",
      "ðŸ‡µðŸ‡±  --> ['poland' 'group' 'nato' 'tom' 'like poland']\n",
      "ðŸ‡µðŸ‡·  --> ['country' 'country reply' 'reply flag' 'flag' 'reply']\n",
      "ðŸ‡µðŸ‡¹  --> ['country' 'portugal' 'armymutual' 'want armymutual' 'reply']\n",
      "ðŸ‡µðŸ‡¼  --> ['travels 0r47ay58e5' '0r47ay58e5' 'let far' 'far tweet' 'tweet travels']\n",
      "ðŸ‡µðŸ‡¾  --> ['country' 'ðŒð€ð“ð‚ð‡ computer' 'enjoy' 'elli' 'elli tom']\n",
      "ðŸ‡·ðŸ‡´  --> ['country reply' 'reply flag' 'flag' 'reply' 'country']\n",
      "ðŸ‡·ðŸ‡º  --> ['russia' 'india' 'tom' 'cost gb' 'gb']\n",
      "ðŸ‡·ðŸ‡¼  --> ['country reply' 'reply flag' 'flag' 'reply' 'country']\n",
      "ðŸ‡¸ðŸ‡¦  --> ['proud' 'man' 'pakistan' 'freedom' 'nigeria']\n",
      "ðŸ‡¸ðŸ‡§  --> ['travels 0r47ay58e5' '0r47ay58e5' 'let far' 'far tweet' 'tweet travels']\n",
      "ðŸ‡¸ðŸ‡ª  --> ['2003' 'eurovision' '2015' '1999 2003' '2011']\n",
      "ðŸ‡¸ðŸ‡¬  --> ['singapore' '62' '53' 'norway' 'gdp']\n",
      "ðŸ‡¸ðŸ‡®  --> ['armymutual' 'want armymutual' 'grand final' 'eurovision grand' 'grand']\n",
      "ðŸ‡¸ðŸ‡±  --> ['67' 'mli' '67 mli' 'sle' 'les 67']\n",
      "ðŸ‡¸ðŸ‡³  --> ['sina' 'heyoon sina' 'diarra' 'uniter atÃ©' 'joalin']\n",
      "ðŸ‡¸ðŸ‡´  --> ['67' 'mli' '67 mli' 'sle' 'les 67']\n",
      "ðŸ‡¸ðŸ‡·  --> ['grand final' 'eurovision grand' 'grand' 'eurovision' 'final']\n",
      "ðŸ‡¸ðŸ‡¸  --> ['travels 0r47ay58e5' '0r47ay58e5' 'let far' 'far tweet' 'tweet travels']\n",
      "ðŸ‡¸ðŸ‡»  --> ['want armymutual' 'armymutual' 'want' 'ðŒð€ð“ð‚ð‡ computer' 'energy drinks']\n",
      "ðŸ‡¸ðŸ‡¾  --> ['country' 'want armymutual' 'armymutual' 'want' 'ðŒð€ð“ð‚ð‡ computer']\n",
      "ðŸ‡¹ðŸ‡©  --> ['67' 'mli' '67 mli' 'sle' 'les 67']\n",
      "ðŸ‡¹ðŸ‡¬  --> ['country' 'reply' 'like reply' 'want armymutual' 'armymutual']\n",
      "ðŸ‡¹ðŸ‡­  --> ['stream' 'thailand' 'indonesia' 'country reply' 'reply flag']\n",
      "ðŸ‡¹ðŸ‡±  --> ['country reply' 'reply flag' 'flag' 'reply' 'country']\n",
      "ðŸ‡¹ðŸ‡³  --> ['grand final' 'eurovision grand' 'grand' 'eurovision' 'final']\n",
      "ðŸ‡¹ðŸ‡·  --> ['country' 'reply' 'like reply' 'fight' 'connect']\n",
      "ðŸ‡¹ðŸ‡¹  --> ['country' 'reply' 'like reply' 'like' 'connect']\n",
      "ðŸ‡¹ðŸ‡¼  --> ['country' 'open' 'taiwan' 'congratulation' 'china']\n",
      "ðŸ‡ºðŸ‡¦  --> ['u20wc' 'trophy' 'baku strasbourg' 'strasbourg' 'amman washington']\n",
      "ðŸ‡ºðŸ‡¬  --> ['let' '0r47ay58e5' 'travels 0r47ay58e5' 'far tweet' 'tweet travels']\n",
      "ðŸ‡ºðŸ‡²  --> ['followers' 'followers followers' 'follower want' 'follower' 'want']\n",
      "ðŸ‡ºðŸ‡³  --> ['council' 'estonia' 'million' 'join' 'country']\n",
      "ðŸ‡ºðŸ‡¸  --> ['thank' 'president' 'trump' 'usa' 'america']\n",
      "ðŸ‡ºðŸ‡¾  --> ['2000 2003' '2006 2008' '2008 2009' '2010 2012' '2012 2013']\n",
      "ðŸ‡»ðŸ‡ª  --> ['gt' 'seven' 'pop' 'year' 'test']\n",
      "ðŸ‡»ðŸ‡¬  --> ['fast 100' '200 300' '500 600' '400 500' '100 200']\n",
      "ðŸ‡»ðŸ‡³  --> ['country' '00pm' '10 country' 'country tweet' 'kst']\n",
      "ðŸ‡½ðŸ‡°  --> ['15 different' 'member 15' 'service member' 'build inte' 'work build']\n",
      "ðŸ‡¿ðŸ‡¦  --> ['country' 'followers' 'follower' 'follower want' 'want']\n",
      "ðŸ‡¿ðŸ‡¼  --> ['follower' '10 20' '20 30' '40 50' '50 60']\n",
      "ðŸŒ€  --> ['disaster cycl' 'hurricanes wildfire' 'extreme weather' 'extreme' 'flood']\n",
      "ðŸŒƒ  --> ['swimsuit' 'size' 'sale' 'city' 'available']\n",
      "ðŸŒ…  --> ['warm' 'reposte' 'weather' 'goodnight art' 'art sunset']\n",
      "ðŸŒˆ  --> ['gay' 'day homophobia' 'homophobia' 'international day' 'international']\n",
      "ðŸŒ‰  --> ['pynlw2bltb' 'hype hard' 'july pynlw2bltb' 'idc hype' 'hard 4th']\n",
      "ðŸŒŠ  --> ['wave' 'sea' 'today' '00' 'ocean']\n",
      "ðŸŒ  --> ['reply' 'fast reply' 'eu' 'gain' 'like']\n",
      "ðŸŒŽ  --> ['00pm' 'fucking' 'fresh' 'stream' 'fast reply']\n",
      "ðŸŒ  --> ['reply' 'fast reply' 'fast' '05' 'major']\n",
      "ðŸŒ“  --> ['small' 'moon' 'thing' 'emison' 'energy drinks']\n",
      "ðŸŒ™  --> ['100' 'clear' 'navy' 'sky' 'blue']\n",
      "ðŸŒš  --> ['mutual' 'going' 'idk' 'post' 'tho']\n",
      "ðŸŒ›  --> ['deal' 'seriously' 'tomorrow' 'goodnight' 'chance']\n",
      "ðŸŒ  --> ['footbal' 'join footbal' 'time rbmitpin' 'rbmitpin chance' 'rbmitpin']\n",
      "ðŸŒž  --> ['good morning' 'morning' 'good' 'sun' 'golden']\n",
      "ðŸŒŸ  --> ['wait' 'acc' 'good' 'thing' 'happy']\n",
      "ðŸŒ§  --> ['weird' '49' 'warm' 'high' 'really']\n",
      "ðŸŒ¨  --> ['guys' 'hand' 'area' 'base' 'currently']\n",
      "ðŸŒ«  --> ['forecast' 'day forecast' 'muafgoi4pq' 'forecast muafgoi4pq' 'day']\n",
      "ðŸŒ­  --> ['yeah' 'right' 'emergency' 'energy' 'end']\n",
      "ðŸŒ±  --> ['vegan' 'red' 'pretty' 'eat' 'pro']\n",
      "ðŸŒ²  --> ['ready' 'world' 'live chat' 'chat' 'continue']\n",
      "ðŸŒ³  --> ['garden' 'area' 'natural' 'love' 'ðŒð€ð“ð‚ð‡ computer']\n",
      "ðŸŒµ  --> ['thank' 'ðŒð€ð“ð‚ð‡ computer' 'emison eqqc7reh7m' 'engineer khatputli'\n",
      " 'engineer']\n",
      "ðŸŒ¶  --> ['bone' 'spicy' 'sweet' 'dog' 'sauce']\n",
      "ðŸŒ¸  --> ['result' 'post' 'turn' 'kiss' 'house']\n",
      "ðŸŒ¹  --> ['hello' 'germany' 'important' 'rose' 'pilot']\n",
      "ðŸŒº  --> ['cute' 'like' 'elli tom' 'ellis' 'ellis tom']\n",
      "ðŸŒ»  --> ['reply' 'world reply' 'country like' 'flag country' 'friend world']\n",
      "ðŸŒ¼  --> ['new' 'fresh' 'goal' 'start' 'time']\n",
      "ðŸŒ½  --> ['netflix vegi' 'tale like' 'vegi tale' 'vegi' 'doofus bitch']\n",
      "ðŸŒ¾  --> ['turkish' 'claim' 'force' 'field' 'set']\n",
      "ðŸŒ¿  --> ['blood' 'super' '25' 'moon' 'gt']\n",
      "ðŸ€  --> ['fan' 'right' 'calm' 'dear' 'wrong']\n",
      "ðŸ  --> ['beauty' 'beautiful' 'look' 'engineer khatputli' 'elli']\n",
      "ðŸ‚  --> ['hard' 'like' 'emison eqqc7reh7m' 'engineer' 'energy drinks']\n",
      "ðŸƒ  --> ['music release' 'we_are_superhuman' '127' 'nct 127' 'nct']\n",
      "ðŸ…  --> ['attack' 'tale like' 'vegi tale' 'vegi' 'doofus bitch']\n",
      "ðŸ†  --> ['look head' 'bitch look' 'tale like' 'vegi tale' 'vegi']\n",
      "ðŸ‡  --> ['force' 'know' 'want' 'emotional' 'engineer']\n",
      "ðŸŠ  --> ['fast reply' 'fast' 'splash' 'reply' 'sea']\n",
      "ðŸ  --> ['rd' '2113 bandera' 'tx iceicebabysa' 'tx' 'chamoy lover']\n",
      "ðŸŽ  --> ['force' 'know' 'apparently' 'apple' 'doctor']\n",
      "ðŸ  --> ['year' 'picture' 'end' 'wait' 'ðŒð€ð“ð‚ð‡ computer']\n",
      "ðŸ‘  --> ['time' 'spicy' 'bone' 'giveaway' 'post']\n",
      "ðŸ’  --> ['black' 'natural' 'make' 'lover 2113' '0crszsz16n']\n",
      "ðŸ“  --> ['available' 'yes' 'force' 'love' 'brazil']\n",
      "ðŸ¢  --> ['closed' 'way' 'ðŒð€ð“ð‚ð‡ computer' 'emison' 'energy drinks']\n",
      "ðŸª  --> ['teach' 'food' 'vegan' 'live' 'ðŒð€ð“ð‚ð‡ computer']\n",
      "ðŸ«  --> ['teach' 'food' 'vegan' 'make sure' 'live']\n",
      "ðŸ¯  --> ['honey' 'bear' 'natural' 'feel' 'emotional']\n",
      "ðŸ³  --> ['sauce wow' 'creamy sauce' 'long cookingwithchefk' 'kcstew creamy'\n",
      " 'kcstew']\n",
      "ðŸ·  --> ['national' 'wine' 'happy' 'favorite' 'day']\n",
      "ðŸ¸  --> ['seoul' 'ðŒð€ð“ð‚ð‡ computer' 'engineer khatputli' 'elland road' 'elli']\n",
      "ðŸ»  --> ['grab' 'try' 'work' 'year' 'wait']\n",
      "ðŸ½  --> ['busy' 'eat' 'medium' 'twitter' 'tonight']\n",
      "ðŸ¿  --> ['fb' 'star' 'summer' 'big' 'emotional']\n",
      "ðŸŽ€  --> ['mind little' 'ijul mind' 'ijul' 'mind' 'little']\n",
      "ðŸŽ  --> ['celebration' 'man' 'enjoy' 'year' 'shop']\n",
      "ðŸŽ‚  --> ['birthday' 'lead' 'friend' 'think' 'blessed']\n",
      "ðŸŽƒ  --> ['dress' 'yo' 'year' 'ðŒð€ð“ð‚ð‡ computer' 'emison eqqc7reh7m']\n",
      "ðŸŽ†  --> ['pynlw2bltb' 'hype hard' 'july pynlw2bltb' 'idc hype' 'hard 4th']\n",
      "ðŸŽ‡  --> ['talented' 'happy birthday' 'birthday' 'beautiful' 'happy']\n",
      "ðŸŽˆ  --> ['birthday' 'happy birthday' 'beautiful' 'happy' 'talented']\n",
      "ðŸŽ‰  --> ['birthday' 'happy' 'happy birthday' 'talented' 'tonight']\n",
      "ðŸŽŠ  --> ['offer' 'twice' 'news' 'surprise' 'year']\n",
      "ðŸŽ“  --> ['graduation' 'service' 'real' 'chase' 'senior bring']\n",
      "ðŸŽ—  --> ['lovely' 'set' 'free' 'est' 'eric bennett']\n",
      "ðŸŽ™  --> ['sauce wow' 'creamy sauce' 'long cookingwithchefk' 'kcstew creamy'\n",
      " 'kcstew']\n",
      "ðŸŽŸ  --> ['good' 'want' 'standard' 'leader' 'set']\n",
      "ðŸŽ¢  --> ['elland' 'elland road' 'rollercoaster' 'leeds' 'bielsa']\n",
      "ðŸŽ¤  --> ['channel' 'tonight' 'watch' 'shawn' 'key']\n",
      "ðŸŽ¥  --> ['omg' 'history' 'germany' 'champ' 'year old']\n",
      "ðŸŽ§  --> ['grab' 'chance win' 'chance' 'gc' 'team']\n",
      "ðŸŽ¨  --> ['important' 'rose' 'rise' 'make' 'time']\n",
      "ðŸŽ¬  --> ['fast reply' 'fast' 'reply' 'elland road' 'elli']\n",
      "ðŸŽ¯  --> ['website' 'bitcoin' 'hey like' 'reply hey' 'turn']\n",
      "ðŸŽµ  --> ['bear' '1995' 'kcstew creamy' 'cookingwithchefk kcstew'\n",
      " 'long cookingwithchefk']\n",
      "ðŸŽ¶  --> ['2019' 'check' 'mr' 'mike' 'stage']\n",
      "ðŸŽ¸  --> ['sauce wow' 'creamy sauce' 'long cookingwithchefk' 'kcstew creamy'\n",
      " 'kcstew']\n",
      "ðŸŽ¹  --> ['shawn' 'key' 'lord' 'blessing' 'play']\n",
      "ðŸŽ»  --> ['mr' 'beautiful' 'help' 'emison eqqc7reh7m' 'engineer']\n",
      "ðŸŽ¼  --> ['open' 'bear' '1995' 'creamy sauce' 'long cookingwithchefk']\n",
      "ðŸŽ¾  --> ['set' 'engineer khatputli' 'elland' 'elland road' 'elli']\n",
      "ðŸ€  --> ['59' 'championship' 'mid' '73' 'global']\n",
      "ðŸ  --> ['straight' 'good morning' 'country reply' 'reply flag' 'et']\n",
      "ðŸƒ  --> ['catch' 'funny' 'sorry' 'ass' 'person']\n",
      "ðŸ†  --> ['win' 'trophy' 'run' 'bts' 'music']\n",
      "ðŸ‡  --> ['park' 'di' 'le' 'et' '2019']\n",
      "ðŸˆ  --> ['football' 'officially' 'away' '100' 'start']\n",
      "ðŸŠ  --> ['teach' 'school' 'child' 'little' 'happy']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸŒ  --> ['learn' 'mind' 'open' 'thing' 'emison eqqc7reh7m']\n",
      "ðŸŽ  --> ['attention' 'woah' 'use' 'real' 'race day']\n",
      "ðŸ  --> ['national' 'indvaus' 'woman' 'team' 'unbelievable']\n",
      "ðŸ  --> ['intensity' 'talk intensity' 'bepartofthegame' 'intensity portugal'\n",
      " 'ko2zm2ahyq bepartofthegame']\n",
      "ðŸ‘  --> ['ðŒð€ð“ð‚ð‡ computer' 'elland' 'elli' 'elli tom' 'ellis']\n",
      "ðŸ’  --> ['sweep' 'finals' 'stanleycup' 'head' 'let']\n",
      "ðŸ  --> ['disaster cycl' 'hurricanes wildfire' 'extreme weather' 'extreme' 'flood']\n",
      "ðŸž  --> ['ready' 'world' 'ðŒð€ð“ð‚ð‡ computer' 'emison eqqc7reh7m' 'engineer']\n",
      "ðŸŸ  --> ['26' 'la' 'memory' 'rose' 'throw']\n",
      "ðŸ¥  --> ['al' 'fight' 'seven' 'continue' 'announce']\n",
      "ðŸ«  --> ['later' 'month' 'sorry' 'class' 'late']\n",
      "ðŸ³  --> ['flag' 'amazing' 'country reply' 'reply flag' 'right']\n",
      "ðŸ´  --> ['country' 'best' 'country reply' 'reply flag' 'el']\n",
      "ðŸ¹  --> ['team usa' 'champion' 'team' 'usa' 'world']\n",
      "ðŸ‡  --> ['bed' 'late' 'come' 'day' 'want']\n",
      "ðŸ‹  --> ['ocean' 'earth' 'today' 'leviathan' 'track monster']\n",
      "ðŸŽ  --> ['black' '63' 'july' 'party' 'summer']\n",
      "ðŸ  --> ['beat' 'make' 'good' 'ðŒð€ð“ð‚ð‡ computer' 'emison eqqc7reh7m']\n",
      "ðŸ‘  --> ['german' 'dog' 'leeds' 'bielsa' 'derby']\n",
      "ðŸ’  --> ['watch' 'balance' 'design' 'honor' 'new']\n",
      "ðŸ”  --> ['americans' 'proud' 'kick' 'great' 'emotional']\n",
      "ðŸ˜  --> ['suggest' 'piece' 'justice' 'finally' '000']\n",
      "ðŸ™  --> ['ocean' 'suffer' 'pain' 'leave' 'feel']\n",
      "ðŸœ  --> ['formiga appear' 'world record' 'appear 1995' 'record formiga' 'appear']\n",
      "ðŸ  --> ['pic' 'safe' 'amazing' 'time' 'engineer khatputli']\n",
      "ðŸŸ  --> ['ocean' 'pain' 'suffer' 'leave' 'feel']\n",
      "ðŸ¢  --> ['count' 'spring' 'year' 'ocean' 'suffer']\n",
      "ðŸ¦  --> ['air nowplaying' 'darkfather' 'live air' 'darkfather live' 'nowplaying']\n",
      "ðŸ­  --> ['car' 'guy' 'emergency' 'energy drinks' 'energy']\n",
      "ðŸ¯  --> ['followers' 'like' 'promotion' 'busy' 'tomorrow']\n",
      "ðŸ°  --> ['wth' 'bunny' 'tear' 'agree' 'vision']\n",
      "ðŸ±  --> ['cat' 'pet' 'guess' 'brexit' 'tv']\n",
      "ðŸ²  --> ['scene' 'favourite' 'late' 'emergency' 'energy drinks']\n",
      "ðŸ´  --> ['raise' 'bar' 'love' 'emotional' 'engineer']\n",
      "ðŸ¶  --> ['dog' 'love' 'pet' 'guess' 'walk']\n",
      "ðŸ¸  --> ['president' 'ðŒð€ð“ð‚ð‡ computer' 'emison eqqc7reh7m' 'engineer'\n",
      " 'energy drinks']\n",
      "ðŸ¹  --> ['cheer' 'cheer kit' 'bts help' 'usa bts' 'jin']\n",
      "ðŸº  --> ['followers' 'weird' 'follower' 'bc' '50k 100k']\n",
      "ðŸ»  --> ['warm' 'reposte' 'weather' 'honey' 'bear']\n",
      "ðŸ¾  --> ['good' 'thing' 'walk' 'good morning' 'morning']\n",
      "ðŸ¿  --> ['movie' 'ah' 'favorite' 'make' 'ðŒð€ð“ð‚ð‡ computer']\n",
      "ðŸ‘€  --> ['people' 'like' 'come' 'project' 'men']\n",
      "ðŸ‘  --> ['question' 'ðŒð€ð“ð‚ð‡ computer' 'engineer khatputli' 'elland road' 'elli']\n",
      "ðŸ‘…  --> ['know' 'daddy' 'want' 'young' 'like']\n",
      "ðŸ‘†  --> ['look' 'mean' 'place' '2018' 'look like']\n",
      "ðŸ‘‡  --> ['year' 'help' 'climate' 'friend' 'new']\n",
      "ðŸ‘ˆ  --> ['gain' 'apparently' 'like' 'apple' 'doctor']\n",
      "ðŸ‘‰  --> ['news' 'need' 'need help' 'play' 'apple']\n",
      "ðŸ‘Š  --> ['watch' 'good' 'use hashtag' 'hashtag' 'kag_camp']\n",
      "ðŸ‘‹  --> ['read' 'sunday' 'decision' 'woman right' 'away']\n",
      "ðŸ‘Œ  --> ['love' 'new' 'excite' 'night' 'thread']\n",
      "ðŸ‘  --> ['good' 'trump' 'nice' 'great' 'enjoy']\n",
      "ðŸ‘  --> ['great' 'woman' 'come' 'right' 'maybe']\n",
      "ðŸ‘  --> ['hard' 'know' 'mind little' 'ijul mind' 'ijul']\n",
      "ðŸ‘‘  --> ['use hashtag' 'hashtag' 'use' 'el' 'su']\n",
      "ðŸ‘™  --> ['guys' 'swimsuit' 'hand' 'patriotic' 'need']\n",
      "ðŸ‘§  --> ['op' 'line' 'read' 'later' 'suppose']\n",
      "ðŸ‘¨  --> ['age' 'abortion' 'letter' 'hell' 'ass']\n",
      "ðŸ‘©  --> ['graduation' 'sexual' 'explain' 'pre' 'exam']\n",
      "ðŸ‘¬  --> ['transphobia' 'away' 'bad' 'story' 'news']\n",
      "ðŸ‘­  --> ['transphobia' 'away' 'bad' 'story' 'news']\n",
      "ðŸ‘®  --> ['beautiful' 'ell' 'elland road' 'elli' 'elli tom']\n",
      "ðŸ‘¯  --> ['winner' 'guy' 'watch' 'ellis tom' 'end']\n",
      "ðŸ‘¶  --> ['baby' 'solo' 'album' 'excited' 'wth']\n",
      "ðŸ‘·  --> ['maga' 'student' 'partne' 'big' 'help']\n",
      "ðŸ‘º  --> ['peep oam4rsks0d' 'peep' 'oam4rsks0d' 'ðŒð€ð“ð‚ð‡ computer'\n",
      " 'emison eqqc7reh7m']\n",
      "ðŸ‘»  --> ['peep oam4rsks0d' 'peep' 'oam4rsks0d' 'ðŒð€ð“ð‚ð‡ computer'\n",
      " 'emison eqqc7reh7m']\n",
      "ðŸ‘¼  --> ['meadow fairy' 'princess fall' 'love meadow' 'fairytale'\n",
      " 'nctzenselcaday przwv8jdxg']\n",
      "ðŸ‘½  --> ['inside' 'seriously' 'save' 'james' 'ðŒð€ð“ð‚ð‡ computer']\n",
      "ðŸ’€  --> ['mutual' 'sis' 'ahead' 'tired' 'going']\n",
      "ðŸ’  --> ['friend' 'slam' 'point' 'lose' 'speak']\n",
      "ðŸ’ƒ  --> ['love' 'minute' 'power' 'friday' 'home']\n",
      "ðŸ’…  --> ['need closer' 'caution dishonest' 'proceed' 'proceed caution' 'love hard']\n",
      "ðŸ’†  --> ['selfie' 'post' 'check' 'fully' 'lie']\n",
      "ðŸ’ˆ  --> ['wait' 'ðŒð€ð“ð‚ð‡ computer' 'emotional' 'engineer khatputli' 'engineer']\n",
      "ðŸ’‹  --> ['proceed caution' 'dishonest' 'proceed' 'caution dishonest' 'caution']\n",
      "ðŸ’Œ  --> ['letter' 'chat' 'peep oam4rsks0d' 'oam4rsks0d' 'peep']\n",
      "ðŸ’  --> ['spend' 'turn' 'wanna' 'lot' 'right']\n",
      "ðŸ’Ž  --> ['kai' 'thing' 'emison eqqc7reh7m' 'engineer khatputli' 'engineer']\n",
      "ðŸ’  --> ['bloom' 'realize' 'mind little' 'ijul mind' 'ijul']\n",
      "ðŸ’“  --> ['wow' 'beautiful' 'engineer khatputli' 'elland road' 'elli']\n",
      "ðŸ’”  --> ['hurt' 'heart' 'shoulder' 'boy' 'break']\n",
      "ðŸ’•  --> ['need' 'day' 'cute' 'time' 'chart']\n",
      "ðŸ’–  --> ['wait' 'cute' 'story' 'work' 'love']\n",
      "ðŸ’—  --> ['beautiful' 'honey' 'link' 'forever' 'love']\n",
      "ðŸ’˜  --> ['real' 'place' 'thank' 'way' 'di']\n",
      "ðŸ’™  --> ['let' 'happy' 'thank' 'look' 'stanleycup']\n",
      "ðŸ’š  --> ['love' 'world' 'send' 'hair' 'tell']\n",
      "ðŸ’›  --> ['congratulation' 'photo' 'play' 'eye' 'light']\n",
      "ðŸ’œ  --> ['btsongma' 'army' 'btsonlssc' 'morning' 'love']\n",
      "ðŸ’ž  --> ['hope' 'thank' 'good night' 'like' 'lil']\n",
      "ðŸ’£  --> ['attention' 'game' 'ðŒð€ð“ð‚ð‡ computer' 'emotional' 'engineer']\n",
      "ðŸ’¥  --> ['day' 'wwg1wga' 'dc' 'enjoy' 'new']\n",
      "ðŸ’¦  --> ['hate' 'tag friend' 'tag' 'corner' 'room']\n",
      "ðŸ’¨  --> ['ahead' 'followers' 'style' 'june' 'finish']\n",
      "ðŸ’©  --> ['stop' 'turn' 'kid' 'bring' 'world']\n",
      "ðŸ’ª  --> ['thank' 'great' 'success' 'excite' 'league']\n",
      "ðŸ’«  --> ['level' 'inside' 'james' 'today' 'soul']\n",
      "ðŸ’¬  --> ['challenge' 'face' 'swimsuit' 'patriotic' 'reply']\n",
      "ðŸ’¯  --> ['yes' 'need' 'agree' 'amen' 'approve']\n",
      "ðŸ’°  --> ['50' 'refuse' 'f5' 'di' 'forget']\n",
      "ðŸ’²  --> ['tech' 'million' 'award' 'hi' 'win']\n",
      "ðŸ’¸  --> ['75' 'toronto' '25' 'meet' 'win 200']\n",
      "ðŸ’»  --> ['proud' '40' 'team' 'final' 'live']\n",
      "ðŸ’¼  --> ['women' 'german' 'meet' 'woman' 'ðŒð€ð“ð‚ð‡ computer']\n",
      "ðŸ’¿  --> ['got7' 'brand' 'sale' 'ep' 'best']\n",
      "ðŸ“€  --> ['glass' 'netflix vegi' 'tale like' 'doofus bitch' 'doofus']\n",
      "ðŸ“„  --> ['tour' 'actually' 'american' 'work' 'today']\n",
      "ðŸ“†  --> ['corner' 'football' 'ready' 'right' '2019']\n",
      "ðŸ“ˆ  --> ['stock' 'lucky' 'shoot' 'spring' 'feel']\n",
      "ðŸ“Š  --> ['40' 'final' 'day' 'emotional' 'engineer']\n",
      "ðŸ“‹  --> ['exo' 'premiosmtvmiaw' 'ðŒð€ð“ð‚ð‡ computer' 'emison eqqc7reh7m' 'engineer']\n",
      "ðŸ“  --> ['south korea' 'south' 'korea' 'emison' 'energy drinks']\n",
      "ðŸ“–  --> ['copy' 'book' 'available' 'sell' 'shawn']\n",
      "ðŸ“š  --> ['2019' 'korean' 'author' 'gc' 'seoul']\n",
      "ðŸ“œ  --> ['hall' 'ðŒð€ð“ð‚ð‡ computer' 'engineer khatputli' 'elland road' 'elli']\n",
      "ðŸ“  --> ['tag friend' 'sign' 'giveaway' 'tag' '20']\n",
      "ðŸ“¡  --> ['air nowplaying' 'live air' 'darkfather' 'darkfather live' 'nowplaying']\n",
      "ðŸ“£  --> ['excite' 'announce' 'special' '2020' 'live']\n",
      "ðŸ“¦  --> ['honeycomb' 'matrix honeycomb' 'matrix' 'yiasxaanw4' 'yiasxaanw4 free']\n",
      "ðŸ“¬  --> ['pregnant' 'male' 'politician' 'post' 'ðŒð€ð“ð‚ð‡ computer']\n",
      "ðŸ“±  --> ['afternoon' 'stadium' 'match' 'action' 'head']\n",
      "ðŸ“²  --> ['minute' 'power' 'friday' 'home' 'ready']\n",
      "ðŸ“·  --> ['photo' 'new' 'ðŒð€ð“ð‚ð‡ computer' 'emison' 'energy drinks']\n",
      "ðŸ“¸  --> ['fast reply' 'photo' 'nice' 'fast' 'girl']\n",
      "ðŸ“º  --> ['park' 'di' 'le' 'et' '2019']\n",
      "ðŸ“¼  --> ['u20wc' 'semi' 'reach' 'stage' 'energy']\n",
      "ðŸ“½  --> ['info' 'social' 'support' 'free' 'leave']\n",
      "ðŸ”  --> ['try' 'way' 'start' 'tag friend' 'sign']\n",
      "ðŸ”„  --> ['challenge' 'complete' 'tag' '500' 'reply']\n",
      "ðŸ”Š  --> ['captain' 'chris' 'middle' 'case' 'goal']\n",
      "ðŸ”Ž  --> ['july' 'summer' 'come' 'ðŒð€ð“ð‚ð‡ computer' 'end']\n",
      "ðŸ”‘  --> ['plan' 'climate' 'ðŒð€ð“ð‚ð‡ computer' 'emotional' 'engineer']\n",
      "ðŸ”’  --> ['promise' 'birth' 'care' 'ass' 'voting']\n",
      "ðŸ””  --> ['proud' 'like' 'scene' 'favourite' 'late']\n",
      "ðŸ”—  --> ['14' 'schedule' 'official' 'home' 'vs']\n",
      "ðŸ”˜  --> ['relationship' 'single' 'lose' 'emison eqqc7reh7m' 'energy drinks']\n",
      "ðŸ”™  --> ['ðŒð€ð“ð‚ð‡ computer' 'elland' 'elli' 'elli tom' 'ellis']\n",
      "ðŸ”œ  --> ['return' 'home' 'time' 'engineer khatputli' 'engineer']\n",
      "ðŸ”ž  --> ['honeycomb' 'matrix honeycomb' 'matrix' 'yiasxaanw4' 'yiasxaanw4 free']\n",
      "ðŸ”¥  --> ['special' 'new' 'today' 'say' 'love']\n",
      "ðŸ”ª  --> ['pretty' 'ðŒð€ð“ð‚ð‡ computer' 'engineer khatputli' 'elland road' 'elli']\n",
      "ðŸ”«  --> ['brain' 'kill' 'right' 'emergency' 'energy']\n",
      "ðŸ”¬  --> ['arrive' 'brain' 'sound' 'safe' 'beautiful']\n",
      "ðŸ”®  --> ['level' 'enjoy' 'elland road' 'elli' 'elli tom']\n",
      "ðŸ”´  --> ['sky' 'weekend' 'tune' 'champion' 'create']\n",
      "ðŸ”µ  --> ['sky' 'reveal' 'champion' 'saturday' 'watch']\n",
      "ðŸ”¶  --> ['journey' 'spread' 'join' 'news' 'fast']\n",
      "ðŸ”·  --> ['ðŒð€ð“ð‚ð‡ computer' 'ðŒð€ð“ð‚ð‡' 'news live' 'live Ø§Ù„Ù†' 'egnr0em5dc mobi']\n",
      "ðŸ”¸  --> ['1kfollower' '3kfollower' 'want 1kfollower' '2kfollower 3kfollower'\n",
      " '2kfollower']\n",
      "ðŸ”¹  --> ['1kfollower' '3kfollower' 'want 1kfollower' '2kfollower 3kfollower'\n",
      " '2kfollower']\n",
      "ðŸ”»  --> ['artist' 'international' '1st' 'bring' 'support']\n",
      "ðŸ•Š  --> ['blessing' 'double' 'kill' 'trump' 'happy']\n",
      "ðŸ•‹  --> ['ah' 'ah ah' 'bruvvvvv' 'waviest' 'london waviest']\n",
      "ðŸ•™  --> ['ðŒð€ð“ð‚ð‡ computer' 'elland' 'elli' 'elli tom' 'ellis']\n",
      "ðŸ•¶  --> ['person probably' 'gm hey' 'probably drive' 'morning person'\n",
      " 'people crazy']\n",
      "ðŸ•º  --> ['dance' 'deserve' 'literally' 'happilyeverafter' 'gaon']\n",
      "ðŸ–‹  --> ['hall' 'ðŒð€ð“ð‚ð‡ computer' 'engineer khatputli' 'elland road' 'elli']\n",
      "ðŸ–  --> ['5th' 'year old' 'eve' 'princess' 'old']\n",
      "ðŸ–•  --> ['respect' 'fan' 'information' 'suicide' 'create']\n",
      "ðŸ–¤  --> ['come' 'date' 'decision' 'maybe' 'trust']\n",
      "ðŸ–¼  --> ['available' 'high' 'great' 'ðŒð€ð“ð‚ð‡ computer' 'emison']\n",
      "ðŸ—‘  --> ['man' 'ðŒð€ð“ð‚ð‡ computer' 'emotional' 'engineer khatputli' 'engineer']\n",
      "ðŸ—“  --> ['vs' '00' 'cloud9' 'ghost' 'eunited']\n",
      "ðŸ—  --> ['inside' 'year' 'know' 'ðŒð€ð“ð‚ð‡ computer' 'engineer khatputli']\n",
      "ðŸ—£  --> ['talk' 'real' 'kid' 'air' 'best']\n",
      "ðŸ—³  --> ['voting' 'pls' 'want' 'tackle climate' 'thursday']\n",
      "ðŸ—½  --> ['york' 'new york' 'heart' 'high' 'look']\n",
      "ðŸ˜€  --> ['board' 'case' 'beautiful' 'usa' 'check']\n",
      "ðŸ˜  --> ['love' 'stream' 'na' 'class' 'excited']\n",
      "ðŸ˜‚  --> ['like' 'man' 'good' 'know' 'make']\n",
      "ðŸ˜ƒ  --> ['protect' 'right' 'band' 'wake' 'ep']\n",
      "ðŸ˜„  --> ['bloom' 'finally' 'mtvbrfandombtsarmy' 'voting' 'hahaha']\n",
      "ðŸ˜…  --> ['new' 'rest' 'like' 'thing' 'wrong']\n",
      "ðŸ˜†  --> ['reposte' 'oh wait' 'end' 'oh' 'tired']\n",
      "ðŸ˜‡  --> ['trump2020' 'promise' 'happy' 'trump' 'announce']\n",
      "ðŸ˜ˆ  --> ['use' 'ride' 'barr' 'vegan' 'actually']\n",
      "ðŸ˜‰  --> ['party' 'moon' 'shout' 'time' 'case']\n",
      "ðŸ˜Š  --> ['thank' 'say' 'game' 'info' 'ya']\n",
      "ðŸ˜‹  --> ['vegan' 'big' 'later' 'sauce' 'stuff']\n",
      "ðŸ˜Œ  --> ['mind' 'pop' 'easy' 'talk' 'thank']\n",
      "ðŸ˜  --> ['love' 'omg' 'look' 'cute' 'want']\n",
      "ðŸ˜Ž  --> ['class' 'time' 'boss' 'epic' 'complete']\n",
      "ðŸ˜  --> ['content' 'action' 'real' 'let' 'town']\n",
      "ðŸ˜‘  --> ['prayer' 'deserve' 'abortion' 'leave' 'germany']\n",
      "ðŸ˜’  --> ['man' 'attention' 'spend' 'lie' 'kid']\n",
      "ðŸ˜“  --> ['bad' 'child' 'head' 'ðŒð€ð“ð‚ð‡ computer' 'emotional']\n",
      "ðŸ˜”  --> ['hope' 'know' 'ask' 'hoe' 'chris']\n",
      "ðŸ˜•  --> ['hate' 'hour' 'weekend' 'house' '12']\n",
      "ðŸ˜˜  --> ['adnourhappyplace' 'love' 'great' 'time' 'thank']\n",
      "ðŸ˜™  --> ['place' 'way' 'ðŒð€ð“ð‚ð‡ computer' 'engineer' 'elland road']\n",
      "ðŸ˜›  --> ['yea' 'energy' 'big' 'hit' 'thread']\n",
      "ðŸ˜œ  --> ['masnada ride' 'ride eh' 'safe finish' 'know masnada' 'come safe']\n",
      "ðŸ˜  --> ['girl' 'come' 'beautiful' '2020' 'ok']\n",
      "ðŸ˜Ÿ  --> ['friend' 'sex' 'ok' 'check' 'member friend']\n",
      "ðŸ˜   --> ['trump' 'repeat' 'ðŒð€ð“ð‚ð‡ computer' 'emison' 'energy drinks']\n",
      "ðŸ˜¡  --> ['exactly' 'trump' 'make' 'poll' 'fun']\n",
      "ðŸ˜¢  --> ['vegan' 'hope' 'lose' 'ass' 'help']\n",
      "ðŸ˜£  --> ['abortion' 'tonight' 'beauty' 'friend' 'sky']\n",
      "ðŸ˜¤  --> ['really' 'shit' 'miss' 'rest' 'deserve']\n",
      "ðŸ˜¥  --> ['area' 'question' 'hell' 'different' 'bring']\n",
      "ðŸ˜¨  --> ['area' 'question' 'hell' 'different' 'bring']\n",
      "ðŸ˜©  --> ['wanna' 'feeling' 'straight' 'think' 'fuck']\n",
      "ðŸ˜ª  --> ['matter' 'word' 'shout' 'jungkook' 'protect']\n",
      "ðŸ˜«  --> ['sexual' 'energy' 'hurt' 'yea' 'oh']\n",
      "ðŸ˜¬  --> ['story' 'poll' 'fave' 'twitter' 'golden']\n",
      "ðŸ˜­  --> ['cute' 'look' 'love' 'like' 'man']\n",
      "ðŸ˜®  --> ['station' 'follower' 'kiss' '19' '20k']\n",
      "ðŸ˜¯  --> ['zcswvl8x9g' 'epic zcswvl8x9g' 'totally epic' 'okay totally' 'totally']\n",
      "ðŸ˜°  --> ['size' 'car' 'person' 'head' 'area']\n",
      "ðŸ˜±  --> ['season' 'need' 'important' 'real' 'kid']\n",
      "ðŸ˜²  --> ['high' 'police' 'baekhyun' 'surprise' 'dress']\n",
      "ðŸ˜³  --> ['use' 'wait' 'look' 'true' 'black']\n",
      "ðŸ˜´  --> ['till' 'lot' 'wait' 'saturday' 'wonderful']\n",
      "ðŸ˜µ  --> ['doofus bitch' 'tale like' 'netflix vegi' 'doofus' 'fuck watch']\n",
      "ðŸ˜·  --> ['ðŒð€ð“ð‚ð‡ computer' 'elland' 'elli' 'elli tom' 'ellis']\n",
      "ðŸ˜¸  --> ['welcome' 'enter' 'elli' 'elli tom' 'ellis']\n",
      "ðŸ˜¹  --> ['mutual' 'kai' 'drink' 'book' 'till']\n",
      "ðŸ˜»  --> ['marvelous' 'marvelous xoxoxo' 'xoxoxo' 'xoxoxo adore' 'adore']\n",
      "ðŸ˜¿  --> ['ðŒð€ð“ð‚ð‡ computer' 'elland' 'elli' 'elli tom' 'ellis']\n",
      "ðŸ™€  --> ['true' 'omg' 'ðŒð€ð“ð‚ð‡ computer' 'emison eqqc7reh7m' 'engineer']\n",
      "ðŸ™  --> ['car' 'nice' 'okay' 'house' 'gift buy']\n",
      "ðŸ™‚  --> ['stop' 'shot' 'day' 'coffee' 'lol']\n",
      "ðŸ™ƒ  --> ['past' 'hell' 'small' 'morning' 'attack']\n",
      "ðŸ™„  --> ['yeah' 'good' 'twice' 'know' 'watch']\n",
      "ðŸ™…  --> ['wish' 'stay' 'person' 'break' 'talk']\n",
      "ðŸ™†  --> ['black' 'use' 'capture' 'absolutely' 'reason']\n",
      "ðŸ™ˆ  --> ['cute' 'word' 'night' 'turn' 'wait']\n",
      "ðŸ™‰  --> ['dear' 'word' 'article' 'confused' 'ask']\n",
      "ðŸ™Š  --> ['good' 'like' 'right' 'wait' 'dear']\n",
      "ðŸ™‹  --> ['raise' 'funny' 'actually' 'hand' 'good morning']\n",
      "ðŸ™Œ  --> ['tonight' 'today' 'day' 'fall' 'scene']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ™  --> ['thank' 'soul' 'family' 'great' 'country']\n",
      "ðŸš€  --> ['inside' 'james' 'future' 'meet' 'make']\n",
      "ðŸš‚  --> ['board' 'usa' 'john' 'james' 'support']\n",
      "ðŸš‘  --> ['fb' 'ðŒð€ð“ð‚ð‡ computer' 'emison eqqc7reh7m' 'engineer khatputli' 'engineer']\n",
      "ðŸš’  --> ['age' 'oh wait' 'oh' 'wait' '15']\n",
      "ðŸš“  --> ['darrell' 'want darrell' 'want' 'ðŒð€ð“ð‚ð‡ computer' 'emison']\n",
      "ðŸš”  --> ['darrell' 'want darrell' 'want' 'ðŒð€ð“ð‚ð‡ computer' 'emison']\n",
      "ðŸš—  --> ['receive' 'al' 'drive' 'line' 'state']\n",
      "ðŸšœ  --> ['america' 'great' 'ðŒð€ð“ð‚ð‡ computer' 'emison eqqc7reh7m' 'energy drinks']\n",
      "ðŸš¨  --> ['help' 'great' 'tow' 'emergency' 'reach']\n",
      "ðŸš©  --> ['follower' '10 20' '20 30' '40 50' '50 60']\n",
      "ðŸš«  --> ['sex' 'support' 'thing' 'right' 'time']\n",
      "ðŸš®  --> ['guess' 'hand' 'news' 'good' 'emergency']\n",
      "ðŸš±  --> ['disaster cycl' 'hurricanes wildfire' 'extreme weather' 'extreme' 'flood']\n",
      "ðŸš²  --> ['senior' 'ðŒð€ð“ð‚ð‡ computer' 'engineer khatputli' 'elland road' 'elli']\n",
      "ðŸš¶  --> ['walk' 'man' 'ðŒð€ð“ð‚ð‡ computer' 'emotional' 'engineer']\n",
      "ðŸš¾  --> ['mf' 'album' 'ready' 'ðŒð€ð“ð‚ð‡ computer' 'emotional']\n",
      "ðŸ›  --> ['dog' 'pet' 'care' 'ðŒð€ð“ð‚ð‡ computer' 'emison eqqc7reh7m']\n",
      "ðŸ›Œ  --> ['bed' 'feel like' 'miss' 'news' 'big']\n",
      "ðŸ›‘  --> ['news' 'return' 'release' 'china' 'emison']\n",
      "ðŸ›’  --> ['pic' 'dream' 'engineer khatputli' 'elland road' 'elli']\n",
      "ðŸ›¢  --> ['believe cow' 'medium like' 'cause climate' 'easytarget' 'people believe']\n",
      "ðŸ›«  --> ['return' 'home' 'time' 'engineer khatputli' 'engineer']\n",
      "ðŸ›¬  --> ['ðŒð€ð“ð‚ð‡ computer' 'elland' 'elli' 'elli tom' 'ellis']\n",
      "ðŸ›°  --> ['man' 'today' 'emison eqqc7reh7m' 'engineer' 'energy drinks']\n",
      "ðŸ›³  --> ['crew' 'base' 'new' 'emison eqqc7reh7m' 'engineer']\n",
      "ðŸ›´  --> ['german' 'hit' 'house' 'emergency' 'energy']\n",
      "ðŸ›¸  --> ['best' 'ðŒð€ð“ð‚ð‡ computer' 'engineer khatputli' 'elland road' 'elli']\n",
      "ðŸ¤  --> ['special' '300k' 'appear' 'small' 'explain']\n",
      "ðŸ¤‘  --> ['dump' 'short' 'money' 'price' 'long']\n",
      "ðŸ¤’  --> ['bitch' 'reposte' 'nervous' 'reason' 'abortion']\n",
      "ðŸ¤“  --> ['final' 'day' 'human' 'peep' 'peep oam4rsks0d']\n",
      "ðŸ¤”  --> ['mike' 'tell' 'think' 'event' 'america']\n",
      "ðŸ¤–  --> ['answer' 'challenge' 'young' 'look' 'new']\n",
      "ðŸ¤—  --> ['new' 'sauce' 'sorry' 'surprise' 'ass']\n",
      "ðŸ¤˜  --> ['actually' 'live' 'hand' 'omg' 'beach']\n",
      "ðŸ¤™  --> ['game' 'ðŒð€ð“ð‚ð‡ computer' 'elland' 'elli' 'elli tom']\n",
      "ðŸ¤š  --> ['past' 'tho' 'comment hey' 'let connect' 'connect']\n",
      "ðŸ¤›  --> ['pop' 'episode' 'song' 'video' 'favourite']\n",
      "ðŸ¤œ  --> ['pop' 'episode' 'song' 'video' 'favourite']\n",
      "ðŸ¤  --> ['abortion' 'kai' 'men' 'trial' 'stage']\n",
      "ðŸ¤Ÿ  --> ['amazing moment' 'moment earth' 'eve ya1jjhuzsv' 'guy eve' 'earth thank']\n",
      "ðŸ¤   --> ['currently' 'germany' 'town' 'road' 'old']\n",
      "ðŸ¤¡  --> ['say' 'course' 'male' 'straight' 'ill']\n",
      "ðŸ¤¢  --> ['think' 'ðŒð€ð“ð‚ð‡ computer' 'emison eqqc7reh7m' 'engineer khatputli'\n",
      " 'engineer']\n",
      "ðŸ¤£  --> ['say' 'beat' 'easy' 'like' 'right']\n",
      "ðŸ¤¤  --> ['red' 'sis' 'try' 'make perfect' 'gift buy']\n",
      "ðŸ¤¥  --> ['hi' 'science' 'reply' 'peep oam4rsks0d' 'peep']\n",
      "ðŸ¤¦  --> ['vegan' 'smoke' 'start' 'cat' 'say']\n",
      "ðŸ¤§  --> ['really' 'anniversary' 'happy' 'today' 'day']\n",
      "ðŸ¤¨  --> ['work' 'wish vote' 'vote time' 'wish' 'vote']\n",
      "ðŸ¤©  --> ['start' 'excited' 'euro' 'brother' 'night']\n",
      "ðŸ¤ª  --> ['like' 'like tweet' 'tweet' 'good' 'cast']\n",
      "ðŸ¤«  --> ['green' 'think' 'period' 'going' 'exactly']\n",
      "ðŸ¤¬  --> ['science' 'respect' 'climate' 'tell' 'woman']\n",
      "ðŸ¤­  --> ['song' 'attack' 'tho' 'mysterious' 'react']\n",
      "ðŸ¤®  --> ['sick' 'police' 'dress' 'hit' 'emison eqqc7reh7m']\n",
      "ðŸ¤¯  --> ['shit' 'watch' 'say' 'like' 'game']\n",
      "ðŸ¤°  --> ['pregnant' 'expect' 'soon' 'baby' 'woman']\n",
      "ðŸ¤³  --> ['selfie' 'better' 'attention' 'cost' 'cat']\n",
      "ðŸ¤·  --> ['time' 'second' 'size' 'lucky' 'good']\n",
      "ðŸ¤¸  --> ['party' 'house' 'engineer khatputli' 'elland road' 'elli']\n",
      "ðŸ¤¼  --> ['tv episode' 'mjf' 'tonight feature' 'episode tonight' 'feature vs']\n",
      "ðŸ¥€  --> ['garden' 'rose' 'dream' 'set' 'ðŒð€ð“ð‚ð‡ computer']\n",
      "ðŸ¥‚  --> ['best' 'ðŒð€ð“ð‚ð‡ computer' 'engineer khatputli' 'elland road' 'elli']\n",
      "ðŸ¥ƒ  --> ['ready' 'champ' 'question' 'point' 'comment']\n",
      "ðŸ¥‡  --> ['pm et' 'gold' 'pm' 'catch' 'et']\n",
      "ðŸ¥ˆ  --> ['bennett' 'eric bennett' 'recurve' 'eric' 'open']\n",
      "ðŸ¥‰  --> ['medal' 'win' 'honor' 'event' 'world']\n",
      "ðŸ¥Š  --> ['website' 'bitcoin' 'new' 'emison' 'energy drinks']\n",
      "ðŸ¥’  --> ['rd' '2113 bandera' 'tx iceicebabysa' 'tx' 'chamoy lover']\n",
      "ðŸ¥“  --> ['true' 'time' 'ðŒð€ð“ð‚ð‡ computer' 'emison eqqc7reh7m' 'engineer']\n",
      "ðŸ¥š  --> ['second' 'season' 'win' 'time' 'emison']\n",
      "ðŸ¥¢  --> ['form' 'hear' 'know' 'ðŒð€ð“ð‚ð‡ computer' 'enter']\n",
      "ðŸ¥­  --> ['rd' '2113 bandera' 'tx iceicebabysa' 'tx' 'chamoy lover']\n",
      "ðŸ¥°  --> ['dress' 'success' 'end' 'hell' 'face']\n",
      "ðŸ¥³  --> ['today' 'vs' 'round' '14' '10k']\n",
      "ðŸ¥´  --> ['joke' 'pregnant' 'cool' 'ass' 'mother']\n",
      "ðŸ¥µ  --> ['hate' 'energy' 'big' 'thank' 'dump']\n",
      "ðŸ¥º  --> ['good' 'love' 'cute' 'think' 'bts']\n",
      "ðŸ¦  --> ['baby' 'trust' 'god' 'support' 'team']\n",
      "ðŸ¦„  --> ['card' 'open' 'esta noche' 'enjoy' 'elli']\n",
      "ðŸ¦…  --> ['barr' 'school' 'lovely' 'set' 'free']\n",
      "ðŸ¦†  --> ['10' 'followback' 'train' 'tag' 'friend']\n",
      "ðŸ¦‹  --> ['wwg1wga' 'thank' 'love meadow' 'fairy' 'nctzenselcaday przwv8jdxg']\n",
      "ðŸ¦‘  --> ['leviathan' 'ep rise' 'unleash track' 'unleash' 'monster']\n",
      "ðŸ¦“  --> ['ðŒð€ð“ð‚ð‡ computer' 'elland' 'elli' 'elli tom' 'ellis']\n",
      "ðŸ¦•  --> ['leviathan' 'ep rise' 'unleash track' 'unleash' 'monster']\n",
      "ðŸ¦·  --> ['education' 'fully' 'action' 'climate change' 'free']\n",
      "ðŸ§‚  --> ['office' 'ass' 'ðŒð€ð“ð‚ð‡ computer' 'emotional' 'engineer khatputli']\n",
      "ðŸ§  --> ['poll' 'science' 'climate' 'tell' 'kind']\n",
      "ðŸ§˜  --> ['fully' 'lie' 'dream' 'really' 'fuck']\n",
      "ðŸ§™  --> ['best' 'ðŒð€ð“ð‚ð‡ computer' 'engineer khatputli' 'elland road' 'elli']\n",
      "ðŸ§š  --> ['meadow fairy' 'princess fall' 'love meadow' 'fairytale'\n",
      " 'nctzenselcaday przwv8jdxg']\n",
      "ðŸ§ž  --> ['best' 'ðŒð€ð“ð‚ð‡ computer' 'engineer khatputli' 'elland road' 'elli']\n",
      "ðŸ§¡  --> ['child' 'action' 'teach' 'make sure' 'school']\n",
      "ðŸ§¨  --> ['pynlw2bltb' 'hype hard' 'july pynlw2bltb' 'idc hype' 'hard 4th']\n",
      "ðŸ§²  --> ['best' 'ðŒð€ð“ð‚ð‡ computer' 'engineer khatputli' 'elland road' 'elli']\n",
      "ðŸ§¹  --> ['count' 'spring' 'finals' 'sweep' 'stanleycup']\n",
      "ðŸ§º  --> ['brexit' 'tv' 'cat' 'channel' 'emotional']\n",
      "ðŸ§¿  --> ['ah' 'ah ah' 'bruvvvvv' 'waviest' 'london waviest']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "class Emoji(object):\n",
    "\n",
    "    def __init__(self):\n",
    "        # fit the Naive Bayes\n",
    "        np.random.seed(42)\n",
    "        self.emojis = pd.read_pickle('./data/df_emoji.pkl')\n",
    "\n",
    "    def fit(self):\n",
    "        '''\n",
    "        # ------- this part needs work\n",
    "        try:\n",
    "            self.labeled_tweets = pd.read_pickle('../data/labeled.pkl')\n",
    "            print('it worked')\n",
    "        except:\n",
    "            from label_tweets import label_tweets\n",
    "            tweets = np.array(list(pickle.load(open('./data/yay_moji.pkl','rb'))))\n",
    "            self.by_emoji,self.labeled_tweets = label_tweets(tweets,self.emojis,top = 50, save = True)\n",
    "        '''\n",
    "        self.y = tweets_merged['emoji']\n",
    "        self.X = tweets_merged['tweets'].values\n",
    "\n",
    "\n",
    "    def model(self, max_df_ = .8, min_df_ = .001, ngram = (1,2)):\n",
    "\n",
    "        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(self.X,self.y)\n",
    "\n",
    "        stopwords = set(list(ENGLISH_STOP_WORDS) + ['rt', 'follow', 'dm', 'https', 'ur', 'll' ,'amp', 'subscribe', 'don', 've', 'retweet', 'im', 'http','lt'])\n",
    "\n",
    "        # fit the tfidf or CountVectorizer\n",
    "        self.tfidf = TfidfVectorizer(max_features=10000, max_df = max_df_, min_df=min_df_, stop_words = stopwords, ngram_range = ngram)\n",
    "\n",
    "        self.tfidf.fit(self.X_train)\n",
    "        self.vector = self.tfidf.transform(self.X_train)\n",
    "\n",
    "        # --> add the emoji name to bag of words for each emoji\n",
    "        self.bag = np.array(self.tfidf.get_feature_names())\n",
    "\n",
    "        self.nb = GaussianNB()\n",
    "        self.nb.fit(self.vector.todense(), self.y_train)\n",
    "\n",
    "    def internal_predict(self, print_side_by_side = True):\n",
    "        test_tfidf = self.tfidf.transform(self.X_test)\n",
    "        predicted = self.nb.predict(test_tfidf.todense())\n",
    "        print('labeled')\n",
    "        acc = np.mean(self.y_test == predicted)\n",
    "\n",
    "        print('Test accuracy =',acc)\n",
    "        print('')\n",
    "\n",
    "        if print_side_by_side:\n",
    "            for true,predict in zip(self.y_test,predicted):\n",
    "                print('-->',true,predict)\n",
    "\n",
    "\n",
    "    def predict(self,text):\n",
    "        top_n = 3\n",
    "        test_tfidf = self.tfidf.transform([text])\n",
    "        probs = self.nb.predict_proba(test_tfidf.todense())\n",
    "        probs = probs.flatten()\n",
    "        above_0 = np.argwhere(probs>0).flatten()\n",
    "        above_0 = np.sort(above_0)[::-1]\n",
    "        print('-->',text,'=',)\n",
    "        \n",
    "        for i in above_0[:5]:\n",
    "            print(self.nb.classes_[i],' ', probs.flatten()[i],' ',)\n",
    "        print('')\n",
    "        return(probs)\n",
    "\n",
    "    def print_top_words(self,top_n_words=5):\n",
    "        # printing top words for each emoji\n",
    "        print('')\n",
    "        print('----- Top {} words for each Emoji in Train set'.format(top_n_words))\n",
    "        print('-'*60)\n",
    "        for i in range(len(self.nb.classes_)):\n",
    "            top =  self.bag[self.nb.theta_[i].argsort()[::-1]][:top_n_words]\n",
    "            print(self.nb.classes_[i],' -->',top)\n",
    "        print('')\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    # run clean_tweets\n",
    "    # run labeled_tweets\n",
    "\n",
    "    emo = Emoji()\n",
    "    emo.fit()\n",
    "    emo.model()\n",
    "    b = emo.predict('ocasio')\n",
    "    c = emo.predict('climate change')\n",
    "    d = emo.predict('vegan')\n",
    "    e = emo.predict('earth')\n",
    "    e = emo.predict('greta')\n",
    "    e = emo.predict('korea')\n",
    "    e = emo.predict('abortion')\n",
    "    e = emo.predict('love you')\n",
    "    e = emo.predict('birthday')\n",
    "    a = emo.predict('i want a divorce')\n",
    "    e = emo.predict('life')\n",
    "    e = emo.predict('baby')\n",
    "    e = emo.predict('basketball')\n",
    "    e = emo.predict('i need coffee')\n",
    "    e = emo.predict('la la land')\n",
    "    e = emo.predict('netflix and chill')\n",
    "    e = emo.predict('i am thankful to be alive')\n",
    "    e = emo.predict('ocean')\n",
    "    e = emo.predict('trump')\n",
    "    e = emo.predict('plastic')\n",
    "    e = emo.predict('boyfriend')\n",
    "\n",
    "    # emo.internal_predict(print_side_by_side = True)\n",
    "    emo.print_top_words(5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
