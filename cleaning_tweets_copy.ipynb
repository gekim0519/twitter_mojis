{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Emojis Speak More than Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GOAL: \n",
    "    1. give an \"issue word\" as an input (ex. ocasio, climate change) and find the most related emoji\n",
    "    to kinda grasp people's opinions\n",
    "    2. give any word or a saying and get a emoji that is most related ex. sparkle --> ✨"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make a columns for emojis and its corresponding tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/sara/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "import nltk.tokenize as tk\n",
    "import en_core_web_sm\n",
    "import string\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "nlp = en_core_web_sm.load()\n",
    "\n",
    "stopwords = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = list(pickle.load(open('./data/yay_moji.pkl','rb')))\n",
    "mojis = pd.read_pickle('./data/df_emoji.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'RT @Fancy2Nancy3: 🚨 ATTN  PATRIOTS 🚨 \\nPlease Retweet &amp; Follow        🇺🇸@Commonm69164249🇺🇸        \\n    🎉 Help Reach 🎉\\n🔥5K FOLLOWERS 🔥 \\n🇺🇸 A…'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords.extend(['\\'s','’s','rt','…','️','...','follow', 'dm', 'https', 'ur', 'll' ,'amp', 'subscribe', 'don', 've', 'retweet', 'im', 'http'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean text \n",
    "punctuations = string.punctuation\n",
    "\n",
    "# Define function to cleanup text by removing personal pronouns, stopwords, and puncuation\n",
    "def cleanup_text(docs, logging=False):\n",
    "    texts = []\n",
    "    counter = 1\n",
    "    for doc in docs:\n",
    "        if counter % 1000 == 0 and logging:\n",
    "            print(\"Processed %d out of %d documents.\" % (counter, len(docs)))\n",
    "        counter += 1\n",
    "        doc = nlp(doc, disable=['parser', 'ner'])\n",
    "        tokens = [tok.lemma_.lower().strip() for tok in doc if tok.lemma_ != '-PRON-']\n",
    "        tokens = [tok for tok in tokens if tok not in stopwords and tok not in punctuations]\n",
    "        tokens = ' '.join(tokens)\n",
    "        tokens = re.sub('@[^\\s]+','', tokens)\n",
    "        texts.append(tokens)\n",
    "    return pd.Series(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tweets = pd.DataFrame(tweets, columns=['tweet'])\n",
    "#tw = [word for word in tweets['tweet']]\n",
    "\n",
    "# Clean up all text\n",
    "tw_clean = cleanup_text(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "letters = '🇦 🇧 🇨 🇩 🇪 🇫 🇬 🇭 🇮 🇯 🇰 🇱 🇲 🇳 🇴 🇵 🇶 🇷 🇸 🇹 🇺 🇻 🇼 🇽 🇾 🇿'.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "flags = mojis['unichar'][1458:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change letters to flags\n",
    "def fix_flags(tweets):\n",
    "    fixed = []\n",
    "    for tweet in tweets:\n",
    "        for l in letters:\n",
    "            if l in tweet:\n",
    "                tweet = re.sub(l+\" \", l, tweet)\n",
    "        fixed.append(tweet)\n",
    "    return(fixed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tw_clean_flags = fix_flags(tw_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# put space after flag emojis\n",
    "def flags_space(tweets):\n",
    "    fixed = []\n",
    "    for tweet in tweets:\n",
    "        for l in flags:\n",
    "            if l in tweet:\n",
    "                tweet = re.sub(l, l+\" \", tweet)\n",
    "        fixed.append(tweet)\n",
    "    return(fixed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "tw_cleaned = flags_space(tw_clean_flags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# complete list of emojis\n",
    "from emoji import UNICODE_EMOJI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "emojis = list(UNICODE_EMOJI.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_mojis(tweets):\n",
    "    emoji = defaultdict(list)\n",
    "\n",
    "    for i, tweet in enumerate(tweets):\n",
    "        for word in tweet.split():\n",
    "            if word in emojis:\n",
    "                emoji['emoji'].append(word)\n",
    "                emoji['index'].append(i)\n",
    "    \n",
    "    # delete overlapping emojis in a tweet\n",
    "    emoji = pd.DataFrame(emoji).drop_duplicates()\n",
    "    \n",
    "    return(emoji)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted = extract_mojis(tw_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_emojis(tweets):\n",
    "    no_emojis = []\n",
    "    \n",
    "    for tweet in tweets:\n",
    "        tweet = tweet.split()\n",
    "        words = []\n",
    "        \n",
    "        for word in tweet:\n",
    "            if word not in list(extracted['emoji']):\n",
    "                words.append(word)\n",
    "        words = ' '.join(words)\n",
    "        no_emojis.append(words)\n",
    "    return(no_emojis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "tw_no_emo = remove_emojis(tw_cleaned)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are still some unremoved emojis but I will come back to that later. Maybe try tdidf..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df = pd.DataFrame(tw_no_emo, columns = ['tweets'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_merged = pd.merge(extracted, tweets_df.reset_index(), on='index', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "# emojis with at least 10 tweets\n",
    "enough_emoji = tweets_merged.groupby('emoji').count()[tweets_merged.groupby('emoji').count()['tweets']>=10]\n",
    "enough_emoji = pd.merge(enough_emoji.reset_index()[['emoji']], tweets_merged, on='emoji', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>emoji</th>\n",
       "      <th>index</th>\n",
       "      <th>tweets</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>😂</td>\n",
       "      <td>0</td>\n",
       "      <td>john daly actually ride cart major man live go...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>🚨</td>\n",
       "      <td>1</td>\n",
       "      <td>attn patriots please retweet amp follow help r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>🇺🇸</td>\n",
       "      <td>1</td>\n",
       "      <td>attn patriots please retweet amp follow help r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>🎉</td>\n",
       "      <td>1</td>\n",
       "      <td>attn patriots please retweet amp follow help r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>🔥</td>\n",
       "      <td>1</td>\n",
       "      <td>attn patriots please retweet amp follow help r...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  emoji  index                                             tweets\n",
       "0     😂      0  john daly actually ride cart major man live go...\n",
       "1     🚨      1  attn patriots please retweet amp follow help r...\n",
       "2    🇺🇸      1  attn patriots please retweet amp follow help r...\n",
       "3     🎉      1  attn patriots please retweet amp follow help r...\n",
       "4     🔥      1  attn patriots please retweet amp follow help r..."
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_merged.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['😂', '🚨', '🇺🇸', ..., '🏏', '😎', '⭐'], dtype=object)"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_merged['emoji'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------\n",
      "Group: 0\n",
      "256 tweets\n",
      "--> love\n",
      "--> yes\n",
      "--> gonna\n",
      "--> cute\n",
      "--> life\n",
      "--> people\n",
      "--> amazing\n",
      "--> guys\n",
      "--> friends\n",
      "--> page\n",
      "💛\n",
      "🇺🇸\n",
      "🙊\n",
      "♥\n",
      "😍\n",
      "🤯\n",
      "♥\n",
      "🇺🇸\n",
      "😁\n",
      "💯\n",
      "🐶\n",
      "😂\n",
      "🇧🇩\n",
      "🇩🇪\n",
      "🇬🇧\n",
      "😍\n",
      "☺\n",
      "🤗\n",
      "🙈\n",
      "🙉\n",
      "🙊\n",
      "💕\n",
      "❤\n",
      "💛\n",
      "💙\n",
      "💜\n",
      "💫\n",
      "🐶\n",
      "🦄\n",
      "🐻\n",
      "🐣\n",
      "🌸\n",
      "🌺\n",
      "🌞\n",
      "⭐\n",
      "🌈\n",
      "☃\n",
      "✨\n",
      "😍\n",
      "💃\n",
      "😍\n",
      "\n",
      "\n",
      "----------\n",
      "Group: 1\n",
      "749 tweets\n",
      "--> just\n",
      "--> need\n",
      "--> think\n",
      "--> man\n",
      "--> really\n",
      "--> shit\n",
      "--> woke\n",
      "--> wow\n",
      "--> just like\n",
      "--> little\n",
      "🤯\n",
      "😂\n",
      "🇺🇸\n",
      "🤢\n",
      "🌱\n",
      "🙊\n",
      "😂\n",
      "❤\n",
      "🙏\n",
      "🇺🇸\n",
      "💎\n",
      "🇩🇪\n",
      "🤯\n",
      "\n",
      "\n",
      "----------\n",
      "Group: 2\n",
      "902 tweets\n",
      "--> like\n",
      "--> people\n",
      "--> got\n",
      "--> just like\n",
      "--> feel\n",
      "--> hurt\n",
      "--> cute\n",
      "--> look\n",
      "--> change\n",
      "--> feel like\n",
      "😂\n",
      "😱\n",
      "🤯\n",
      "🙈\n",
      "🙊\n",
      "💙\n",
      "🙊\n",
      "😂\n",
      "☺\n",
      "💜\n",
      "🐯\n",
      "😅\n",
      "😢\n",
      "😂\n",
      "🤣\n",
      "\n",
      "\n",
      "----------\n",
      "Group: 3\n",
      "778 tweets\n",
      "--> good\n",
      "--> morning\n",
      "--> good morning\n",
      "--> know\n",
      "--> better\n",
      "--> news\n",
      "--> luck\n",
      "--> night\n",
      "--> hope\n",
      "--> post_abortive87\n",
      "😩\n",
      "👍\n",
      "🇺🇸\n",
      "⭐\n",
      "🥺\n",
      "🥺\n",
      "🙊\n",
      "😂\n",
      "🙄\n",
      "😎\n",
      "🤣\n",
      "😂\n",
      "🏁\n",
      "🇺🇸\n",
      "🌹\n",
      "🇰🇷\n",
      "🇸🇦\n",
      "\n",
      "\n",
      "----------\n",
      "Group: 4\n",
      "1091 tweets\n",
      "--> trump\n",
      "--> realdonaldtrump\n",
      "--> president\n",
      "--> america\n",
      "--> god\n",
      "--> president trump\n",
      "--> 2020\n",
      "--> people\n",
      "--> god bless\n",
      "--> bless\n",
      "🇺🇸\n",
      "🇺🇸\n",
      "👍\n",
      "👍\n",
      "🇺🇸\n",
      "🤦\n",
      "🤦‍♀️\n",
      "♀\n",
      "😂\n",
      "🇺🇸\n",
      "❤\n",
      "👍\n",
      "🙏\n",
      "🇺🇸\n",
      "🇺🇸\n",
      "🇺🇸\n",
      "\n",
      "\n",
      "----------\n",
      "Group: 5\n",
      "495 tweets\n",
      "--> vegan\n",
      "--> food\n",
      "--> let\n",
      "--> friend\n",
      "--> years\n",
      "--> 2019\n",
      "--> lol\n",
      "--> think\n",
      "--> animals\n",
      "--> mind\n",
      "🌱\n",
      "😢\n",
      "😍\n",
      "👏\n",
      "😀\n",
      "🤮\n",
      "©\n",
      "🍴\n",
      "😍\n",
      "🌱\n",
      "💚\n",
      "🌱\n",
      "😭\n",
      "\n",
      "\n",
      "----------\n",
      "Group: 6\n",
      "1217 tweets\n",
      "--> day\n",
      "--> today\n",
      "--> great\n",
      "--> happy\n",
      "--> july\n",
      "--> 4th\n",
      "--> hope\n",
      "--> going\n",
      "--> 4th july\n",
      "--> great day\n",
      "♥\n",
      "🤯\n",
      "❤\n",
      "🌨\n",
      "🌫\n",
      "☔\n",
      "😂\n",
      "💨\n",
      "👆\n",
      "🏎\n",
      "🏁\n",
      "🇩🇪\n",
      "🇮🇹\n",
      "🇺🇸\n",
      "💫\n",
      "🙌\n",
      "💛\n",
      "♥\n",
      "🤓\n",
      "🤯\n",
      "\n",
      "\n",
      "----------\n",
      "Group: 7\n",
      "1498 tweets\n",
      "--> time\n",
      "--> new\n",
      "--> world\n",
      "--> 10\n",
      "--> win\n",
      "--> week\n",
      "--> home\n",
      "--> did\n",
      "--> know\n",
      "--> chance\n",
      "😂\n",
      "🇺🇸\n",
      "😂\n",
      "✌\n",
      "⚡\n",
      "♀\n",
      "🤯\n",
      "😤\n",
      "❤\n",
      "😘\n",
      "🤗\n",
      "🌼\n",
      "☹\n",
      "😍\n",
      "💞\n",
      "🍑\n",
      "🐶\n",
      "🐱\n",
      "\n",
      "\n",
      "----------\n",
      "Group: 8\n",
      "695 tweets\n",
      "--> thank\n",
      "--> great\n",
      "--> yes\n",
      "--> beautiful\n",
      "--> guys\n",
      "--> right\n",
      "--> hope\n",
      "--> look\n",
      "--> following\n",
      "--> mr\n",
      "💙\n",
      "👍\n",
      "♥\n",
      "😊\n",
      "💪\n",
      "🇺🇸\n",
      "🙊\n",
      "💓\n",
      "❤\n",
      "🙏\n",
      "😈\n",
      "💘\n",
      "❣\n",
      "🙈\n",
      "😘\n",
      "💚\n",
      "\n",
      "\n",
      "----------\n",
      "Group: 9\n",
      "1062 tweets\n",
      "--> best\n",
      "--> want\n",
      "--> followers\n",
      "--> followers want\n",
      "--> 1k\n",
      "--> 3k\n",
      "--> 4k\n",
      "--> 2k\n",
      "--> world\n",
      "--> going\n",
      "🎟\n",
      "😍\n",
      "❤\n",
      "🇺🇸\n",
      "☠\n",
      "🏴\n",
      "🏴‍☠️\n",
      "🇺🇸\n",
      "❤\n",
      "🇦🇺\n",
      "🇬🇧\n",
      "🇮🇳\n",
      "🇰🇪\n",
      "🇺🇲\n",
      "🇦🇺\n",
      "🇬🇧\n",
      "🇮🇳\n",
      "🇵🇰\n",
      "🇧🇷\n",
      "🇨🇦\n",
      "🇬🇧\n",
      "🇳🇬\n",
      "🇺🇲\n",
      "🇦🇺\n",
      "🇬🇧\n",
      "🇮🇳\n",
      "🇵🇰\n",
      "\n",
      "\n",
      "🇺🇸 996\n",
      "🤯 400\n",
      "😂 395\n",
      "😍 219\n",
      "🙊 211\n",
      "❤ 203\n",
      "🔥 141\n",
      "😭 127\n",
      "👍 101\n",
      "🇩🇪 95\n",
      "🇰🇷 93\n",
      "🇬🇧 86\n",
      "🙏 81\n",
      "🤣 61\n",
      "👏 58\n",
      "🇯🇵 57\n",
      "♀ 53\n",
      "💜 53\n",
      "🇨🇦 52\n",
      "🙈 47\n",
      "💙 45\n",
      "💚 45\n",
      "♥ 44\n",
      "✨ 44\n",
      "😘 43\n",
      "♂ 43\n",
      "💯 42\n",
      "😎 42\n",
      "🇫🇷 42\n",
      "😊 42\n",
      "🥺 41\n",
      "💪 40\n",
      "💕 38\n",
      "🇧🇷 38\n",
      "🇳🇬 38\n",
      "👀 37\n",
      "🇦🇺 37\n",
      "🎉 37\n",
      "🙌 36\n",
      "🇵🇰 36\n",
      "🚨 35\n",
      "🇮🇳 35\n",
      "🇪🇸 35\n",
      "🤔 33\n",
      "😩 32\n",
      "💥 32\n",
      "🌎 31\n",
      "🤦 30\n",
      "🤷 29\n",
      "😳 29\n",
      "😂\n",
      "\n",
      "🏆\n",
      "🔙\n",
      "🇺🇸\n",
      "\n",
      "®\n",
      "\n",
      "🥺\n",
      "\n",
      "🐎\n",
      "🍻\n",
      "🇺🇸\n",
      "\n",
      "😭\n",
      "👇\n",
      "\n",
      "🤯\n",
      "👀\n",
      "\n",
      "😂\n",
      "\n",
      "⚡\n",
      "\n",
      "🙊\n",
      "💙\n",
      "\n",
      "🗽\n",
      "\n",
      "🚨\n",
      "\n",
      "😍\n",
      "🌈\n",
      "🏳\n",
      "🏳️‍🌈\n",
      "\n",
      "🇺🇸\n",
      "\n",
      "😬\n",
      "\n",
      "🤧\n",
      "\n",
      "🤦\n",
      "♀\n",
      "\n",
      "❌\n",
      "🇦🇺\n",
      "🇰🇷\n",
      "🇲🇾\n",
      "\n",
      "🇺🇸\n",
      "\n",
      "😩\n",
      "\n",
      "😂\n",
      "\n",
      "📣\n",
      "\n",
      "🧹\n",
      "\n",
      "❤\n",
      "🍢\n",
      "\n",
      "😍\n",
      "🙊\n",
      "\n",
      "😍\n",
      "💜\n",
      "🇺🇸\n",
      "\n",
      "💚\n",
      "\n",
      "💔\n",
      "🇺🇸\n",
      "\n",
      "♥\n",
      "\n",
      "⭐\n",
      "🔸\n",
      "\n",
      "👍\n",
      "🇺🇸\n",
      "\n",
      "👇\n",
      "🌎\n",
      "\n",
      "🙊\n",
      "🏃\n",
      "\n",
      "😘\n",
      "\n",
      "🇺🇸\n",
      "\n",
      "🤯\n",
      "\n",
      "📦\n",
      "🔨\n",
      "🔞\n",
      "➡\n",
      "⬅\n",
      "✅\n",
      "🇺🇸\n",
      "\n",
      "😓\n",
      "\n",
      "🙄\n",
      "😢\n",
      "🚌\n",
      "🇬🇧\n",
      "🇸🇬\n",
      "🇺🇸\n",
      "\n",
      "😂\n",
      "\n",
      "🦄\n",
      "🇺🇸\n",
      "\n",
      "😒\n",
      "\n",
      "🤯\n",
      "\n",
      "😍\n",
      "🤯\n",
      "😩\n",
      "\n",
      "👇\n",
      "\n",
      "💕\n",
      "🤛\n",
      "🤜\n",
      "\n",
      "💕\n",
      "📽\n",
      "\n",
      "😡\n",
      "\n",
      "😍\n",
      "❤\n",
      "💇\n",
      "🗣\n",
      "📲\n",
      "♀\n",
      "\n",
      "😹\n",
      "\n",
      "💯\n",
      "🙏\n",
      "\n",
      "🚨\n",
      "🔗\n",
      "\n",
      "😎\n",
      "👋\n",
      "👍\n",
      "✔\n",
      "🇺🇸\n",
      "\n",
      "😂\n",
      "\n",
      "♥\n",
      "\n",
      "🎾\n",
      "\n",
      "🔥\n",
      "💰\n",
      "1️⃣\n",
      "2️⃣\n",
      "\n",
      "👨\n",
      "👩\n",
      "\n",
      "🤯\n",
      "\n",
      "🌎\n",
      "🇺🇸\n",
      "\n",
      "🙌\n",
      "🇺🇸\n",
      "\n",
      "💜\n",
      "\n",
      "🙊\n",
      "\n",
      "😇\n",
      "💯\n",
      "🇺🇸\n",
      "\n",
      "🇺🇸\n",
      "\n",
      "🇦🇪\n",
      "🇦🇺\n",
      "🇧🇪\n",
      "🇧🇷\n",
      "🇩🇪\n",
      "🇬🇧\n",
      "🇮🇳\n",
      "🇰🇷\n",
      "🇳🇫\n",
      "🇳🇬\n",
      "🇳🇱\n",
      "🇺🇸\n",
      "🇿🇦\n",
      "🇿🇼\n",
      "\n",
      "😂\n",
      "\n",
      "🙊\n",
      "\n",
      "🇸🇪\n",
      "\n",
      "👇\n",
      "\n",
      "😁\n",
      "🤗\n",
      "❤\n",
      "🐶\n",
      "🐾\n",
      "🇦🇺\n",
      "🇨🇦\n",
      "🇺🇸\n",
      "\n",
      "🖐\n",
      "\n",
      "😂\n",
      "\n",
      "🥚\n",
      "\n",
      "🚨\n",
      "\n",
      "😢\n",
      "🇺🇸\n",
      "\n",
      "😍\n",
      "\n",
      "🔥\n",
      "\n",
      "💜\n",
      "🔥\n",
      "\n",
      "❤\n",
      "🙏\n",
      "🇺🇸\n",
      "\n",
      "🥺\n",
      "😭\n",
      "\n",
      "🇦🇲\n",
      "🇬🇳\n",
      "🇭🇰\n",
      "🇮🇱\n",
      "🇮🇲\n",
      "🇮🇹\n",
      "🇯🇲\n",
      "🇯🇴\n",
      "🇯🇵\n",
      "🇰🇪\n",
      "🇰🇭\n",
      "🇰🇲\n",
      "🇰🇳\n",
      "🇰🇵\n",
      "🇰🇷\n",
      "🇰🇼\n",
      "🇱🇦\n",
      "🇱🇮\n",
      "🇱🇰\n",
      "🇱🇷\n",
      "🇱🇸\n",
      "🇱🇹\n",
      "🇲🇱\n",
      "🇲🇴\n",
      "🇲🇷\n",
      "🇲🇽\n",
      "🇲🇾\n",
      "🇳🇦\n",
      "🇳🇬\n",
      "🇳🇮\n",
      "🇳🇱\n",
      "🇳🇴\n",
      "🇳🇷\n",
      "🇳🇺\n",
      "🇳🇿\n",
      "🇵🇰\n",
      "🇸🇱\n",
      "🇹🇱\n",
      "🇺🇳\n",
      "\n",
      "🤦\n",
      "🤦‍♀️\n",
      "♀\n",
      "\n",
      "😢\n",
      "❤\n",
      "✨\n",
      "🇺🇸\n",
      "\n",
      "🤣\n",
      "👍\n",
      "🇺🇸\n",
      "\n",
      "🇺🇸\n",
      "\n",
      "🗣\n",
      "\n",
      "😭\n",
      "\n",
      "🇺🇸\n",
      "\n",
      "🔥\n",
      "✨\n",
      "🇰🇷\n",
      "\n",
      "🤣\n",
      "🤷\n",
      "♂\n",
      "\n",
      "❤\n",
      "👏\n",
      "🇺🇸\n",
      "\n",
      "💁\n",
      "♀\n",
      "\n",
      "🇺🇸\n",
      "\n",
      "🐎\n",
      "🍻\n",
      "🇺🇸\n",
      "\n",
      "😍\n",
      "💜\n",
      "💯\n",
      "\n",
      "😭\n",
      "\n",
      "😻\n",
      "😽\n",
      "💬\n",
      "🙌\n",
      "📣\n",
      "🎼\n",
      "📲\n",
      "💱\n",
      "🔄\n",
      "🔂\n",
      "🇦🇺\n",
      "\n",
      "🦖\n",
      "🌈\n",
      "\n",
      "🤯\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS\n",
    "from sklearn.decomposition import NMF\n",
    "import math\n",
    "from collections import Counter\n",
    "import re\n",
    "\n",
    "def find_emoji(word,df_emoji):\n",
    "    options = []\n",
    "    for i, name in enumerate(df_emoji['short_name']):\n",
    "        if word in name:\n",
    "            options.append((name,df_emoji['unichar'][i]))\n",
    "            print(name,df_emoji['unichar'][i])\n",
    "    return options\n",
    "\n",
    "def get_emojis(tweet_lst,df_emoji):\n",
    "    emoji_idx = []\n",
    "    emoji_char =[]\n",
    "    for tweet in tweet_lst:\n",
    "        for i,uni in enumerate(df_emoji['unichar']):\n",
    "            if uni in tweet:\n",
    "                 emoji_idx.append(i)\n",
    "                 emoji_char.append(uni)\n",
    "    return emoji_idx, emoji_char\n",
    "\n",
    "def get_emojis_by_tweet(tweet_lst,df_emoji):\n",
    "    by_tweet = []\n",
    "\n",
    "    for tweet in tweet_lst:\n",
    "        emoji_char =[]\n",
    "        for i,uni in enumerate(df_emoji['unichar']):\n",
    "            if uni in tweet:\n",
    "                 emoji_char.append(uni)\n",
    "        by_tweet.append(emoji_char)\n",
    "    return by_tweet\n",
    "\n",
    "#this seems to get some emojis that i dont but also missed some that i do. It also get duplicates per tweet that i dont\n",
    "# def get_emojis_2(tweet_lst):\n",
    "#     emojis = []\n",
    "#     for tweet in tweet_lst:\n",
    "#         emoji = re.compile(u'[\\uD800-\\uDBFF][\\uDC00-\\uDFFF]')\n",
    "#         emojis.append(emoji.findall(tweet))\n",
    "#     return emojis\n",
    "\n",
    "def print_emoji(tweet,emoji_char):\n",
    "    for uni in emoji_char:\n",
    "        if uni in tweet:\n",
    "            print(uni)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    tweets = np.array(list(pickle.load(open('./data/yay_moji.pkl','rb'))))\n",
    "    # type is list\n",
    "\n",
    "    emojis = pd.read_pickle('./data/df_emoji.pkl')\n",
    "    # type is DataFrame\n",
    "\n",
    "\n",
    "\n",
    "    # ------------- tfidf\n",
    "    stopwords = set(list(ENGLISH_STOP_WORDS) + ['rt', 'follow', 'dm', 'https', 'ur', 'll' ,'amp', 'subscribe', 'don', 've', 'retweet', 'im', 'http'])\n",
    "\n",
    "    tfidf = TfidfVectorizer(max_features=10000, max_df=0.05, min_df=0.001, stop_words = stopwords, ngram_range = (1,2))\n",
    "\n",
    "    #lemmetizing need to consider cleaning the tweets myself\n",
    "\n",
    "    tfidf_tweets = tfidf.fit_transform(tweets)\n",
    "    bag = np.array(tfidf.get_feature_names())\n",
    "\n",
    "    # -------------- NMF\n",
    "    k = 10\n",
    "     #number of groups\n",
    "    nmf2 = NMF(n_components = k)\n",
    "    nmf2.fit(tfidf_tweets)\n",
    "    W = nmf2.transform(tfidf_tweets) #len(yay_moji,k)\n",
    "    H = nmf2.components_ #k,len(yay_moji)\n",
    "\n",
    "\n",
    "    # --------------- Printing Top 10\n",
    "    tweet_lst = []\n",
    "    top = 10\n",
    "    tweet_in_group_thresh = .005 #score thresh if we consider that tweet as part of that group\n",
    "    for group in range(k):\n",
    "        #idx of the top ten words for each group\n",
    "        i_words = np.argsort(H[group])[::-1][:top]\n",
    "        words = bag[i_words]\n",
    "\n",
    "        # idx of the top ten tweets for each group\n",
    "        i_emojis = np.argsort(W[:,group])[::-1][:top]\n",
    "        # most common 10 emojis for each group\n",
    "\n",
    "        print('-'*10)\n",
    "        print('Group:',group)\n",
    "        counted_tweets = np.argwhere(W[:,group] > tweet_in_group_thresh)\n",
    "        print(counted_tweets.shape[0], 'tweets')\n",
    "        for word in words:\n",
    "            print('-->',word)\n",
    "        for i_tweet in i_emojis:\n",
    "            print_emoji(tweets[i_tweet], emojis['unichar'])\n",
    "            tweet_lst.append(tweets[i_tweet])\n",
    "        ind, emo_lst = get_emojis(tweets[i_emojis],emojis)\n",
    "        # find percentage of emoji per group\n",
    "        most_emoji, how_many = Counter(emo_lst).most_common(1)[0]\n",
    "        score = float(how_many)/top\n",
    "        # print score #score is not perfect - similar emojis and repeat in the same tweet\n",
    "        print('\\n')\n",
    "\n",
    "    # --------------- printing most common emojis\n",
    "    most_common = 50\n",
    "    b,all_emojis = get_emojis(tweets,emojis)\n",
    "    count = Counter(all_emojis).most_common(most_common)\n",
    "    unicode_top = []\n",
    "    for emo, i in count:\n",
    "        print(emo,i)\n",
    "        for j, char in enumerate(emojis['unichar']):\n",
    "            if char == emo:\n",
    "                unicode_top.append(emojis['unichar'][j])\n",
    "\n",
    "\n",
    "# test stuff\n",
    "    jan = get_emojis_by_tweet(tweets[0:100],emojis)\n",
    "    for tweet in jan:\n",
    "        for emo in tweet:\n",
    "            print(emo,)\n",
    "        print('')\n",
    "    # name_of = find_emoji('heart',emojis)\n",
    "\n",
    "\n",
    "    '''\n",
    "    to do's\n",
    "    --> how big are the groups? do a most common\n",
    "    --> get a better score system\n",
    "    --> allow for tweets with multiple emojis\n",
    "    --> sub set for tweets with a specific emoji\n",
    "    --> commonly combined emojis\n",
    "    --> naive bayes\n",
    "        prediction accuracy between emojis for how similar they are\n",
    "    whats the purpose:\n",
    "    --> to help use emojis as labels for tweets\n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--> ocasio =\n",
      "🧹   3.2315936025548407e-40  \n",
      "🧡   5.134160423670935e-69  \n",
      "🧐   3.7119745947176606e-35  \n",
      "🦓   0.11111088111755894  \n",
      "🦋   9.340148717550855e-83  \n",
      "\n",
      "--> climate change =\n",
      "🤒   6.634989776190943e-33  \n",
      "🚗   5.1888566027721555e-96  \n",
      "🗳   2.294988066281684e-52  \n",
      "👇   9.257689864392038e-303  \n",
      "🎓   2.524296308827204e-144  \n",
      "\n",
      "--> vegan =\n",
      "🤦   3.599614923792856e-232  \n",
      "😢   4.0794779157928546e-75  \n",
      "😡   1.127158024820009e-168  \n",
      "😔   1.0662844337858481e-198  \n",
      "😋   6.122445647588763e-110  \n",
      "\n",
      "--> earth =\n",
      "🤟   2.028946692897737e-36  \n",
      "🐋   1.0  \n",
      "🏟   1.3189489143397244e-51  \n",
      "🌏   7.057320726225616e-287  \n",
      "\n",
      "--> greta =\n",
      "🧹   3.2315936025548407e-40  \n",
      "🧡   5.134160423670935e-69  \n",
      "🧐   3.7119745947176606e-35  \n",
      "🦓   0.11111088111755894  \n",
      "🦋   9.340148717550855e-83  \n",
      "\n",
      "--> korea =\n",
      "🕺   8.480675382235633e-109  \n",
      "💛   1.8814951513477593e-209  \n",
      "💔   1.0805641867324546e-120  \n",
      "👑   1.0  \n",
      "🎉   4.2086070926140675e-306  \n",
      "\n",
      "--> abortion =\n",
      "🤬   2.6287289042468105e-21  \n",
      "🤝   4.341096738792882e-10  \n",
      "🤒   8.491163585538909e-29  \n",
      "😳   1.473566594080649e-250  \n",
      "😣   2.3566451314108866e-36  \n",
      "\n",
      "--> love you =\n",
      "🧡   3.926179898523362e-37  \n",
      "🦋   1.2523513123842175e-77  \n",
      "🤷   1.6984523642347511e-298  \n",
      "🤜   1.7606501363575637e-32  \n",
      "🤛   1.0402848252882199e-29  \n",
      "\n",
      "--> birthday =\n",
      "🧡   3.534752075779215e-24  \n",
      "🤷   2.0368564797358588e-266  \n",
      "🤦   3.820451157903502e-286  \n",
      "😳   2.61600675461105e-258  \n",
      "🎶   3.5383772526604523e-295  \n",
      "\n",
      "--> i want a divorce =\n",
      "🥴   2.2474385840075704e-85  \n",
      "🤡   1.3720875791743404e-166  \n",
      "😻   1.839898928686422e-57  \n",
      "😮   1.0432037668532157e-213  \n",
      "😢   2.7692598826147156e-136  \n",
      "\n",
      "--> life =\n",
      "🥴   9.14389091022032e-25  \n",
      "🤪   3.677989735300684e-103  \n",
      "🤣   5e-324  \n",
      "😡   1.7322762635846725e-149  \n",
      "😁   2.9402719535910826e-99  \n",
      "\n",
      "--> baby =\n",
      "🦁   1.0  \n",
      "💔   1.515849186285941e-220  \n",
      "👶   3.938588461129528e-68  \n",
      "🐰   3.4629578516397155e-65  \n",
      "🌸   7.440683998421424e-138  \n",
      "\n",
      "--> basketball =\n",
      "🧹   3.2315936025548407e-40  \n",
      "🧡   5.134160423670935e-69  \n",
      "🧐   3.7119745947176606e-35  \n",
      "🦓   0.11111088111755894  \n",
      "🦋   9.340148717550855e-83  \n",
      "\n",
      "--> i need coffee =\n",
      "😘   1.0  \n",
      "\n",
      "--> la la land =\n",
      "🏟   3.0541529188756623e-32  \n",
      "🎶   6.469453313756039e-243  \n",
      "⬇   1.0  \n",
      "\n",
      "--> netflix and chill =\n",
      "🥰   1.2126687423441382e-129  \n",
      "😵   1.1887172648636834e-80  \n",
      "📀   2.514549911531027e-34  \n",
      "💻   4.201244206727935e-149  \n",
      "🍅   1.0  \n",
      "\n",
      "--> i am thankful to be alive =\n",
      "🥰   1.0  \n",
      "\n",
      "--> ocean =\n",
      "🐢   1.0  \n",
      "🐙   1.254335799832252e-65  \n",
      "🐋   6.332816078630981e-45  \n",
      "\n",
      "--> trump =\n",
      "🤦   3.72109421816215e-245  \n",
      "🚂   8.1888327114749e-58  \n",
      "😡   7.947575429311131e-161  \n",
      "😉   1.3109909261213645e-232  \n",
      "😇   2.4362578995895307e-251  \n",
      "\n",
      "--> plastic =\n",
      "🧹   3.2315936025548407e-40  \n",
      "🧡   5.134160423670935e-69  \n",
      "🧐   3.7119745947176606e-35  \n",
      "🦓   0.11111088111755894  \n",
      "🦋   9.340148717550855e-83  \n",
      "\n",
      "--> boyfriend =\n",
      "🧹   3.2315936025548407e-40  \n",
      "🧡   5.134160423670935e-69  \n",
      "🧐   3.7119745947176606e-35  \n",
      "🦓   0.11111088111755894  \n",
      "🦋   9.340148717550855e-83  \n",
      "\n",
      "\n",
      "----- Top 5 words for each Emoji in Train set\n",
      "------------------------------------------------------------\n",
      "1️⃣  --> ['10' 'enter' 'winner' 'giveaway' '100']\n",
      "3️⃣  --> ['year' 'year award' 'coach' 'award' 'leader']\n",
      "6️⃣  --> ['year' 'year award' 'coach' 'award' 'leader']\n",
      "©  --> ['work' 'art' 'point' 'update' 'concert']\n",
      "®  --> ['chance win' 'post' 'chance' 'amazing' 'win']\n",
      "™  --> ['gay' 'photo' 'lol' 'abortion' 'th']\n",
      "⏩  --> ['currently' 'united states' 'states' 'united' 'elland road']\n",
      "⏪  --> ['currently' 'united states' 'states' 'united' 'elland road']\n",
      "⏰  --> ['expect' 'house' 'vote' 'time' 'change']\n",
      "⏳  --> ['berlin' 'brexit' 'lose' 'climate change' 'climate']\n",
      "▶  --> ['perfect gift' 'order' 'gift' 'perfect' 'family member']\n",
      "◀  --> ['sale' 'american' 'maga' 'buy' 'usa']\n",
      "☀  --> ['ya' 'beach' 'debut' 'summer' '19']\n",
      "☁  --> ['nctzenselcaday' 'maybe' 'star' 'nctzenselcaday przwv8jdxg' 'fairy']\n",
      "☄  --> ['yea' 'baby' 'fan' 'set' 'life']\n",
      "☑  --> ['gain' 'like' 'stock' 'lucky' 'twitter world']\n",
      "☔  --> ['rain' 'day' 'muafgoi4pq' 'day forecast' 'forecast']\n",
      "☕  --> ['morning' 'twitter' 'pilot' 'morning person' 'drive people']\n",
      "☝  --> ['talk' 'champ' 'tag' 'nice' 'time']\n",
      "☠  --> ['best' '𝐌𝐀𝐓𝐂𝐇 computer' 'engineer khatputli' 'elland road' 'elli']\n",
      "☦  --> ['kag2020' 'washington' '𝐌𝐀𝐓𝐂𝐇 computer' 'energy drinks' 'energy']\n",
      "☮  --> ['run' 'resist fgkqdvmvfr' 'strongertogether resist' 'resist'\n",
      " 'list strongertogether']\n",
      "☹  --> ['sad' 'home' 'time' 'tech' 'control']\n",
      "☺  --> ['yo' 'excited' 'love' 'like' 'cool']\n",
      "♀  --> ['time' 'say' 'start' 'talk' 'trump']\n",
      "♂  --> ['smoke' 'ball' 'try' 'best' 'blonde']\n",
      "♈  --> ['mercury' 'ekcraiquxc' 'mar' 'venus mar' 'sun moon']\n",
      "♉  --> ['mercury' 'ekcraiquxc' 'mar' 'venus mar' 'sun moon']\n",
      "♍  --> ['mercury' 'ekcraiquxc' 'mar' 'venus mar' 'sun moon']\n",
      "♎  --> ['mercury' 'ekcraiquxc' 'mar' 'venus mar' 'sun moon']\n",
      "♏  --> ['mercury' 'ekcraiquxc' 'mar' 'venus mar' 'sun moon']\n",
      "♐  --> ['mercury' 'ekcraiquxc' 'mar' 'venus mar' 'sun moon']\n",
      "♑  --> ['mercury' 'ekcraiquxc' 'mar' 'venus mar' 'sun moon']\n",
      "♒  --> ['mercury' 'ekcraiquxc' 'mar' 'venus mar' 'sun moon']\n",
      "♓  --> ['mercury' 'ekcraiquxc' 'mar' 'venus mar' 'sun moon']\n",
      "♥  --> ['love' 'thank' 'protect' 'heart' 'twice']\n",
      "♦  --> ['job' 'uk' 'really' 'usa' '𝐌𝐀𝐓𝐂𝐇 computer']\n",
      "♻  --> ['fast reply' 'brexit' 'fast' 'climate change' 'climate']\n",
      "♾  --> ['somebody' 'fully' 'understand' 'need' 'world']\n",
      "⚓  --> ['add' '2nd' 'officially' 'second' 'friday']\n",
      "⚔  --> ['wwg1wga' 'thank' 'emergency' 'energy drinks' 'energy']\n",
      "⚖  --> ['inside' 'barr' 'brexit' 'year' 'climate change']\n",
      "⚠  --> ['best' 'bts' 'vote' 'let' 'proceed caution']\n",
      "⚡  --> ['acc' 'time' 'start' 'happen' 'excite']\n",
      "⚪  --> ['sky' 'relationship' 'single' 'beach' 'lose']\n",
      "⚫  --> ['85' '𝐌𝐀𝐓𝐂𝐇 computer' 'enjoy' 'elli' 'elli tom']\n",
      "⚽  --> ['arrive' 'stand' 'gt' 'fan' 'gt gt']\n",
      "⚾  --> ['great' 'defeat' 'baseball' 'line' 'sure']\n",
      "⛈  --> ['disaster cycl' 'hurricanes wildfire' 'extreme weather' 'extreme' 'flood']\n",
      "⛓  --> ['refuse' 'forget' '𝐌𝐀𝐓𝐂𝐇 computer' 'enter' 'engineer khatputli']\n",
      "⛔  --> ['join' 'north' 'stand' 'stop' 'let']\n",
      "⛰  --> ['book' 'car' 'buy' 'come' 'rbmitpin chance']\n",
      "⛳  --> ['play' 'course' 'best' 'think' 'make']\n",
      "⛷  --> ['motivate' 'dump' 'seriously' 'bear' 'exam']\n",
      "⛺  --> ['fast reply' 'fast' 'reply' 'elland road' 'elli']\n",
      "⛽  --> ['believe cow' 'medium like' 'cause climate' 'easytarget' 'people believe']\n",
      "✅  --> ['trump' 'suffer' 'climate change' 'climate' 'late']\n",
      "✈  --> ['safe flight' 'flight' 'safe' 'send' 'home']\n",
      "✊  --> ['air' 'child' 'hate' 'man' 'today']\n",
      "✌  --> ['time' 'bts' 'real' 'goal' 'hoe']\n",
      "✍  --> ['𝐌𝐀𝐓𝐂𝐇 computer' 'elland' 'elli' 'elli tom' 'ellis']\n",
      "✔  --> ['president' 'favorite' 'kag2020' 'brand' 'online']\n",
      "✝  --> ['prayer' 'great' 'amen' 'thank' 'service']\n",
      "✡  --> ['kag2020' 'washington' '𝐌𝐀𝐓𝐂𝐇 computer' 'energy drinks' 'energy']\n",
      "✨  --> ['right' 'need' 'shine' 'star' 'ready']\n",
      "❄  --> ['past' 'order' 'read' 'open' 'hi']\n",
      "❌  --> ['light' 'wall' 'emergency' 'border' 'fake']\n",
      "❓  --> ['reply' '50k reply' '1k 5k' 'country reply' 'reply flag']\n",
      "❔  --> ['fast reply' 'fast' 'reply' 'elland road' 'elli']\n",
      "❗  --> ['watch' 'new' 'let' 'prayer' 'beach']\n",
      "❣  --> ['want' 'tweet' 'women' 'power' 'leg']\n",
      "❤  --> ['love' 'thank' 'happy' 'good' 'great']\n",
      "➕  --> ['gain' 'like' 'gain 800' '800 follower' 'gain time']\n",
      "➖  --> ['𝐌𝐀𝐓𝐂𝐇 computer' 'elland' 'elli' 'elli tom' 'ellis']\n",
      "➡  --> ['leader' 'reveal' 'read' 'ya' 'sport']\n",
      "⬅  --> ['border' '10' 'wall' 'population' 'euro']\n",
      "⬇  --> ['live' 'tale' 'late' 'afternoon' 'stadium']\n",
      "⬛  --> ['create' '40' 'buy' 'today' 'day']\n",
      "⭐  --> ['good' 'question' 'proud' 'era' 'rollercoaster']\n",
      "⭕  --> ['gain' 'like' 'twitter world' 'reply yes' 'hi twitter']\n",
      "🅰  --> ['era' 'single' 'season' '12' '10']\n",
      "🆚  --> ['stadium' 'el' 'su' 'park' 'america']\n",
      "🇦🇨  --> ['new york' 'york' 'toronto' 'la' 'washington']\n",
      "🇦🇩  --> ['fast 100' '200 300' '500 600' '400 500' '100 200']\n",
      "🇦🇪  --> ['trade' 'sell' 'uk run' 'surplus sell' 'sell sell']\n",
      "🇦🇫  --> ['100' '100 000' 'death' 'pakistan' 'age']\n",
      "🇦🇱  --> ['grand final' 'eurovision grand' 'grand' 'eurovision' 'final']\n",
      "🇦🇴  --> ['fast 100' '200 300' '500 600' '400 500' '100 200']\n",
      "🇦🇶  --> ['followers' 'followers followers' 'real fast' 'fast followers'\n",
      " 'followers follower']\n",
      "🇦🇷  --> ['followers' 'country' 'follower' 'follower want' '50k 100k']\n",
      "🇦🇸  --> ['fast 100' '200 300' '500 600' '400 500' '100 200']\n",
      "🇦🇹  --> ['itst' 'scotland' 'face future' 'yes itst' 'future yes']\n",
      "🇦🇺  --> ['followers' 'followers followers' 'follower' 'follower want'\n",
      " 'want followers']\n",
      "🇦🇼  --> ['followers' 'followers followers' 'real fast' 'fast followers'\n",
      " 'followers follower']\n",
      "🇦🇿  --> ['fast 100' '200 300' '500 600' '400 500' '100 200']\n",
      "🇧🇧  --> ['fast 100' '200 300' '500 600' '400 500' '100 200']\n",
      "🇧🇩  --> ['fast 100' '200 300' '500 600' '400 500' '100 200']\n",
      "🇧🇪  --> ['attack' 'project' 'school' 'project project' 'result']\n",
      "🇧🇬  --> ['su' 'grand final' 'eurovision grand' 'grand' 'young']\n",
      "🇧🇮  --> ['gaqhihqews' 'offer xlskobir3z' 'late bookmaker' 'bookmaker offer'\n",
      " 'bookmaker']\n",
      "🇧🇯  --> ['tv episode' 'mjf' 'tonight feature' 'episode tonight' 'feature vs']\n",
      "🇧🇱  --> ['country' '𝐌𝐀𝐓𝐂𝐇 computer' 'enjoy' 'elli' 'elli tom']\n",
      "🇧🇷  --> ['followers' 'reply' 'country' 'want' 'like reply']\n",
      "🇧🇸  --> ['fast 100' '200 300' '500 600' '400 500' '100 200']\n",
      "🇧🇾  --> ['trade' 'sell' '500 600' '300 400' '400 500']\n",
      "🇨🇦  --> ['canada' 'country' 'followers' '2003' 'age']\n",
      "🇨🇫  --> ['67' 'mli' '67 mli' 'sle' 'les 67']\n",
      "🇨🇬  --> ['country' '𝐌𝐀𝐓𝐂𝐇 computer' 'enjoy' 'elli' 'elli tom']\n",
      "🇨🇭  --> ['switzerland' 'australia' 'usa' 'canada' '62']\n",
      "🇨🇮  --> ['followers' 'followers followers' 'want followers' 'follower want'\n",
      " 'follower']\n",
      "🇨🇱  --> ['want armymutual' 'armymutual' 'want' 'want gain' 'fast comment']\n",
      "🇨🇳  --> ['china' '2019' 'june' 'germany' '2003 2007']\n",
      "🇨🇴  --> ['cloud9' 'eunited' 'ghost' 'gt' 'vs']\n",
      "🇨🇵  --> ['grand final' 'eurovision grand' 'grand' 'eurovision' 'final']\n",
      "🇨🇺  --> ['country' '𝐌𝐀𝐓𝐂𝐇 computer' 'enjoy' 'elli' 'elli tom']\n",
      "🇨🇾  --> ['eurovision' 'grand final' 'eurovision grand' 'grand' 'final']\n",
      "🇨🇿  --> ['korean' 'air' 'eurovision grand' 'grand final' 'grand']\n",
      "🇩🇪  --> ['germany' 'klose' 'german' 'berlin' 'country']\n",
      "🇩🇰  --> ['country' 'reply' 'like reply' 'like' 'connect']\n",
      "🇩🇿  --> ['country' 'handsome njkgdf6qiz' 'handsome' 'njkgdf6qiz' 'like reply']\n",
      "🇪🇦  --> ['grand final' 'eurovision grand' 'grand' 'eurovision' 'final']\n",
      "🇪🇨  --> ['age' 'forget' 'little' 'emotional' 'engineer khatputli']\n",
      "🇪🇪  --> ['estonia' 'summer' 'council' 'happy' 'million']\n",
      "🇪🇬  --> ['eurovision grand' 'grand final' 'grand' 'eurovision' 'final']\n",
      "🇪🇷  --> ['reply' 'world reply' 'country like' 'flag country' 'friend world']\n",
      "🇪🇸  --> ['country' 'connect' 'comment' 'fast' '26']\n",
      "🇪🇺  --> ['brexit' 'climate change' 'climate' 'change' 'country']\n",
      "🇫🇮  --> ['navy' 'ship' 'stand' 'group' 'exercise']\n",
      "🇫🇯  --> ['followers' 'followers followers' 'real fast' 'fast followers'\n",
      " 'followers follower']\n",
      "🇫🇷  --> ['instagram' 'safe flight' 'flight' 'want' '2019']\n",
      "🇬🇧  --> ['followers' 'followers followers' 'follower' 'follower want'\n",
      " 'want followers']\n",
      "🇬🇭  --> ['follower' 'let' 'follower want' 'let connect' 'connect']\n",
      "🇬🇳  --> ['follower want' 'follower' 'want' '500k reply' '50k 100k']\n",
      "🇬🇷  --> ['map' 'battle' 'early' 'continue' 'july']\n",
      "🇬🇸  --> ['travels 0r47ay58e5' '0r47ay58e5' 'let far' 'far tweet' 'tweet travels']\n",
      "🇬🇹  --> ['offer xlskobir3z' 'xlskobir3z gaqhihqews' 'bookmaker offer' 'bookmaker'\n",
      " 'late bookmaker']\n",
      "🇬🇾  --> ['country' '𝐌𝐀𝐓𝐂𝐇 computer' 'enjoy' 'elli' 'elli tom']\n",
      "🇭🇰  --> ['country' 'trade' 'sell' 'trade surplus' 'uk run']\n",
      "🇭🇲  --> ['53' '62' 'norway' 'gdp' 'singapore']\n",
      "🇭🇳  --> ['country' '𝐌𝐀𝐓𝐂𝐇 computer' 'enjoy' 'elli' 'elli tom']\n",
      "🇭🇷  --> ['country' 'face future' 'itst' 'yes itst' 'future yes']\n",
      "🇭🇹  --> ['country' 'reply' 'like reply' 'connect' 'comment']\n",
      "🇭🇺  --> ['country' '15 different' 'member 15' 'service member' 'build inte']\n",
      "🇮🇩  --> ['00pm' 'bear' '50' '1995' 'armymutual']\n",
      "🇮🇪  --> ['country' 'armymutual' 'want armymutual' 'want' '1kfollower 2kfollower']\n",
      "🇮🇱  --> ['country' 'fly' 'second' 'thank' 'travels country']\n",
      "🇮🇲  --> ['country' '𝐌𝐀𝐓𝐂𝐇 computer' 'enjoy' 'elli' 'elli tom']\n",
      "🇮🇳  --> ['followers' 'followers followers' 'want followers' 'india' 'follower']\n",
      "🇮🇴  --> ['country' '𝐌𝐀𝐓𝐂𝐇 computer' 'enjoy' 'elli' 'elli tom']\n",
      "🇮🇶  --> ['country' '𝐌𝐀𝐓𝐂𝐇 computer' 'enjoy' 'elli' 'elli tom']\n",
      "🇮🇷  --> ['country' '𝐌𝐀𝐓𝐂𝐇 computer' 'enjoy' 'elli' 'elli tom']\n",
      "🇮🇸  --> ['eurovision grand' 'grand final' 'grand' '2010 2012' '2008 2009']\n",
      "🇮🇹  --> ['instagram' 'project' 'eurovision' 'tom' 'project project']\n",
      "🇯🇲  --> ['followers' 'follower' 'jamaica' 'let' 'follower want']\n",
      "🇯🇴  --> ['art sunset' 'perfect goodnight' 'sunset' 'goodnight art'\n",
      " 'travels country']\n",
      "🇯🇵  --> ['country' 'japan' 'want' 'let' 'reply']\n",
      "🇰🇪  --> ['country' 'follower' 'let' 'follower want' 'want']\n",
      "🇰🇬  --> ['country' '𝐌𝐀𝐓𝐂𝐇 computer' 'enjoy' 'elli' 'elli tom']\n",
      "🇰🇭  --> ['country' 'travels country' 'tweet travels' 'travels' 'let far']\n",
      "🇰🇳  --> ['country' 'perfect goodnight' 'sunset' 'goodnight art' 'art sunset']\n",
      "🇰🇵  --> ['country' 'country reply' 'reply flag' 'flag' 'reply']\n",
      "🇰🇷  --> ['korea' 'south korea' 'south' 'country' 'seoul']\n",
      "🇰🇼  --> ['country' '𝐌𝐀𝐓𝐂𝐇 computer' 'enjoy' 'elli' 'elli tom']\n",
      "🇰🇾  --> ['country' '𝐌𝐀𝐓𝐂𝐇 computer' 'enjoy' 'elli' 'elli tom']\n",
      "🇱🇦  --> ['country reply' 'reply flag' 'country' 'flag' 'reply']\n",
      "🇱🇮  --> ['perfect goodnight' 'sunset' 'goodnight art' 'art sunset'\n",
      " 'travels country']\n",
      "🇱🇰  --> ['perfect goodnight' 'sunset' 'goodnight art' 'art sunset'\n",
      " 'travels country']\n",
      "🇱🇷  --> ['country' '80' 'country reply' 'reply flag' 'flag']\n",
      "🇱🇸  --> ['country reply' 'reply flag' 'country' 'flag' 'reply']\n",
      "🇱🇹  --> ['country' 'perfect goodnight' 'sunset' 'art sunset' 'goodnight art']\n",
      "🇱🇺  --> ['country' 'country reply' 'reply flag' 'flag' 'reply']\n",
      "🇱🇻  --> ['country reply' 'reply flag' 'flag' 'reply' 'country']\n",
      "🇲🇦  --> ['country' 'reply' 'connect' 'comment' 'like reply']\n",
      "🇲🇰  --> ['grand final' 'eurovision grand' 'grand' 'eurovision' 'final']\n",
      "🇲🇲  --> ['country reply' 'reply flag' 'flag' 'reply' 'country']\n",
      "🇲🇷  --> ['travels 0r47ay58e5' '0r47ay58e5' 'let far' 'far tweet' 'tweet travels']\n",
      "🇲🇹  --> ['0r47ay58e5' 'travels 0r47ay58e5' 'yes itst' 'face future' 'scotland']\n",
      "🇲🇺  --> ['travels 0r47ay58e5' '0r47ay58e5' 'let far' 'far tweet' 'tweet travels']\n",
      "🇲🇻  --> ['travels 0r47ay58e5' '0r47ay58e5' 'let far' 'far tweet' 'tweet travels']\n",
      "🇲🇼  --> ['travels 0r47ay58e5' '0r47ay58e5' 'let far' 'far tweet' 'tweet travels']\n",
      "🇲🇽  --> ['pm' 'author' 'yay' 'realize' 'music']\n",
      "🇲🇾  --> ['country' 'country reply' 'reply flag' 'flag' 'reply']\n",
      "🇲🇿  --> ['travels 0r47ay58e5' '0r47ay58e5' 'let far' 'far tweet' 'tweet travels']\n",
      "🇳🇦  --> ['travels country' 'tweet travels' 'travels' 'let far' 'far tweet']\n",
      "🇳🇪  --> ['reply' 'let' 'connect comment' 'let connect' 'connect']\n",
      "🇳🇬  --> ['let' 'followers' 'nigeria' 'let connect' 'reply']\n",
      "🇳🇮  --> ['travels country' 'tweet travels' 'travels' 'let far' 'far tweet']\n",
      "🇳🇱  --> ['eurovision' '2010' '2009 2010' '2003 2005' '2005']\n",
      "🇳🇴  --> ['country reply' 'reply flag' 'country' 'flag' 'reply']\n",
      "🇳🇷  --> ['goodnight art' 'art sunset' 'sunset' 'perfect goodnight' 'goodnight']\n",
      "🇳🇿  --> ['armymutual' 'want armymutual' '2000 2003' '2006 2008' '2008 2009']\n",
      "🇵🇪  --> ['country' '𝐌𝐀𝐓𝐂𝐇 computer' 'enjoy' 'elli' 'elli tom']\n",
      "🇵🇭  --> ['philippines' 'thailand' 'thailand philippines' 'philippines united'\n",
      " 'countries']\n",
      "🇵🇰  --> ['country' 'reply' 'like reply' 'followers' 'like']\n",
      "🇵🇱  --> ['poland' 'group' 'nato' 'tom' 'like poland']\n",
      "🇵🇷  --> ['country' 'country reply' 'reply flag' 'flag' 'reply']\n",
      "🇵🇹  --> ['country' 'portugal' 'armymutual' 'want armymutual' 'reply']\n",
      "🇵🇼  --> ['travels 0r47ay58e5' '0r47ay58e5' 'let far' 'far tweet' 'tweet travels']\n",
      "🇵🇾  --> ['country' '𝐌𝐀𝐓𝐂𝐇 computer' 'enjoy' 'elli' 'elli tom']\n",
      "🇷🇴  --> ['country reply' 'reply flag' 'flag' 'reply' 'country']\n",
      "🇷🇺  --> ['russia' 'india' 'tom' 'cost gb' 'gb']\n",
      "🇷🇼  --> ['country reply' 'reply flag' 'flag' 'reply' 'country']\n",
      "🇸🇦  --> ['proud' 'man' 'pakistan' 'freedom' 'nigeria']\n",
      "🇸🇧  --> ['travels 0r47ay58e5' '0r47ay58e5' 'let far' 'far tweet' 'tweet travels']\n",
      "🇸🇪  --> ['2003' 'eurovision' '2015' '1999 2003' '2011']\n",
      "🇸🇬  --> ['singapore' '62' '53' 'norway' 'gdp']\n",
      "🇸🇮  --> ['armymutual' 'want armymutual' 'grand final' 'eurovision grand' 'grand']\n",
      "🇸🇱  --> ['67' 'mli' '67 mli' 'sle' 'les 67']\n",
      "🇸🇳  --> ['sina' 'heyoon sina' 'diarra' 'uniter até' 'joalin']\n",
      "🇸🇴  --> ['67' 'mli' '67 mli' 'sle' 'les 67']\n",
      "🇸🇷  --> ['grand final' 'eurovision grand' 'grand' 'eurovision' 'final']\n",
      "🇸🇸  --> ['travels 0r47ay58e5' '0r47ay58e5' 'let far' 'far tweet' 'tweet travels']\n",
      "🇸🇻  --> ['want armymutual' 'armymutual' 'want' '𝐌𝐀𝐓𝐂𝐇 computer' 'energy drinks']\n",
      "🇸🇾  --> ['country' 'want armymutual' 'armymutual' 'want' '𝐌𝐀𝐓𝐂𝐇 computer']\n",
      "🇹🇩  --> ['67' 'mli' '67 mli' 'sle' 'les 67']\n",
      "🇹🇬  --> ['country' 'reply' 'like reply' 'want armymutual' 'armymutual']\n",
      "🇹🇭  --> ['stream' 'thailand' 'indonesia' 'country reply' 'reply flag']\n",
      "🇹🇱  --> ['country reply' 'reply flag' 'flag' 'reply' 'country']\n",
      "🇹🇳  --> ['grand final' 'eurovision grand' 'grand' 'eurovision' 'final']\n",
      "🇹🇷  --> ['country' 'reply' 'like reply' 'fight' 'connect']\n",
      "🇹🇹  --> ['country' 'reply' 'like reply' 'like' 'connect']\n",
      "🇹🇼  --> ['country' 'open' 'taiwan' 'congratulation' 'china']\n",
      "🇺🇦  --> ['u20wc' 'trophy' 'baku strasbourg' 'strasbourg' 'amman washington']\n",
      "🇺🇬  --> ['let' '0r47ay58e5' 'travels 0r47ay58e5' 'far tweet' 'tweet travels']\n",
      "🇺🇲  --> ['followers' 'followers followers' 'follower want' 'follower' 'want']\n",
      "🇺🇳  --> ['council' 'estonia' 'million' 'join' 'country']\n",
      "🇺🇸  --> ['thank' 'president' 'trump' 'usa' 'america']\n",
      "🇺🇾  --> ['2000 2003' '2006 2008' '2008 2009' '2010 2012' '2012 2013']\n",
      "🇻🇪  --> ['gt' 'seven' 'pop' 'year' 'test']\n",
      "🇻🇬  --> ['fast 100' '200 300' '500 600' '400 500' '100 200']\n",
      "🇻🇳  --> ['country' '00pm' '10 country' 'country tweet' 'kst']\n",
      "🇽🇰  --> ['15 different' 'member 15' 'service member' 'build inte' 'work build']\n",
      "🇿🇦  --> ['country' 'followers' 'follower' 'follower want' 'want']\n",
      "🇿🇼  --> ['follower' '10 20' '20 30' '40 50' '50 60']\n",
      "🌀  --> ['disaster cycl' 'hurricanes wildfire' 'extreme weather' 'extreme' 'flood']\n",
      "🌃  --> ['swimsuit' 'size' 'sale' 'city' 'available']\n",
      "🌅  --> ['warm' 'reposte' 'weather' 'goodnight art' 'art sunset']\n",
      "🌈  --> ['gay' 'day homophobia' 'homophobia' 'international day' 'international']\n",
      "🌉  --> ['pynlw2bltb' 'hype hard' 'july pynlw2bltb' 'idc hype' 'hard 4th']\n",
      "🌊  --> ['wave' 'sea' 'today' '00' 'ocean']\n",
      "🌍  --> ['reply' 'fast reply' 'eu' 'gain' 'like']\n",
      "🌎  --> ['00pm' 'fucking' 'fresh' 'stream' 'fast reply']\n",
      "🌏  --> ['reply' 'fast reply' 'fast' '05' 'major']\n",
      "🌓  --> ['small' 'moon' 'thing' 'emison' 'energy drinks']\n",
      "🌙  --> ['100' 'clear' 'navy' 'sky' 'blue']\n",
      "🌚  --> ['mutual' 'going' 'idk' 'post' 'tho']\n",
      "🌛  --> ['deal' 'seriously' 'tomorrow' 'goodnight' 'chance']\n",
      "🌝  --> ['footbal' 'join footbal' 'time rbmitpin' 'rbmitpin chance' 'rbmitpin']\n",
      "🌞  --> ['good morning' 'morning' 'good' 'sun' 'golden']\n",
      "🌟  --> ['wait' 'acc' 'good' 'thing' 'happy']\n",
      "🌧  --> ['weird' '49' 'warm' 'high' 'really']\n",
      "🌨  --> ['guys' 'hand' 'area' 'base' 'currently']\n",
      "🌫  --> ['forecast' 'day forecast' 'muafgoi4pq' 'forecast muafgoi4pq' 'day']\n",
      "🌭  --> ['yeah' 'right' 'emergency' 'energy' 'end']\n",
      "🌱  --> ['vegan' 'red' 'pretty' 'eat' 'pro']\n",
      "🌲  --> ['ready' 'world' 'live chat' 'chat' 'continue']\n",
      "🌳  --> ['garden' 'area' 'natural' 'love' '𝐌𝐀𝐓𝐂𝐇 computer']\n",
      "🌵  --> ['thank' '𝐌𝐀𝐓𝐂𝐇 computer' 'emison eqqc7reh7m' 'engineer khatputli'\n",
      " 'engineer']\n",
      "🌶  --> ['bone' 'spicy' 'sweet' 'dog' 'sauce']\n",
      "🌸  --> ['result' 'post' 'turn' 'kiss' 'house']\n",
      "🌹  --> ['hello' 'germany' 'important' 'rose' 'pilot']\n",
      "🌺  --> ['cute' 'like' 'elli tom' 'ellis' 'ellis tom']\n",
      "🌻  --> ['reply' 'world reply' 'country like' 'flag country' 'friend world']\n",
      "🌼  --> ['new' 'fresh' 'goal' 'start' 'time']\n",
      "🌽  --> ['netflix vegi' 'tale like' 'vegi tale' 'vegi' 'doofus bitch']\n",
      "🌾  --> ['turkish' 'claim' 'force' 'field' 'set']\n",
      "🌿  --> ['blood' 'super' '25' 'moon' 'gt']\n",
      "🍀  --> ['fan' 'right' 'calm' 'dear' 'wrong']\n",
      "🍁  --> ['beauty' 'beautiful' 'look' 'engineer khatputli' 'elli']\n",
      "🍂  --> ['hard' 'like' 'emison eqqc7reh7m' 'engineer' 'energy drinks']\n",
      "🍃  --> ['music release' 'we_are_superhuman' '127' 'nct 127' 'nct']\n",
      "🍅  --> ['attack' 'tale like' 'vegi tale' 'vegi' 'doofus bitch']\n",
      "🍆  --> ['look head' 'bitch look' 'tale like' 'vegi tale' 'vegi']\n",
      "🍇  --> ['force' 'know' 'want' 'emotional' 'engineer']\n",
      "🍊  --> ['fast reply' 'fast' 'splash' 'reply' 'sea']\n",
      "🍍  --> ['rd' '2113 bandera' 'tx iceicebabysa' 'tx' 'chamoy lover']\n",
      "🍎  --> ['force' 'know' 'apparently' 'apple' 'doctor']\n",
      "🍏  --> ['year' 'picture' 'end' 'wait' '𝐌𝐀𝐓𝐂𝐇 computer']\n",
      "🍑  --> ['time' 'spicy' 'bone' 'giveaway' 'post']\n",
      "🍒  --> ['black' 'natural' 'make' 'lover 2113' '0crszsz16n']\n",
      "🍓  --> ['available' 'yes' 'force' 'love' 'brazil']\n",
      "🍢  --> ['closed' 'way' '𝐌𝐀𝐓𝐂𝐇 computer' 'emison' 'energy drinks']\n",
      "🍪  --> ['teach' 'food' 'vegan' 'live' '𝐌𝐀𝐓𝐂𝐇 computer']\n",
      "🍫  --> ['teach' 'food' 'vegan' 'make sure' 'live']\n",
      "🍯  --> ['honey' 'bear' 'natural' 'feel' 'emotional']\n",
      "🍳  --> ['sauce wow' 'creamy sauce' 'long cookingwithchefk' 'kcstew creamy'\n",
      " 'kcstew']\n",
      "🍷  --> ['national' 'wine' 'happy' 'favorite' 'day']\n",
      "🍸  --> ['seoul' '𝐌𝐀𝐓𝐂𝐇 computer' 'engineer khatputli' 'elland road' 'elli']\n",
      "🍻  --> ['grab' 'try' 'work' 'year' 'wait']\n",
      "🍽  --> ['busy' 'eat' 'medium' 'twitter' 'tonight']\n",
      "🍿  --> ['fb' 'star' 'summer' 'big' 'emotional']\n",
      "🎀  --> ['mind little' 'ijul mind' 'ijul' 'mind' 'little']\n",
      "🎁  --> ['celebration' 'man' 'enjoy' 'year' 'shop']\n",
      "🎂  --> ['birthday' 'lead' 'friend' 'think' 'blessed']\n",
      "🎃  --> ['dress' 'yo' 'year' '𝐌𝐀𝐓𝐂𝐇 computer' 'emison eqqc7reh7m']\n",
      "🎆  --> ['pynlw2bltb' 'hype hard' 'july pynlw2bltb' 'idc hype' 'hard 4th']\n",
      "🎇  --> ['talented' 'happy birthday' 'birthday' 'beautiful' 'happy']\n",
      "🎈  --> ['birthday' 'happy birthday' 'beautiful' 'happy' 'talented']\n",
      "🎉  --> ['birthday' 'happy' 'happy birthday' 'talented' 'tonight']\n",
      "🎊  --> ['offer' 'twice' 'news' 'surprise' 'year']\n",
      "🎓  --> ['graduation' 'service' 'real' 'chase' 'senior bring']\n",
      "🎗  --> ['lovely' 'set' 'free' 'est' 'eric bennett']\n",
      "🎙  --> ['sauce wow' 'creamy sauce' 'long cookingwithchefk' 'kcstew creamy'\n",
      " 'kcstew']\n",
      "🎟  --> ['good' 'want' 'standard' 'leader' 'set']\n",
      "🎢  --> ['elland' 'elland road' 'rollercoaster' 'leeds' 'bielsa']\n",
      "🎤  --> ['channel' 'tonight' 'watch' 'shawn' 'key']\n",
      "🎥  --> ['omg' 'history' 'germany' 'champ' 'year old']\n",
      "🎧  --> ['grab' 'chance win' 'chance' 'gc' 'team']\n",
      "🎨  --> ['important' 'rose' 'rise' 'make' 'time']\n",
      "🎬  --> ['fast reply' 'fast' 'reply' 'elland road' 'elli']\n",
      "🎯  --> ['website' 'bitcoin' 'hey like' 'reply hey' 'turn']\n",
      "🎵  --> ['bear' '1995' 'kcstew creamy' 'cookingwithchefk kcstew'\n",
      " 'long cookingwithchefk']\n",
      "🎶  --> ['2019' 'check' 'mr' 'mike' 'stage']\n",
      "🎸  --> ['sauce wow' 'creamy sauce' 'long cookingwithchefk' 'kcstew creamy'\n",
      " 'kcstew']\n",
      "🎹  --> ['shawn' 'key' 'lord' 'blessing' 'play']\n",
      "🎻  --> ['mr' 'beautiful' 'help' 'emison eqqc7reh7m' 'engineer']\n",
      "🎼  --> ['open' 'bear' '1995' 'creamy sauce' 'long cookingwithchefk']\n",
      "🎾  --> ['set' 'engineer khatputli' 'elland' 'elland road' 'elli']\n",
      "🏀  --> ['59' 'championship' 'mid' '73' 'global']\n",
      "🏁  --> ['straight' 'good morning' 'country reply' 'reply flag' 'et']\n",
      "🏃  --> ['catch' 'funny' 'sorry' 'ass' 'person']\n",
      "🏆  --> ['win' 'trophy' 'run' 'bts' 'music']\n",
      "🏇  --> ['park' 'di' 'le' 'et' '2019']\n",
      "🏈  --> ['football' 'officially' 'away' '100' 'start']\n",
      "🏊  --> ['teach' 'school' 'child' 'little' 'happy']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🏌  --> ['learn' 'mind' 'open' 'thing' 'emison eqqc7reh7m']\n",
      "🏎  --> ['attention' 'woah' 'use' 'real' 'race day']\n",
      "🏏  --> ['national' 'indvaus' 'woman' 'team' 'unbelievable']\n",
      "🏐  --> ['intensity' 'talk intensity' 'bepartofthegame' 'intensity portugal'\n",
      " 'ko2zm2ahyq bepartofthegame']\n",
      "🏑  --> ['𝐌𝐀𝐓𝐂𝐇 computer' 'elland' 'elli' 'elli tom' 'ellis']\n",
      "🏒  --> ['sweep' 'finals' 'stanleycup' 'head' 'let']\n",
      "🏝  --> ['disaster cycl' 'hurricanes wildfire' 'extreme weather' 'extreme' 'flood']\n",
      "🏞  --> ['ready' 'world' '𝐌𝐀𝐓𝐂𝐇 computer' 'emison eqqc7reh7m' 'engineer']\n",
      "🏟  --> ['26' 'la' 'memory' 'rose' 'throw']\n",
      "🏥  --> ['al' 'fight' 'seven' 'continue' 'announce']\n",
      "🏫  --> ['later' 'month' 'sorry' 'class' 'late']\n",
      "🏳  --> ['flag' 'amazing' 'country reply' 'reply flag' 'right']\n",
      "🏴  --> ['country' 'best' 'country reply' 'reply flag' 'el']\n",
      "🏹  --> ['team usa' 'champion' 'team' 'usa' 'world']\n",
      "🐇  --> ['bed' 'late' 'come' 'day' 'want']\n",
      "🐋  --> ['ocean' 'earth' 'today' 'leviathan' 'track monster']\n",
      "🐎  --> ['black' '63' 'july' 'party' 'summer']\n",
      "🐐  --> ['beat' 'make' 'good' '𝐌𝐀𝐓𝐂𝐇 computer' 'emison eqqc7reh7m']\n",
      "🐑  --> ['german' 'dog' 'leeds' 'bielsa' 'derby']\n",
      "🐒  --> ['watch' 'balance' 'design' 'honor' 'new']\n",
      "🐔  --> ['americans' 'proud' 'kick' 'great' 'emotional']\n",
      "🐘  --> ['suggest' 'piece' 'justice' 'finally' '000']\n",
      "🐙  --> ['ocean' 'suffer' 'pain' 'leave' 'feel']\n",
      "🐜  --> ['formiga appear' 'world record' 'appear 1995' 'record formiga' 'appear']\n",
      "🐝  --> ['pic' 'safe' 'amazing' 'time' 'engineer khatputli']\n",
      "🐟  --> ['ocean' 'pain' 'suffer' 'leave' 'feel']\n",
      "🐢  --> ['count' 'spring' 'year' 'ocean' 'suffer']\n",
      "🐦  --> ['air nowplaying' 'darkfather' 'live air' 'darkfather live' 'nowplaying']\n",
      "🐭  --> ['car' 'guy' 'emergency' 'energy drinks' 'energy']\n",
      "🐯  --> ['followers' 'like' 'promotion' 'busy' 'tomorrow']\n",
      "🐰  --> ['wth' 'bunny' 'tear' 'agree' 'vision']\n",
      "🐱  --> ['cat' 'pet' 'guess' 'brexit' 'tv']\n",
      "🐲  --> ['scene' 'favourite' 'late' 'emergency' 'energy drinks']\n",
      "🐴  --> ['raise' 'bar' 'love' 'emotional' 'engineer']\n",
      "🐶  --> ['dog' 'love' 'pet' 'guess' 'walk']\n",
      "🐸  --> ['president' '𝐌𝐀𝐓𝐂𝐇 computer' 'emison eqqc7reh7m' 'engineer'\n",
      " 'energy drinks']\n",
      "🐹  --> ['cheer' 'cheer kit' 'bts help' 'usa bts' 'jin']\n",
      "🐺  --> ['followers' 'weird' 'follower' 'bc' '50k 100k']\n",
      "🐻  --> ['warm' 'reposte' 'weather' 'honey' 'bear']\n",
      "🐾  --> ['good' 'thing' 'walk' 'good morning' 'morning']\n",
      "🐿  --> ['movie' 'ah' 'favorite' 'make' '𝐌𝐀𝐓𝐂𝐇 computer']\n",
      "👀  --> ['people' 'like' 'come' 'project' 'men']\n",
      "👁  --> ['question' '𝐌𝐀𝐓𝐂𝐇 computer' 'engineer khatputli' 'elland road' 'elli']\n",
      "👅  --> ['know' 'daddy' 'want' 'young' 'like']\n",
      "👆  --> ['look' 'mean' 'place' '2018' 'look like']\n",
      "👇  --> ['year' 'help' 'climate' 'friend' 'new']\n",
      "👈  --> ['gain' 'apparently' 'like' 'apple' 'doctor']\n",
      "👉  --> ['news' 'need' 'need help' 'play' 'apple']\n",
      "👊  --> ['watch' 'good' 'use hashtag' 'hashtag' 'kag_camp']\n",
      "👋  --> ['read' 'sunday' 'decision' 'woman right' 'away']\n",
      "👌  --> ['love' 'new' 'excite' 'night' 'thread']\n",
      "👍  --> ['good' 'trump' 'nice' 'great' 'enjoy']\n",
      "👏  --> ['great' 'woman' 'come' 'right' 'maybe']\n",
      "👐  --> ['hard' 'know' 'mind little' 'ijul mind' 'ijul']\n",
      "👑  --> ['use hashtag' 'hashtag' 'use' 'el' 'su']\n",
      "👙  --> ['guys' 'swimsuit' 'hand' 'patriotic' 'need']\n",
      "👧  --> ['op' 'line' 'read' 'later' 'suppose']\n",
      "👨  --> ['age' 'abortion' 'letter' 'hell' 'ass']\n",
      "👩  --> ['graduation' 'sexual' 'explain' 'pre' 'exam']\n",
      "👬  --> ['transphobia' 'away' 'bad' 'story' 'news']\n",
      "👭  --> ['transphobia' 'away' 'bad' 'story' 'news']\n",
      "👮  --> ['beautiful' 'ell' 'elland road' 'elli' 'elli tom']\n",
      "👯  --> ['winner' 'guy' 'watch' 'ellis tom' 'end']\n",
      "👶  --> ['baby' 'solo' 'album' 'excited' 'wth']\n",
      "👷  --> ['maga' 'student' 'partne' 'big' 'help']\n",
      "👺  --> ['peep oam4rsks0d' 'peep' 'oam4rsks0d' '𝐌𝐀𝐓𝐂𝐇 computer'\n",
      " 'emison eqqc7reh7m']\n",
      "👻  --> ['peep oam4rsks0d' 'peep' 'oam4rsks0d' '𝐌𝐀𝐓𝐂𝐇 computer'\n",
      " 'emison eqqc7reh7m']\n",
      "👼  --> ['meadow fairy' 'princess fall' 'love meadow' 'fairytale'\n",
      " 'nctzenselcaday przwv8jdxg']\n",
      "👽  --> ['inside' 'seriously' 'save' 'james' '𝐌𝐀𝐓𝐂𝐇 computer']\n",
      "💀  --> ['mutual' 'sis' 'ahead' 'tired' 'going']\n",
      "💁  --> ['friend' 'slam' 'point' 'lose' 'speak']\n",
      "💃  --> ['love' 'minute' 'power' 'friday' 'home']\n",
      "💅  --> ['need closer' 'caution dishonest' 'proceed' 'proceed caution' 'love hard']\n",
      "💆  --> ['selfie' 'post' 'check' 'fully' 'lie']\n",
      "💈  --> ['wait' '𝐌𝐀𝐓𝐂𝐇 computer' 'emotional' 'engineer khatputli' 'engineer']\n",
      "💋  --> ['proceed caution' 'dishonest' 'proceed' 'caution dishonest' 'caution']\n",
      "💌  --> ['letter' 'chat' 'peep oam4rsks0d' 'oam4rsks0d' 'peep']\n",
      "💍  --> ['spend' 'turn' 'wanna' 'lot' 'right']\n",
      "💎  --> ['kai' 'thing' 'emison eqqc7reh7m' 'engineer khatputli' 'engineer']\n",
      "💐  --> ['bloom' 'realize' 'mind little' 'ijul mind' 'ijul']\n",
      "💓  --> ['wow' 'beautiful' 'engineer khatputli' 'elland road' 'elli']\n",
      "💔  --> ['hurt' 'heart' 'shoulder' 'boy' 'break']\n",
      "💕  --> ['need' 'day' 'cute' 'time' 'chart']\n",
      "💖  --> ['wait' 'cute' 'story' 'work' 'love']\n",
      "💗  --> ['beautiful' 'honey' 'link' 'forever' 'love']\n",
      "💘  --> ['real' 'place' 'thank' 'way' 'di']\n",
      "💙  --> ['let' 'happy' 'thank' 'look' 'stanleycup']\n",
      "💚  --> ['love' 'world' 'send' 'hair' 'tell']\n",
      "💛  --> ['congratulation' 'photo' 'play' 'eye' 'light']\n",
      "💜  --> ['btsongma' 'army' 'btsonlssc' 'morning' 'love']\n",
      "💞  --> ['hope' 'thank' 'good night' 'like' 'lil']\n",
      "💣  --> ['attention' 'game' '𝐌𝐀𝐓𝐂𝐇 computer' 'emotional' 'engineer']\n",
      "💥  --> ['day' 'wwg1wga' 'dc' 'enjoy' 'new']\n",
      "💦  --> ['hate' 'tag friend' 'tag' 'corner' 'room']\n",
      "💨  --> ['ahead' 'followers' 'style' 'june' 'finish']\n",
      "💩  --> ['stop' 'turn' 'kid' 'bring' 'world']\n",
      "💪  --> ['thank' 'great' 'success' 'excite' 'league']\n",
      "💫  --> ['level' 'inside' 'james' 'today' 'soul']\n",
      "💬  --> ['challenge' 'face' 'swimsuit' 'patriotic' 'reply']\n",
      "💯  --> ['yes' 'need' 'agree' 'amen' 'approve']\n",
      "💰  --> ['50' 'refuse' 'f5' 'di' 'forget']\n",
      "💲  --> ['tech' 'million' 'award' 'hi' 'win']\n",
      "💸  --> ['75' 'toronto' '25' 'meet' 'win 200']\n",
      "💻  --> ['proud' '40' 'team' 'final' 'live']\n",
      "💼  --> ['women' 'german' 'meet' 'woman' '𝐌𝐀𝐓𝐂𝐇 computer']\n",
      "💿  --> ['got7' 'brand' 'sale' 'ep' 'best']\n",
      "📀  --> ['glass' 'netflix vegi' 'tale like' 'doofus bitch' 'doofus']\n",
      "📄  --> ['tour' 'actually' 'american' 'work' 'today']\n",
      "📆  --> ['corner' 'football' 'ready' 'right' '2019']\n",
      "📈  --> ['stock' 'lucky' 'shoot' 'spring' 'feel']\n",
      "📊  --> ['40' 'final' 'day' 'emotional' 'engineer']\n",
      "📋  --> ['exo' 'premiosmtvmiaw' '𝐌𝐀𝐓𝐂𝐇 computer' 'emison eqqc7reh7m' 'engineer']\n",
      "📍  --> ['south korea' 'south' 'korea' 'emison' 'energy drinks']\n",
      "📖  --> ['copy' 'book' 'available' 'sell' 'shawn']\n",
      "📚  --> ['2019' 'korean' 'author' 'gc' 'seoul']\n",
      "📜  --> ['hall' '𝐌𝐀𝐓𝐂𝐇 computer' 'engineer khatputli' 'elland road' 'elli']\n",
      "📝  --> ['tag friend' 'sign' 'giveaway' 'tag' '20']\n",
      "📡  --> ['air nowplaying' 'live air' 'darkfather' 'darkfather live' 'nowplaying']\n",
      "📣  --> ['excite' 'announce' 'special' '2020' 'live']\n",
      "📦  --> ['honeycomb' 'matrix honeycomb' 'matrix' 'yiasxaanw4' 'yiasxaanw4 free']\n",
      "📬  --> ['pregnant' 'male' 'politician' 'post' '𝐌𝐀𝐓𝐂𝐇 computer']\n",
      "📱  --> ['afternoon' 'stadium' 'match' 'action' 'head']\n",
      "📲  --> ['minute' 'power' 'friday' 'home' 'ready']\n",
      "📷  --> ['photo' 'new' '𝐌𝐀𝐓𝐂𝐇 computer' 'emison' 'energy drinks']\n",
      "📸  --> ['fast reply' 'photo' 'nice' 'fast' 'girl']\n",
      "📺  --> ['park' 'di' 'le' 'et' '2019']\n",
      "📼  --> ['u20wc' 'semi' 'reach' 'stage' 'energy']\n",
      "📽  --> ['info' 'social' 'support' 'free' 'leave']\n",
      "🔁  --> ['try' 'way' 'start' 'tag friend' 'sign']\n",
      "🔄  --> ['challenge' 'complete' 'tag' '500' 'reply']\n",
      "🔊  --> ['captain' 'chris' 'middle' 'case' 'goal']\n",
      "🔎  --> ['july' 'summer' 'come' '𝐌𝐀𝐓𝐂𝐇 computer' 'end']\n",
      "🔑  --> ['plan' 'climate' '𝐌𝐀𝐓𝐂𝐇 computer' 'emotional' 'engineer']\n",
      "🔒  --> ['promise' 'birth' 'care' 'ass' 'voting']\n",
      "🔔  --> ['proud' 'like' 'scene' 'favourite' 'late']\n",
      "🔗  --> ['14' 'schedule' 'official' 'home' 'vs']\n",
      "🔘  --> ['relationship' 'single' 'lose' 'emison eqqc7reh7m' 'energy drinks']\n",
      "🔙  --> ['𝐌𝐀𝐓𝐂𝐇 computer' 'elland' 'elli' 'elli tom' 'ellis']\n",
      "🔜  --> ['return' 'home' 'time' 'engineer khatputli' 'engineer']\n",
      "🔞  --> ['honeycomb' 'matrix honeycomb' 'matrix' 'yiasxaanw4' 'yiasxaanw4 free']\n",
      "🔥  --> ['special' 'new' 'today' 'say' 'love']\n",
      "🔪  --> ['pretty' '𝐌𝐀𝐓𝐂𝐇 computer' 'engineer khatputli' 'elland road' 'elli']\n",
      "🔫  --> ['brain' 'kill' 'right' 'emergency' 'energy']\n",
      "🔬  --> ['arrive' 'brain' 'sound' 'safe' 'beautiful']\n",
      "🔮  --> ['level' 'enjoy' 'elland road' 'elli' 'elli tom']\n",
      "🔴  --> ['sky' 'weekend' 'tune' 'champion' 'create']\n",
      "🔵  --> ['sky' 'reveal' 'champion' 'saturday' 'watch']\n",
      "🔶  --> ['journey' 'spread' 'join' 'news' 'fast']\n",
      "🔷  --> ['𝐌𝐀𝐓𝐂𝐇 computer' '𝐌𝐀𝐓𝐂𝐇' 'news live' 'live الن' 'egnr0em5dc mobi']\n",
      "🔸  --> ['1kfollower' '3kfollower' 'want 1kfollower' '2kfollower 3kfollower'\n",
      " '2kfollower']\n",
      "🔹  --> ['1kfollower' '3kfollower' 'want 1kfollower' '2kfollower 3kfollower'\n",
      " '2kfollower']\n",
      "🔻  --> ['artist' 'international' '1st' 'bring' 'support']\n",
      "🕊  --> ['blessing' 'double' 'kill' 'trump' 'happy']\n",
      "🕋  --> ['ah' 'ah ah' 'bruvvvvv' 'waviest' 'london waviest']\n",
      "🕙  --> ['𝐌𝐀𝐓𝐂𝐇 computer' 'elland' 'elli' 'elli tom' 'ellis']\n",
      "🕶  --> ['person probably' 'gm hey' 'probably drive' 'morning person'\n",
      " 'people crazy']\n",
      "🕺  --> ['dance' 'deserve' 'literally' 'happilyeverafter' 'gaon']\n",
      "🖋  --> ['hall' '𝐌𝐀𝐓𝐂𝐇 computer' 'engineer khatputli' 'elland road' 'elli']\n",
      "🖐  --> ['5th' 'year old' 'eve' 'princess' 'old']\n",
      "🖕  --> ['respect' 'fan' 'information' 'suicide' 'create']\n",
      "🖤  --> ['come' 'date' 'decision' 'maybe' 'trust']\n",
      "🖼  --> ['available' 'high' 'great' '𝐌𝐀𝐓𝐂𝐇 computer' 'emison']\n",
      "🗑  --> ['man' '𝐌𝐀𝐓𝐂𝐇 computer' 'emotional' 'engineer khatputli' 'engineer']\n",
      "🗓  --> ['vs' '00' 'cloud9' 'ghost' 'eunited']\n",
      "🗝  --> ['inside' 'year' 'know' '𝐌𝐀𝐓𝐂𝐇 computer' 'engineer khatputli']\n",
      "🗣  --> ['talk' 'real' 'kid' 'air' 'best']\n",
      "🗳  --> ['voting' 'pls' 'want' 'tackle climate' 'thursday']\n",
      "🗽  --> ['york' 'new york' 'heart' 'high' 'look']\n",
      "😀  --> ['board' 'case' 'beautiful' 'usa' 'check']\n",
      "😁  --> ['love' 'stream' 'na' 'class' 'excited']\n",
      "😂  --> ['like' 'man' 'good' 'know' 'make']\n",
      "😃  --> ['protect' 'right' 'band' 'wake' 'ep']\n",
      "😄  --> ['bloom' 'finally' 'mtvbrfandombtsarmy' 'voting' 'hahaha']\n",
      "😅  --> ['new' 'rest' 'like' 'thing' 'wrong']\n",
      "😆  --> ['reposte' 'oh wait' 'end' 'oh' 'tired']\n",
      "😇  --> ['trump2020' 'promise' 'happy' 'trump' 'announce']\n",
      "😈  --> ['use' 'ride' 'barr' 'vegan' 'actually']\n",
      "😉  --> ['party' 'moon' 'shout' 'time' 'case']\n",
      "😊  --> ['thank' 'say' 'game' 'info' 'ya']\n",
      "😋  --> ['vegan' 'big' 'later' 'sauce' 'stuff']\n",
      "😌  --> ['mind' 'pop' 'easy' 'talk' 'thank']\n",
      "😍  --> ['love' 'omg' 'look' 'cute' 'want']\n",
      "😎  --> ['class' 'time' 'boss' 'epic' 'complete']\n",
      "😏  --> ['content' 'action' 'real' 'let' 'town']\n",
      "😑  --> ['prayer' 'deserve' 'abortion' 'leave' 'germany']\n",
      "😒  --> ['man' 'attention' 'spend' 'lie' 'kid']\n",
      "😓  --> ['bad' 'child' 'head' '𝐌𝐀𝐓𝐂𝐇 computer' 'emotional']\n",
      "😔  --> ['hope' 'know' 'ask' 'hoe' 'chris']\n",
      "😕  --> ['hate' 'hour' 'weekend' 'house' '12']\n",
      "😘  --> ['adnourhappyplace' 'love' 'great' 'time' 'thank']\n",
      "😙  --> ['place' 'way' '𝐌𝐀𝐓𝐂𝐇 computer' 'engineer' 'elland road']\n",
      "😛  --> ['yea' 'energy' 'big' 'hit' 'thread']\n",
      "😜  --> ['masnada ride' 'ride eh' 'safe finish' 'know masnada' 'come safe']\n",
      "😝  --> ['girl' 'come' 'beautiful' '2020' 'ok']\n",
      "😟  --> ['friend' 'sex' 'ok' 'check' 'member friend']\n",
      "😠  --> ['trump' 'repeat' '𝐌𝐀𝐓𝐂𝐇 computer' 'emison' 'energy drinks']\n",
      "😡  --> ['exactly' 'trump' 'make' 'poll' 'fun']\n",
      "😢  --> ['vegan' 'hope' 'lose' 'ass' 'help']\n",
      "😣  --> ['abortion' 'tonight' 'beauty' 'friend' 'sky']\n",
      "😤  --> ['really' 'shit' 'miss' 'rest' 'deserve']\n",
      "😥  --> ['area' 'question' 'hell' 'different' 'bring']\n",
      "😨  --> ['area' 'question' 'hell' 'different' 'bring']\n",
      "😩  --> ['wanna' 'feeling' 'straight' 'think' 'fuck']\n",
      "😪  --> ['matter' 'word' 'shout' 'jungkook' 'protect']\n",
      "😫  --> ['sexual' 'energy' 'hurt' 'yea' 'oh']\n",
      "😬  --> ['story' 'poll' 'fave' 'twitter' 'golden']\n",
      "😭  --> ['cute' 'look' 'love' 'like' 'man']\n",
      "😮  --> ['station' 'follower' 'kiss' '19' '20k']\n",
      "😯  --> ['zcswvl8x9g' 'epic zcswvl8x9g' 'totally epic' 'okay totally' 'totally']\n",
      "😰  --> ['size' 'car' 'person' 'head' 'area']\n",
      "😱  --> ['season' 'need' 'important' 'real' 'kid']\n",
      "😲  --> ['high' 'police' 'baekhyun' 'surprise' 'dress']\n",
      "😳  --> ['use' 'wait' 'look' 'true' 'black']\n",
      "😴  --> ['till' 'lot' 'wait' 'saturday' 'wonderful']\n",
      "😵  --> ['doofus bitch' 'tale like' 'netflix vegi' 'doofus' 'fuck watch']\n",
      "😷  --> ['𝐌𝐀𝐓𝐂𝐇 computer' 'elland' 'elli' 'elli tom' 'ellis']\n",
      "😸  --> ['welcome' 'enter' 'elli' 'elli tom' 'ellis']\n",
      "😹  --> ['mutual' 'kai' 'drink' 'book' 'till']\n",
      "😻  --> ['marvelous' 'marvelous xoxoxo' 'xoxoxo' 'xoxoxo adore' 'adore']\n",
      "😿  --> ['𝐌𝐀𝐓𝐂𝐇 computer' 'elland' 'elli' 'elli tom' 'ellis']\n",
      "🙀  --> ['true' 'omg' '𝐌𝐀𝐓𝐂𝐇 computer' 'emison eqqc7reh7m' 'engineer']\n",
      "🙁  --> ['car' 'nice' 'okay' 'house' 'gift buy']\n",
      "🙂  --> ['stop' 'shot' 'day' 'coffee' 'lol']\n",
      "🙃  --> ['past' 'hell' 'small' 'morning' 'attack']\n",
      "🙄  --> ['yeah' 'good' 'twice' 'know' 'watch']\n",
      "🙅  --> ['wish' 'stay' 'person' 'break' 'talk']\n",
      "🙆  --> ['black' 'use' 'capture' 'absolutely' 'reason']\n",
      "🙈  --> ['cute' 'word' 'night' 'turn' 'wait']\n",
      "🙉  --> ['dear' 'word' 'article' 'confused' 'ask']\n",
      "🙊  --> ['good' 'like' 'right' 'wait' 'dear']\n",
      "🙋  --> ['raise' 'funny' 'actually' 'hand' 'good morning']\n",
      "🙌  --> ['tonight' 'today' 'day' 'fall' 'scene']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🙏  --> ['thank' 'soul' 'family' 'great' 'country']\n",
      "🚀  --> ['inside' 'james' 'future' 'meet' 'make']\n",
      "🚂  --> ['board' 'usa' 'john' 'james' 'support']\n",
      "🚑  --> ['fb' '𝐌𝐀𝐓𝐂𝐇 computer' 'emison eqqc7reh7m' 'engineer khatputli' 'engineer']\n",
      "🚒  --> ['age' 'oh wait' 'oh' 'wait' '15']\n",
      "🚓  --> ['darrell' 'want darrell' 'want' '𝐌𝐀𝐓𝐂𝐇 computer' 'emison']\n",
      "🚔  --> ['darrell' 'want darrell' 'want' '𝐌𝐀𝐓𝐂𝐇 computer' 'emison']\n",
      "🚗  --> ['receive' 'al' 'drive' 'line' 'state']\n",
      "🚜  --> ['america' 'great' '𝐌𝐀𝐓𝐂𝐇 computer' 'emison eqqc7reh7m' 'energy drinks']\n",
      "🚨  --> ['help' 'great' 'tow' 'emergency' 'reach']\n",
      "🚩  --> ['follower' '10 20' '20 30' '40 50' '50 60']\n",
      "🚫  --> ['sex' 'support' 'thing' 'right' 'time']\n",
      "🚮  --> ['guess' 'hand' 'news' 'good' 'emergency']\n",
      "🚱  --> ['disaster cycl' 'hurricanes wildfire' 'extreme weather' 'extreme' 'flood']\n",
      "🚲  --> ['senior' '𝐌𝐀𝐓𝐂𝐇 computer' 'engineer khatputli' 'elland road' 'elli']\n",
      "🚶  --> ['walk' 'man' '𝐌𝐀𝐓𝐂𝐇 computer' 'emotional' 'engineer']\n",
      "🚾  --> ['mf' 'album' 'ready' '𝐌𝐀𝐓𝐂𝐇 computer' 'emotional']\n",
      "🛁  --> ['dog' 'pet' 'care' '𝐌𝐀𝐓𝐂𝐇 computer' 'emison eqqc7reh7m']\n",
      "🛌  --> ['bed' 'feel like' 'miss' 'news' 'big']\n",
      "🛑  --> ['news' 'return' 'release' 'china' 'emison']\n",
      "🛒  --> ['pic' 'dream' 'engineer khatputli' 'elland road' 'elli']\n",
      "🛢  --> ['believe cow' 'medium like' 'cause climate' 'easytarget' 'people believe']\n",
      "🛫  --> ['return' 'home' 'time' 'engineer khatputli' 'engineer']\n",
      "🛬  --> ['𝐌𝐀𝐓𝐂𝐇 computer' 'elland' 'elli' 'elli tom' 'ellis']\n",
      "🛰  --> ['man' 'today' 'emison eqqc7reh7m' 'engineer' 'energy drinks']\n",
      "🛳  --> ['crew' 'base' 'new' 'emison eqqc7reh7m' 'engineer']\n",
      "🛴  --> ['german' 'hit' 'house' 'emergency' 'energy']\n",
      "🛸  --> ['best' '𝐌𝐀𝐓𝐂𝐇 computer' 'engineer khatputli' 'elland road' 'elli']\n",
      "🤐  --> ['special' '300k' 'appear' 'small' 'explain']\n",
      "🤑  --> ['dump' 'short' 'money' 'price' 'long']\n",
      "🤒  --> ['bitch' 'reposte' 'nervous' 'reason' 'abortion']\n",
      "🤓  --> ['final' 'day' 'human' 'peep' 'peep oam4rsks0d']\n",
      "🤔  --> ['mike' 'tell' 'think' 'event' 'america']\n",
      "🤖  --> ['answer' 'challenge' 'young' 'look' 'new']\n",
      "🤗  --> ['new' 'sauce' 'sorry' 'surprise' 'ass']\n",
      "🤘  --> ['actually' 'live' 'hand' 'omg' 'beach']\n",
      "🤙  --> ['game' '𝐌𝐀𝐓𝐂𝐇 computer' 'elland' 'elli' 'elli tom']\n",
      "🤚  --> ['past' 'tho' 'comment hey' 'let connect' 'connect']\n",
      "🤛  --> ['pop' 'episode' 'song' 'video' 'favourite']\n",
      "🤜  --> ['pop' 'episode' 'song' 'video' 'favourite']\n",
      "🤝  --> ['abortion' 'kai' 'men' 'trial' 'stage']\n",
      "🤟  --> ['amazing moment' 'moment earth' 'eve ya1jjhuzsv' 'guy eve' 'earth thank']\n",
      "🤠  --> ['currently' 'germany' 'town' 'road' 'old']\n",
      "🤡  --> ['say' 'course' 'male' 'straight' 'ill']\n",
      "🤢  --> ['think' '𝐌𝐀𝐓𝐂𝐇 computer' 'emison eqqc7reh7m' 'engineer khatputli'\n",
      " 'engineer']\n",
      "🤣  --> ['say' 'beat' 'easy' 'like' 'right']\n",
      "🤤  --> ['red' 'sis' 'try' 'make perfect' 'gift buy']\n",
      "🤥  --> ['hi' 'science' 'reply' 'peep oam4rsks0d' 'peep']\n",
      "🤦  --> ['vegan' 'smoke' 'start' 'cat' 'say']\n",
      "🤧  --> ['really' 'anniversary' 'happy' 'today' 'day']\n",
      "🤨  --> ['work' 'wish vote' 'vote time' 'wish' 'vote']\n",
      "🤩  --> ['start' 'excited' 'euro' 'brother' 'night']\n",
      "🤪  --> ['like' 'like tweet' 'tweet' 'good' 'cast']\n",
      "🤫  --> ['green' 'think' 'period' 'going' 'exactly']\n",
      "🤬  --> ['science' 'respect' 'climate' 'tell' 'woman']\n",
      "🤭  --> ['song' 'attack' 'tho' 'mysterious' 'react']\n",
      "🤮  --> ['sick' 'police' 'dress' 'hit' 'emison eqqc7reh7m']\n",
      "🤯  --> ['shit' 'watch' 'say' 'like' 'game']\n",
      "🤰  --> ['pregnant' 'expect' 'soon' 'baby' 'woman']\n",
      "🤳  --> ['selfie' 'better' 'attention' 'cost' 'cat']\n",
      "🤷  --> ['time' 'second' 'size' 'lucky' 'good']\n",
      "🤸  --> ['party' 'house' 'engineer khatputli' 'elland road' 'elli']\n",
      "🤼  --> ['tv episode' 'mjf' 'tonight feature' 'episode tonight' 'feature vs']\n",
      "🥀  --> ['garden' 'rose' 'dream' 'set' '𝐌𝐀𝐓𝐂𝐇 computer']\n",
      "🥂  --> ['best' '𝐌𝐀𝐓𝐂𝐇 computer' 'engineer khatputli' 'elland road' 'elli']\n",
      "🥃  --> ['ready' 'champ' 'question' 'point' 'comment']\n",
      "🥇  --> ['pm et' 'gold' 'pm' 'catch' 'et']\n",
      "🥈  --> ['bennett' 'eric bennett' 'recurve' 'eric' 'open']\n",
      "🥉  --> ['medal' 'win' 'honor' 'event' 'world']\n",
      "🥊  --> ['website' 'bitcoin' 'new' 'emison' 'energy drinks']\n",
      "🥒  --> ['rd' '2113 bandera' 'tx iceicebabysa' 'tx' 'chamoy lover']\n",
      "🥓  --> ['true' 'time' '𝐌𝐀𝐓𝐂𝐇 computer' 'emison eqqc7reh7m' 'engineer']\n",
      "🥚  --> ['second' 'season' 'win' 'time' 'emison']\n",
      "🥢  --> ['form' 'hear' 'know' '𝐌𝐀𝐓𝐂𝐇 computer' 'enter']\n",
      "🥭  --> ['rd' '2113 bandera' 'tx iceicebabysa' 'tx' 'chamoy lover']\n",
      "🥰  --> ['dress' 'success' 'end' 'hell' 'face']\n",
      "🥳  --> ['today' 'vs' 'round' '14' '10k']\n",
      "🥴  --> ['joke' 'pregnant' 'cool' 'ass' 'mother']\n",
      "🥵  --> ['hate' 'energy' 'big' 'thank' 'dump']\n",
      "🥺  --> ['good' 'love' 'cute' 'think' 'bts']\n",
      "🦁  --> ['baby' 'trust' 'god' 'support' 'team']\n",
      "🦄  --> ['card' 'open' 'esta noche' 'enjoy' 'elli']\n",
      "🦅  --> ['barr' 'school' 'lovely' 'set' 'free']\n",
      "🦆  --> ['10' 'followback' 'train' 'tag' 'friend']\n",
      "🦋  --> ['wwg1wga' 'thank' 'love meadow' 'fairy' 'nctzenselcaday przwv8jdxg']\n",
      "🦑  --> ['leviathan' 'ep rise' 'unleash track' 'unleash' 'monster']\n",
      "🦓  --> ['𝐌𝐀𝐓𝐂𝐇 computer' 'elland' 'elli' 'elli tom' 'ellis']\n",
      "🦕  --> ['leviathan' 'ep rise' 'unleash track' 'unleash' 'monster']\n",
      "🦷  --> ['education' 'fully' 'action' 'climate change' 'free']\n",
      "🧂  --> ['office' 'ass' '𝐌𝐀𝐓𝐂𝐇 computer' 'emotional' 'engineer khatputli']\n",
      "🧐  --> ['poll' 'science' 'climate' 'tell' 'kind']\n",
      "🧘  --> ['fully' 'lie' 'dream' 'really' 'fuck']\n",
      "🧙  --> ['best' '𝐌𝐀𝐓𝐂𝐇 computer' 'engineer khatputli' 'elland road' 'elli']\n",
      "🧚  --> ['meadow fairy' 'princess fall' 'love meadow' 'fairytale'\n",
      " 'nctzenselcaday przwv8jdxg']\n",
      "🧞  --> ['best' '𝐌𝐀𝐓𝐂𝐇 computer' 'engineer khatputli' 'elland road' 'elli']\n",
      "🧡  --> ['child' 'action' 'teach' 'make sure' 'school']\n",
      "🧨  --> ['pynlw2bltb' 'hype hard' 'july pynlw2bltb' 'idc hype' 'hard 4th']\n",
      "🧲  --> ['best' '𝐌𝐀𝐓𝐂𝐇 computer' 'engineer khatputli' 'elland road' 'elli']\n",
      "🧹  --> ['count' 'spring' 'finals' 'sweep' 'stanleycup']\n",
      "🧺  --> ['brexit' 'tv' 'cat' 'channel' 'emotional']\n",
      "🧿  --> ['ah' 'ah ah' 'bruvvvvv' 'waviest' 'london waviest']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "class Emoji(object):\n",
    "\n",
    "    def __init__(self):\n",
    "        # fit the Naive Bayes\n",
    "        np.random.seed(42)\n",
    "        self.emojis = pd.read_pickle('./data/df_emoji.pkl')\n",
    "\n",
    "    def fit(self):\n",
    "        '''\n",
    "        # ------- this part needs work\n",
    "        try:\n",
    "            self.labeled_tweets = pd.read_pickle('../data/labeled.pkl')\n",
    "            print('it worked')\n",
    "        except:\n",
    "            from label_tweets import label_tweets\n",
    "            tweets = np.array(list(pickle.load(open('./data/yay_moji.pkl','rb'))))\n",
    "            self.by_emoji,self.labeled_tweets = label_tweets(tweets,self.emojis,top = 50, save = True)\n",
    "        '''\n",
    "        self.y = tweets_merged['emoji']\n",
    "        self.X = tweets_merged['tweets'].values\n",
    "\n",
    "\n",
    "    def model(self, max_df_ = .8, min_df_ = .001, ngram = (1,2)):\n",
    "\n",
    "        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(self.X,self.y)\n",
    "\n",
    "        stopwords = set(list(ENGLISH_STOP_WORDS) + ['rt', 'follow', 'dm', 'https', 'ur', 'll' ,'amp', 'subscribe', 'don', 've', 'retweet', 'im', 'http','lt'])\n",
    "\n",
    "        # fit the tfidf or CountVectorizer\n",
    "        self.tfidf = TfidfVectorizer(max_features=10000, max_df = max_df_, min_df=min_df_, stop_words = stopwords, ngram_range = ngram)\n",
    "\n",
    "        self.tfidf.fit(self.X_train)\n",
    "        self.vector = self.tfidf.transform(self.X_train)\n",
    "\n",
    "        # --> add the emoji name to bag of words for each emoji\n",
    "        self.bag = np.array(self.tfidf.get_feature_names())\n",
    "\n",
    "        self.nb = GaussianNB()\n",
    "        self.nb.fit(self.vector.todense(), self.y_train)\n",
    "\n",
    "    def internal_predict(self, print_side_by_side = True):\n",
    "        test_tfidf = self.tfidf.transform(self.X_test)\n",
    "        predicted = self.nb.predict(test_tfidf.todense())\n",
    "        print('labeled')\n",
    "        acc = np.mean(self.y_test == predicted)\n",
    "\n",
    "        print('Test accuracy =',acc)\n",
    "        print('')\n",
    "\n",
    "        if print_side_by_side:\n",
    "            for true,predict in zip(self.y_test,predicted):\n",
    "                print('-->',true,predict)\n",
    "\n",
    "\n",
    "    def predict(self,text):\n",
    "        top_n = 3\n",
    "        test_tfidf = self.tfidf.transform([text])\n",
    "        probs = self.nb.predict_proba(test_tfidf.todense())\n",
    "        probs = probs.flatten()\n",
    "        above_0 = np.argwhere(probs>0).flatten()\n",
    "        above_0 = np.sort(above_0)[::-1]\n",
    "        print('-->',text,'=',)\n",
    "        \n",
    "        for i in above_0[:5]:\n",
    "            print(self.nb.classes_[i],' ', probs.flatten()[i],' ',)\n",
    "        print('')\n",
    "        return(probs)\n",
    "\n",
    "    def print_top_words(self,top_n_words=5):\n",
    "        # printing top words for each emoji\n",
    "        print('')\n",
    "        print('----- Top {} words for each Emoji in Train set'.format(top_n_words))\n",
    "        print('-'*60)\n",
    "        for i in range(len(self.nb.classes_)):\n",
    "            top =  self.bag[self.nb.theta_[i].argsort()[::-1]][:top_n_words]\n",
    "            print(self.nb.classes_[i],' -->',top)\n",
    "        print('')\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    # run clean_tweets\n",
    "    # run labeled_tweets\n",
    "\n",
    "    emo = Emoji()\n",
    "    emo.fit()\n",
    "    emo.model()\n",
    "    b = emo.predict('ocasio')\n",
    "    c = emo.predict('climate change')\n",
    "    d = emo.predict('vegan')\n",
    "    e = emo.predict('earth')\n",
    "    e = emo.predict('greta')\n",
    "    e = emo.predict('korea')\n",
    "    e = emo.predict('abortion')\n",
    "    e = emo.predict('love you')\n",
    "    e = emo.predict('birthday')\n",
    "    a = emo.predict('i want a divorce')\n",
    "    e = emo.predict('life')\n",
    "    e = emo.predict('baby')\n",
    "    e = emo.predict('basketball')\n",
    "    e = emo.predict('i need coffee')\n",
    "    e = emo.predict('la la land')\n",
    "    e = emo.predict('netflix and chill')\n",
    "    e = emo.predict('i am thankful to be alive')\n",
    "    e = emo.predict('ocean')\n",
    "    e = emo.predict('trump')\n",
    "    e = emo.predict('plastic')\n",
    "    e = emo.predict('boyfriend')\n",
    "\n",
    "    # emo.internal_predict(print_side_by_side = True)\n",
    "    emo.print_top_words(5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
