{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "altered code from https://github.com/janvanzeghbroeck/urban-emoji/blob/master/twitter_api.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from __future__ import unicode_literals\n",
    "import twitter_credentials as cred\n",
    "import tweepy\n",
    "import pickle\n",
    "import os\n",
    "import datetime\n",
    "import time\n",
    "import re\n",
    "import boto\n",
    "from os import listdir \n",
    "from os.path import isfile, join \n",
    "import pandas as pd\n",
    "\n",
    "class read_tweets(object):\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.auth = tweepy.OAuthHandler(cred.consumer_key, cred.consumer_secret)\n",
    "        self.auth.set_access_token(cred.access_token, cred.access_token_secret)\n",
    "        self.api = tweepy.API(self.auth)\n",
    "        self.df_emoji = pd.read_pickle('./data/df_emoji.pkl')\n",
    "\n",
    "    def get_tweets(self, topic, save_file_name, num_batches = 10, num_tweets = 20, to_bucket = False): # num_batches * 100 is total tweets target\n",
    "        tweets = set()\n",
    "        # public_tweets = api.home_timeline()\n",
    "        for i in range(num_batches):\n",
    "            try:\n",
    "                print('Loading', i+1, 'of', num_batches)\n",
    "                for tweet in tweepy.Cursor(self.api.search, q=topic).items(num_tweets): #100 batches of 20\n",
    "\n",
    "                    if tweet.lang == 'en':\n",
    "                        tweets.add(tweet.text) \n",
    "\n",
    "                time.sleep(35) \n",
    "            except:\n",
    "                print('Waiting for API to allow more calls...')\n",
    "                time.sleep(60)\n",
    "                pass\n",
    "\n",
    "        # if to_bucket:\n",
    "            pass\n",
    "        else:\n",
    "            pickle.dump(tweets, open( \"{}.pkl\".format(save_file_name), \"wb\" ) )\n",
    "            print('Succesfully pickled', len(tweets), 'tweets!')\n",
    "            \n",
    "        return(tweets)\n",
    "    \n",
    "    def keyword_tweets(self, keywords, num_batches = 10, num_tweets = 20):\n",
    "        \n",
    "        now = datetime.datetime.today().ctime()\n",
    "        now = re.sub(' ','_',now)\n",
    "        now = re.sub(':','-',now)\n",
    "\n",
    "\n",
    "        # use boto to connect to aws buckets\n",
    "        conn = boto.connect_s3(cred.aws_access_key, cred.aws_access_secret_key)\n",
    "\n",
    "        # what bucket?\n",
    "        bucket_name = 'emoji-tweets'\n",
    "\n",
    "        # check if bucket exists if not make it\n",
    "        if conn.lookup(bucket_name) is None:\n",
    "            b = conn.create_bucket(bucket_name)\n",
    "        else:\n",
    "            b = conn.get_bucket(bucket_name)\n",
    "\n",
    "        #some simple english words\n",
    "        #words = ['is', 'it', 'the']\n",
    "        words = []\n",
    "        words.extend(keywords)\n",
    "        \n",
    "        tweets = []\n",
    "\n",
    "        for word in words:\n",
    "            pkl_name = './tweet_data/tweets_{}_{}'.format(now,word)\n",
    "            s3_name = 'tweets_{}_{}.pkl'.format(now,word)\n",
    "            loc_name = './tweet_data/tweets_{}_{}.pkl'.format(now,word)\n",
    "            tweets += self.get_tweets(word, pkl_name, num_batches = num_batches, num_tweets = num_tweets)\n",
    "\n",
    "            # save the pkl file\n",
    "            file_object = b.new_key(s3_name)#where to save\n",
    "            file_object.set_contents_from_filename(loc_name)\n",
    "\n",
    "            print('Successfully saved {} to S3 bucket {}'.format(s3_name,bucket_name))\n",
    "        # to read the file\n",
    "        #fil_object.get_contents_to_file('folder/file')        \n",
    "        return(tweets)\n",
    "    \n",
    "    def all_tweets(self, keywords, num_batches = 10, num_tweets = 20):\n",
    "        '''\n",
    "        run keyword_tweets() and \n",
    "        combinine with the tweets that is already read and are in tweet_data folder\n",
    "        '''\n",
    "        mypath = './tweet_data'\n",
    "        files = [f for f in listdir(mypath) if isfile(join(mypath, f))]\n",
    "        \n",
    "        tweets = self.keyword_tweets(keywords, num_batches = num_batches, num_tweets = num_tweets)\n",
    "        for i in files:\n",
    "            if i != '.DS_Store':\n",
    "                file = './tweet_data/'+ i\n",
    "                tweets += list(pickle.load(open(file,'rb')))\n",
    "        \n",
    "        # removing duplicates\n",
    "        tweets = list(dict.fromkeys(tweets))\n",
    "        return(tweets)\n",
    "\n",
    "    def emoji_tweets(self, keywords, num_batches = 10, num_tweets = 20):\n",
    "        '''\n",
    "        finding the tweets with emojis\n",
    "        ''' \n",
    "        tweets = self.all_tweets(keywords, num_batches = num_batches, num_tweets = num_tweets)\n",
    "        no_moji = []\n",
    "        yay_moji = []\n",
    "        for tweet in tweets:\n",
    "            tweet = str(tweet) #some have type tweepy.models.Status\n",
    "            yay = False\n",
    "            for uni in self.df_emoji['unichar']:\n",
    "                #if emoji in str(tweet):\n",
    "                if uni in tweet:\n",
    "                    yay = True\n",
    "            if yay:\n",
    "                yay_moji.append(tweet)\n",
    "            else: # else statement to create no_moji list\n",
    "                no_moji.append(tweet)\n",
    "        \n",
    "        pickle.dump(yay_moji, open( \"./data/yay_moji.pkl\", \"wb\"))\n",
    "        print('Succesfully pickled {} tweets and emoji data frame'.format(len(yay_moji)))\n",
    "        \n",
    "        return(yay_moji)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "read = read_tweets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading 1 of 1\n",
      "Succesfully pickled 5 tweets!\n",
      "Successfully saved tweets_Tue_Jul__9_12-00-24_2019_vegan.pkl to S3 bucket emoji-tweets\n",
      "Loading 1 of 1\n",
      "Succesfully pickled 5 tweets!\n",
      "Successfully saved tweets_Tue_Jul__9_12-00-24_2019_trump.pkl to S3 bucket emoji-tweets\n",
      "Succesfully pickled 7779 tweets and emoji data frame\n"
     ]
    }
   ],
   "source": [
    "tweets = read.emoji_tweets(['vegan','trump'], num_batches = 1, num_tweets = 5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
